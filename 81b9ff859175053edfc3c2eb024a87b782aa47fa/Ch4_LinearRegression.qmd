# Linear Regression 


This chapter is based on the following references: 

* Chapter 3 of [An Introduction to Statistical Learning](https://www.statlearning.com/) 
* Chapter 1 of Econometrics by Fumio Hayashi



## Assumptions {#sec-LinModAssumptions}

The (multiple) linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:

**Assumption 1: Model and Sampling**

**Part (a): Linear Model**

$$
\begin{align}
  Y_i= \underbrace{\sum_{k=0}^p\beta_k X_{ik}}_{=f(X_i)}+\epsilon_i, \quad i=1,\dots,n,
\end{align}
$${#eq-LinMod}
where 
$$
X_{i0}=1
$$ 
for all $i=1,\dots,n.$ 

* $Y_i$ is called "dependent variable" or "outcome variable" or "regressand"
* $X_{ik}$ is called the $k$th "independent variable" or "predictor variable" or "regressor" or "explanatory variable" or "control variable." 
* $\epsilon_i$ is the error term.

It is convenient to write @eq-LinMod using matrix notation
$$
\begin{eqnarray*}
  Y_i&=&\underset{(1\times (p+1))}{X_i'}\underset{((p+1)\times 1)}{\beta} +\epsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}
$$
where 
$$
 X_i=\left(\begin{matrix}X_{i0}\\ \vdots\\  X_{ip}\end{matrix}\right)
  \quad\text{and}\quad 
\beta=\left(\begin{matrix}\beta_0\\ \vdots\\ \beta_p\end{matrix}\right).
$$ 
Stacking all individual rows, $X_i',$ $i=1,\dots,n$ leads to
$$
\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&=&\underset{(n\times (p+1))}{X}\underset{((p+1)\times 1)}{\beta} + \underset{(n\times 1)}{\epsilon},
\end{eqnarray*}
$$
where 
$$
\begin{equation*}
Y=\left(\begin{matrix}Y_1
\\ \vdots\\
Y_n
\end{matrix}\right),
\quad 
X=\left(\begin{matrix}
X_{10}&\dots&X_{1p}\\
\vdots&\ddots&\vdots\\ 
X_{n0}&\dots&X_{np}\\
\end{matrix}\right),
\quad
\text{and}
\quad 
\epsilon=
\left(\begin{matrix}
\epsilon_1\\ \vdots\\ 
\epsilon_n
\end{matrix}\right).
\end{equation*}
$$

::: {.callout-tip}

# Simple Linear Regression and Polynomial Regression Model 

The special case of $p=1$
$$
Y_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i
$$
is called the ***simple* linear regression model**. With the simple linear regression model, only straight line fits are possible. 

By contrast, with the multiple linear regression model, we can also fit polynomials. For instance, we can define
$$
X_{i2} := X_{i1}^2
$$
which leads to a quadratic regression model (often used for life-cycle analyses that include the predictor `Age`$_i=X_{i1}$)
$$
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i1}^2 + \epsilon_i.
$$
Of course, further predictor variables $X_{i2},\dots,X_{ip}$ can (and should) be added to this model. 

The same logic applies to polynomials with higher polynomial degrees $(\geq 2).$ Large polynomial degrees, however, can lead to unstable estimation results. 
:::


::: {.callout-note}
The assumption $f(X_i) = X_i'\beta$ may be a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple.  
:::


**Part (b): Random Sample**

We assume that the $n$ observed data points 
$$
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{np}))
$$ 
are a realization of the **training data random sample** 
$$
((Y_{1},X_{10},\dots,X_{1(p+1)}),\dots,(Y_{n},X_{n0},\dots,X_{np})).
$$ 

That is, the $i$th observed $p+2$ dimensional data point 
$$
(y_{i},x_{i0},\dots,x_{ip})\in\mathbb{R}^{p+2}
$$ 
is a realization of a $p+2$ dimensional random variable 
$$
(Y_{i},X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+2},
$$ 
where 

1. $(Y_{i},X_{i0},\dots,X_{ip})$ has the identical $p+2$ dimensional distribution for all $i=1,\dots,n.$ 
2. $(Y_{i},X_{i0},\dots,X_{ip})$ is independent of 
$(Y_{j},X_{j0},\dots,X_{j(p+1)})$ for all $i\neq j=1,\dots,n.$

<!-- 
::: {.callout-note}
Due to @eq-LinMod, this i.i.d. assumption is equivalent to assuming that the multivariate random variables 
$$
(\epsilon_i,X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+2}
$$ 
are i.i.d. across $i=1,\dots,n$. 
::: 
-->

::: {.callout-caution}
**Remark:** Often, we do not use a different notation for observed realizations 
$(y_{i},x_{i0},\dots,x_{ip})\in\mathbb{R}^{p+2}$ 
and for the corresponding random variable 
$(Y_{i},X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+2}$ 
since often both interpretations (random variable and its realizations) can make sense 
in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.
:::



**Assumption 2: Exogeneity**
$$
E(\epsilon_i|X_i)=0,\quad i=1,\dots,n
$${#eq-assExogen}

This assumption demands that the mean of the random error term $\epsilon_i$ is zero irrespective of the realizations of $X_i$. This exogeneity assumption is also called

* "orthogonality assumption" or 
* "mean independence assumption."

::: {.callout-note}
Together with the random sample assumption (Assumption 1, Part (b)) @eq-assExogen even implies **strict exogeneity** 
$$
E(\epsilon|X) = \underset{(n\times 1)}{0},
$$ 
since we have independence across $i=1,\dots,n$.  Under strict exogeneity, the mean of the random error **vector** $\epsilon\in\mathbb{R}^n$ is zero irrespective of the realizations of the $(n\times (p+1))$-dimensional random predictor matrix $X.$ 
:::


::: {.callout icon="false"}

# Example: Independence between error term and predictors

Let

* $E(\epsilon_i)=0$ and
* $\epsilon_i$ be independent of $X_i$

Here the assumption of exogeneity is fulfilled since by the independence between $\epsilon_i$ and $X_i$ we have that
$$
E(\epsilon_i|X_i) = E(\epsilon_i) 
$$
and by assumption $E(\epsilon_i)=0$ such that 
$$
E(\epsilon_i|X_i) = 0.
$$

Note: The assumption $E(\epsilon_i)=0$ is not critical (i.e. not restrictive) due to the intercept term in @eq-assExogen. 
:::


::: {.callout icon="false"}

# Examples: Heteroskedastic Error

Let

* $\epsilon_i\sim\mathcal{N}(0,\sigma_i^2),$ where 
* $\sigma_i = |X_{i1}|$ 

Here the assumption of exogeneity is fulfilled since realizations of $X_i$ do not affect the mean of $\epsilon_i,$ thus
$$
E(\epsilon_i|X_i) = 0. 
$$

However, $\epsilon_i$ and $X_i$ are not independent of each other, since the conditional variance of $\epsilon_i$ is a function of $X_{i1}$
$$
Var(\epsilon_i|X_i) = |X_{i1}|^2.
$$
:::



**Assumption 3: Rank Condition (no perfect multicollinearity)**

$$
\begin{align*}
\operatorname{rank}(X)&=(p+1)\quad\text{a.s.}\\[2ex]
\Leftrightarrow P\big(\operatorname{rank}(X)&=(p+1)\big)=1
\end{align*}
$$
This assumption demands that, with probability one, no predictor variable $X_{k}\in\mathbb{R}^n$ is linearly dependent of the others. (This is the literal translation of the "almost surely (a.s.)" concept.) 


**Note:** The assumption implies that $n\geq (p+1),$ since 
$$
\operatorname{rank}(X)\leq \min\{n,(p+1)\}\quad(a.s.)
$$

This rank assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation).  The violation of this assumption harms any economic interpretation since we cannot disentangle the explanatory variables' individual effects on $Y$. Therefore, this assumption is also often called an **identification assumption**.


::: {.callout-tip}

* Under Assumption 3, we have that 
$$
\operatorname{rank}(X)=(p+1)\quad\text{(a.s.)}
$$ 

* This implies that the $((p+1)\times (p+1))$-dimensional matrix $X'X$ has full rank, i.e. that 
$$
\operatorname{rank}(X'X)=(p+1)\quad\text{(a.s.)}
$$

* Thus $(X'X)$ is invertible; i.e. there exists a $((p+1)\times (p+1))$-dimensional matrix $(X'X)^{-1}$ such that 
$$
(X'X)(X'X)^{-1} = (X'X)^{-1}(X'X) = I_{(p+1)}.
$$ 
:::



**Assumption 4: Error distribution** 

There are different more or less restrictive assumptions.  Some of the most common ones are the following:

- **Conditional distribution with sufficiently many moments:** 
$$
\epsilon_i|X_i \sim f_{\epsilon|X}
$$ 
for all $i=1,\dots,n$ and for any distribution $f_{\epsilon|X}$ with two (or more) finite moments.
  - **Example: Conditional normal distribution** 
  $$
  \epsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))
  $$ 
  for all $i=1,\dots,n$.

- **Independence between error and predictors:** $\epsilon_i\sim f_\epsilon$ for all $i=1,\dots,n$ such that $f_\epsilon=f_{\epsilon|X}$ and such that $f_\epsilon$ has two (or more) finite moments.  
   - **Example: Independence between a Gaussian error and the predictors** 
   $$
   f_\epsilon=\mathcal{N}(0,\sigma^2),
   $$ 
   where $\sigma^2$ does not depend on $X.$

- **Spherical errors:** The conditional distributions of  $\epsilon_i|X_i$ may generally depend on $X_i$ for all $i=1,\dots,n,$ but only such that
$$
E(\epsilon|X)=\underset{(n\times 1)}{0}
$$
and 
$$
\begin{align*}
&\underset{(n\times n)}{Var\left(\epsilon|X\right)}=\\[2ex] 
& = \left(\begin{matrix}
Var(\epsilon_1|X)&Cov(\epsilon_1,\epsilon_2|X)&\dots&Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&Var(\epsilon_2|X)&\dots&Cov(\epsilon_2,\epsilon_n|X)\\
\vdots&\vdots&\ddots&\vdots\\
Cov(\epsilon_n,\epsilon_1|X)&Cov(\epsilon_n,\epsilon_2|X)&\dots&Var(\epsilon_n|X)
\end{matrix}\right)\\[2ex]
& = \left(\begin{matrix}
\sigma^2&0&\dots&0\\
0&\sigma^2&\dots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\dots&\sigma^2
\end{matrix}\right)
= \sigma^2 I_n,
\end{align*}
$$
where $I_n$ denotes the $(n\times n)$ identity matrix with ones on the diagonal and zeros else.</br> 
Thus, under the spherical errors assumption, one has, for all possible realizations of $X$, that: 
   * **uncorrelated:** $Cov(\epsilon_i,\epsilon_j|X)=0$ for all $i=1,\dots,n$ and all $j=1,\dots,n$ such that $i\neq j$ 
   * **homoskedastic:** $Var(\epsilon_i|X)=\sigma^2$ for all $i=1,\dots,n$




All four Assumptions 1-4 must hold for doing inference using the (multiple) linear regression model.




#### Homoskedastic versus Heteroskedastic Error Terms {-}


The i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between $\epsilon_i$ and $(X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+1}$. That is, the error term $\epsilon_i$ can have a conditional distribution which depends on $(X_{i0},\dots,X_{ip}).$ 


The exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of $\epsilon_i$ is independent of $X_i$. Besides this, dependencies between $\epsilon_i$ and $X_{i0},\dots,X_{ip}$ are allowed. For instance, the variance of $\epsilon_i$ can be a function of $X_{i0},\dots,X_{ip}.$ If this is the case, $\epsilon_i$ is said to be **"heteroskedastic."** 

- **Heteroskedastic error terms:** The conditional variances 
$$
Var(\epsilon_i|X_i=x_i)=\sigma^2(x_i)
$$ 
are a non-constant function $\sigma^2(x_i)>0$ of the realizations $X_i=x_i.$ 




- **Homoskedastic error terms:** The conditional variances 
$$
Var(\epsilon_i|X_i=x_i)=\sigma^2
$$ 
are constant $\sigma^2>0$ for every possible realization $X_i=x_i.$ 






::: {.callout icon="false"}

# Example: Heteroskedastic Error 

$$
\epsilon_i|X_i\sim \mathcal{U}[-0.5|X_{i2}|, 0.5|X_{i2}|],
$$ 
with 
$$
X_{i2}\sim \mathcal{U}[-4,4]
$$ 
for all $i=1,\dots,n,$ where $\mathcal{U}[a,b]$ denotes the uniform distribution over $[a,b].$ 

This error term is mean independent of $X_i$ since $E(\epsilon_i|X_i)=0$, but it has a heteroskedastic conditional variance since 
$$
Var(\epsilon_i|X_i)=\frac{1}{12}X_{i2}^2
$$ 
depends on $X_{i2}.$
:::






::: {.callout icon="false"}

# Example: Homoskedastic Error

$$
\epsilon_i\sim{\mathcal N} (0, \sigma^2)
$$
for all $i=1,\dots,n.$ Here, the conditional variance of the error terms $\epsilon_i$ given $X_i$
$$
Var(\epsilon_i|X_i)=Var(\epsilon_i)=\sigma^2
$$ 
are equal to the constant $\sigma^2>0$ for all $i=1,\dots,n$ and for every possible realization of $X_i.$ 
::: 






## Deriving the Expression of the OLS Estimator

We derive the expression for the OLS estimator 
$$
\hat\beta=(\hat\beta_0,\dots,\hat\beta_p)'\in\mathbb{R}^{p+1}
$$ 
as the vector-valued minimizing argument of the sum of squared residuals, 
$$
\operatorname{RSS}(b)=\sum_{i=1}^n\big(\underbrace{Y_i-X_i'b}_{\text{$i$th residual}}\big)^2
$$ 
with $b\in\mathbb{R}^(p+1)$, for a given sample 
$$
((Y_1,X_1),\dots,(Y_n,X_n)).
$$  

Using matrix/vector notation we can write $S_n(b)$ as 
$$
\begin{align*}
\operatorname{RSS}(b)
&=\sum_{i=1}^n(Y_i-X_i'b)^2\\[2ex]
&=(Y-X b)^{\prime}(Y-X b)\\[2ex]
&=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}
$$
To find the minimizing argument 
$$
\hat\beta = \arg\min_{b\in\mathbb{R}^{p+1}}\operatorname{RSS}(b)
$$ 
we compute the vector containing all partial derivatives
$$
\begin{align*}
\underset{((p+1)\times 1)}{\frac{\partial \operatorname{RSS}(b)}{\partial b}} &=-2\left(X^{\prime}Y -X^{\prime} Xb\right).
\end{align*}
$$
Setting each partial derivative to zero leads to $(p+1)$ linear equations ("normal equations") in $(p+1)$ unknowns. This linear system of equations defines the OLS estimates, $\hat{\beta}$, for a given dataset:
$$
\begin{align*}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)
&=\underset{((p+1)\times 1)}{0}\\[2ex]
X^{\prime} X\hat{\beta}
&=\underset{((p+1)\times 1)}{X^{\prime}Y}.
\end{align*}
$$
From our full rank assumption (Assumption 3) it follows that $X^{\prime}X$ is an invertible $((p+1)\times (p+1))$-dimensional matrix which allows us to solve the equation system by 
$$
\begin{align*}
\underset{((p+1)\times 1)}{\hat{\beta}} &=\left(X^{\prime} X\right)^{-1} X^{\prime} Y.
\end{align*}
$$


The following codes computes the estimate $\hat{\beta}$ for a given dataset with $X_i\in\mathbb{R}^{p+1}$, $p=2$. 

```{r, fig.align="center"}
# Some given data
X_1     <- c(1.9,0.8,1.1,0.1,-0.1,4.4,4.6,1.6,5.5,3.4)
X_2     <- c(66, 62, 64, 61, 63, 70, 68, 62, 68, 66)
Y       <- c(0.7,-1.0,-0.2,-1.2,-0.1,3.4,0.0,0.8,3.7,2.0)
dataset <-  data.frame("X_1" = X_1, "X_2" = X_2, "Y" = Y)
## Compute the OLS estimation
lmobj   <- lm(Y ~ X_1 + X_2, data = dataset)
## Plot sample regression surface
library("scatterplot3d") # library for 3d plots
plot3d  <- scatterplot3d(x = X_1, y = X_2, z = Y,
            angle = 33, scale.y = 0.8, pch = 16,
            color ="red", 
            xlab = expression(X[1]),
            ylab = expression(X[2]),
            main ="OLS Regression Surface")
plot3d$plane3d(lmobj, lty.box = "solid", col=gray(.5), draw_polygon=TRUE)
```


#### Special Case: Simple Linear Regression Model {-}

Let's consider the simple linear regression model $(p=1)$
$$
Y_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i,\qquad i=1,\dots,n.
$$
For this case, the expression of the OLS-estimators simplifies. For a given observed realization of the training data random sample 
$$
(x_1,y_1),\dots,(x_n,y_n)
$$
the values of the OLS estimator
$$
\hat\beta = 
\begin{pmatrix}
\hat\beta_0\\
\hat\beta_1
\end{pmatrix}
$$
are given by 
$$
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
$$
and
$$
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
$$
where 
$\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i$ 
and 
$\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$. 

![](images/Fig_3_1.png)

<!-- 
![](images/Fig_3_2.png) 
-->




#### Some Quantities of Interest {-}


**Predicted values and residuals.**

- The (OLS) **predicted values**: 
$$
\hat{Y}_i=X_i'\hat\beta, \quad i=1,\dots,n
$$ 
The $(n\times 1)$ vector of predicted values
$$
\begin{align*}
\hat{Y} = \left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)
&=X\hat{\beta}\\[-2ex]
&=\underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&=P_X Y
\end{align*}
$$

- The (OLS) **residuals**: 
$$
e_i=Y_i-\hat{Y}_i, \quad i=1,\dots,n
$$ 
The $(n\times 1)$ vector of residuals
$$
\begin{align*}
e = 
\left(\begin{matrix}e_1\\e_2\\ \vdots\\ e_n\end{matrix}\right)
&=
\left(\begin{matrix}Y_1\\[.5ex]Y_2\\[.5ex] \vdots\\[.5ex] Y_n\end{matrix}\right)-
\left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)\\[2ex]
&=Y - \hat{Y}\\[2ex]
%&=Y - X\hat{\beta}\\[-2ex]
%&=Y - \underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&=Y - P_X Y\\[2ex]
&=\underbrace{(I_n - P_X)}_{=M_X} Y\\[2ex]
&=M_XY
\end{align*}
$$

**Projection matrices.** 

The matrix 
$$
P_X=X(X'X)^{-1}X'
$$ 
is the $(n\times n)$ **projection matrix** that projects any vector from $\mathbb{R}^n$ into the column space spanned by the column vectors of $X$ and 
$$
M_X=I_n-X(X'X)^{-1}X'=I_n-P_X
$$ 
is the associated $(n\times n)$ **orthogonal projection matrix** that projects any vector from $\mathbb{R}^n$ into the vector space that is orthogonal to that spanned by the column vectors of $X.$




## Assessing the Accuracy of the Model Fit $\hat{f}$

The larger the proportion of the explained variance, the better is the fit of the estimated model $\hat{f}$ to the training data. This motivates the definition of the so-called $R^2$ coefficient of determination:
$$
\begin{eqnarray*}
R^2 
%&=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&=1-\frac{\sum_{i=1}^ne_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&=1-\frac{\operatorname{RSS}}{\operatorname{TSS}}
\end{eqnarray*}
$$
with 
$$
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat\beta)=\sum_{i=1}^n\left(y_i-x_i'\hat\beta\right)^2=\sum_{i=1}^ne_i^2
\end{align*}
$$
and
$$
\begin{align*}
\operatorname{TSS}=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2.
\end{align*}
$$


$\operatorname{TSS}$ "Total Sum of Squares"

$\operatorname{RSS}$ "Residual Sum of Squares"


* Obviously, we have that $0\leq R^2\leq 1$. 

* The closer $R^2$ lies to $1$, the better is the fit of the model to the observed training data. 


In tendency an accurate model has ...

* a low residual standard error $\operatorname{RSE}$
$$
\operatorname{RSE}=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}
$$

* a high $R^2$

$$
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
$$
where $0\leq R^2\leq 1.$



::: {.callout-caution}

**Cautionary Note Nr 1:** 

Do not forget that there is a **irreducible error** $Var(\epsilon)=\sigma^2>0$. Thus 

* very low $\operatorname{RSE}$ values $(\operatorname{RSE}\approx 0)$ and 
* very high $R^2$ values $(R^2\approx 1)$ 

can be warning signals indicating overfitting. While overfitting typically does not happen with a simple linear regression model, it can happen with multiple linear regression models containing a lot of parameters (large $p$).


**Cautionary Note Nr 2:** 

The $R^2$ and $\operatorname{RSE}$ are only based on **training data**. In @sec-SL, we have seen that a proper assessment of the model accuracy needs to take into account **test data**.
:::



#### $R^2$ and correlation coefficient {-}

In the case of the simple linear regression model, $R^2$ equals the squared sample correlation coefficient between $Y$ and $X$,
$$
R^2 = r_{yx_1}^2,
$$
where 
$$
r_{yx_1}=\frac{\sum_{i=1}^n(x_{i1}-\bar{x}_1)(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_{i1}-\bar{x}_1)^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}},
$$
where $\bar{x}_1=n^{-1}\sum_{i=1}^nx_{i1}.$

::: {.callout-tip}
In the multiple linear regression model $Y_i=\beta_0+\sum_{j=1}^p\beta_jX_{ij}+\epsilon_i,$ the $R^2$ equals the squared correlation between response and the fitted values: 
$$
R^2=r^2_{y\hat{y}}
$$
with 
$$
r_{y\hat{y}}=\frac{\sum_{i=1}^n(y_i-\bar{y})(\hat{y}_i-\bar{\hat{y}})}{\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}\sqrt{\sum_{i=1}^n(\hat{y}_i-\bar{\hat{y}})^2}},
$$
where $\bar{y}=n^{-1}\sum_{i=1}^ny_{i}.$
:::



::: {.callout-caution}
* A high/low $R^2$ value only means that the predictors have high/low *predictive power* with respect to the training data. 

* A high/low $R^2$ does not mean a validation/falsification of the estimated model. Any econometric model needs a plausible explanation from relevant economic theory.   
:::


The most often criticized disadvantage of the $R^2$ is that additional regressors (relevant or not) will increase the $R^2$. The below `R`-codes demonstrates this problem.

```{r}
set.seed(123)
n     <- 100                  # Sample size
X     <- runif(n, 0, 10)      # Relevant X variable
X_ir  <- runif(n, 5, 20)      # Irrelevant X variable
error <- rt(n, df = 10)*10    # True (usually unknown) error
Y     <- 1 + 5 * X + error    # Y variable
lm1   <- summary(lm(Y~X))     # Correct OLS regression 
lm2   <- summary(lm(Y~X+X_ir))# OLS regression with X_ir 
lm1$r.squared < lm2$r.squared
```
So, $R^2$ increases here even though `X_ir` is a completely irrelevant explanatory variable.  

Because of this, the $R^2$ cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called **adjusted** $R^2$, $\overline{R}^2,$ defined as
$$
\begin{eqnarray*}
  \overline{R}^2&=&1-\frac{\frac{1}{n-(p+1)}\sum_{i=1}^ne^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}\leq R^2%\\
\end{eqnarray*}
$$
The adjustment is in terms of the degrees of freedom $n-(p+1)$.


```{r}
round(lm1$adj.r.squared, digits = 3) # model without X_ir
round(lm2$adj.r.squared, digits = 3) # model with X_ir
```


## Assessing the Accuracy of the Coefficient Estimators $\hat{\beta}$


### Bias of $\hat{\beta}$


Under the Assumptions 1-4, one can show that the OLS estimator 
$$
\hat\beta = (X'X)^{-1}X'Y
$$
is unbiased, i.e.
$$
\begin{align*}
\operatorname{Bias}(\hat\beta) 
& = E(\hat\beta) - \beta \\[2ex]
& = \beta  - \beta \\[2ex]
& = \underset{(p+1)\times 1}{0}.
\end{align*}
$${#eq-OLSUnbiased}
That is, in the mean $\hat\beta$ equals $\beta.$


@eq-OLSUnbiased can be shown as following: 

Observe that 
$$
\hat\beta=(X'X)^{-1}X'Y
$$ 
consists of two multivariate random variables; namely 

* $X\in\mathbb{R}^{n\times(p+1)}$ and 
* $Y\in\mathbb{R}^n.$ 

Thus we firstly need to derive the **conditional mean** $E(\hat\beta|X),$ which effectively allows us to focus on randomness due to $\epsilon,$  
$$
\begin{align*}
E(\hat\beta|X) 
&= E((X'X)^{-1}X'Y|X) \\[2ex]
&\text{Using that (by Assumption 1 Part (a)) $Y=X\beta + \epsilon$:}\\[2ex]
&= E((X'X)^{-1}X'\underbrace{Y}_{=X\beta+\epsilon}|X) \\[2ex]
&= E((X'X)^{-1}X'(X\beta+\epsilon)|X)                 \\[2ex]
&= E(\underbrace{(X'X)^{-1}X'X}_{=I_K}\beta|X) + E((X'X)^{-1}X'\epsilon|X)\\[2ex]
&= \underbrace{E(\beta|X)}_{=\beta} + \underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=(X'X)^{-1}X'E(\epsilon|X)}  \\[2ex]
&= \beta + (X'X)^{-1}X'\underbrace{E(\epsilon|X)}_{=0}\\[2ex] 
&= \underset{(p+1)\times 1}{\beta}  
\end{align*}
$$
Thus $\hat\beta$ is **conditionally unbiased:** 
$$
\begin{align*}
\operatorname{Bias}(\hat\beta|X) 
& = E(\hat\beta|X) - \beta\\[2ex]
& = \beta - \beta \\[2ex]
& = \underset{(p+1)\times 1}{0}   
\end{align*}
$$


From this if follows, by the iterated law of expectations, that the OLS estimator is also **unconditionally unbiased:**  
$$
\begin{align*}
\operatorname{Bias}(\hat\beta) 
& = E(\operatorname{Bias}(\hat\beta|X))\\[2ex] 
& = E\left(E(\hat\beta|X) - \beta\right)\\[2ex] 
& = E\left(E(\hat\beta|X)\right) - E\left(\beta\right)\\[2ex] 
& = E\left(\beta\right) - E\left(\beta\right)\\[2ex] 
& = \beta - \beta = 0.
\end{align*}
$$


### Standard Error of $\hat{\beta}_j$

The standard error of $\hat{\beta}_j,$ for each $j=0,\dots,p,$ is given by
$$
\operatorname{SE}(\hat\beta_j|X)=\sqrt{Var(\hat\beta_j|X)},
$$
where 
$$
Var(\hat\beta_j|X) = \left[Var(\hat\beta|X)\right]_{(j,j)}
$$ 
denotes the $j$th diagonal element of the symmetric $(p+1)\times (p+1)$ variance-covariance matrix
$$
\begin{align*}
&Var(\hat\beta|X)=\\[2ex]
&=\begin{pmatrix}
Var(\hat\beta_0|X)&Cov(\hat\beta_0,\hat\beta_1|X)&\cdots&Cov(\hat\beta_0,\hat\beta_{p}|X)\\
Cov(\hat\beta_1,\hat\beta_0|X)&Var(\hat\beta_1|X)&  &Cov(\hat\beta_1,\hat\beta_{p}|X)\\
\vdots &&\ddots&\\
Cov(\hat\beta_p,\hat\beta_0|X)&Cov(\hat\beta_p,\hat\beta_1|X)&\cdots&Var(\hat\beta_{p}|X)\\
\end{pmatrix}
\end{align*}
$$

Thus, to compute a useful, explicit expression for 
$$
\operatorname{SE}(\hat\beta_j|X)=?,
$$ 
we need to compute an explicit expression for the symmetric $(p+1)\times(p+1)$ variance-covariance matrix $Var(\hat\beta|X).$ 


Let us derive the general explicit expression for $Var(\hat\beta|X).$

Note that
$$
\begin{align*}
\hat{\beta}
&=(X'X)^{-1}X'Y\\[2ex]
&\text{Using that (by Assumption 1 Part (a)) $Y=X\beta + \epsilon$:}\\[2ex]
&=(X'X)^{-1}X'(X\beta + \epsilon)\\[2ex]
&=\underbrace{(X'X)^{-1}X'X\beta}_{=\beta} + (X'X)^{-1}X'\epsilon
\end{align*}
$$
This leads to the so-called **sampling error expression** 
$$
\hat{\beta} - \beta = (X'X)^{-1}X'\epsilon.
$$
With this, we can derive the general explicit expression for $Var(\hat\beta|X).$
$$
\begin{align*}
&Var(\hat\beta|X)=\\[2ex]
&\text{Adding/subtracting constants does not change variance:}\\[2ex]
&=Var(\hat\beta - \beta|X)\\[2ex]
&\text{Using the sampling error expression:}\\[2ex]
&=Var((X'X)^{-1}X'\epsilon|X)\\[2ex]
&=E\Big[\big((X'X)^{-1}X'\epsilon-\underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=0}\big)\times\\[2ex]
&\phantom{=\Big(}\,\times\big((X'X)^{-1}X'\epsilon-\underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=0}\big)'|X\Big]\\[2ex]
&=E\left[((X'X)^{-1}X'\epsilon)((X'X)^{-1}X'\epsilon)'|X\right]\\[2ex]
&=E\left[(X'X)^{-1}X'\epsilon\epsilon' X(X'X)^{-1}|X\right]\\[2ex]
&=\;\;\;(X'X)^{-1}X'\underbrace{E\left(\epsilon\epsilon'|X\right)}_{=Var(\epsilon|X)}X(X'X)^{-1}
\end{align*}
$$
That is, the explicit expression for $Var(\hat\beta|X)$ depends on the explicit form of the symmetric $(n\times n)$ matrix $Var(\epsilon|X)$ 
$$
\begin{align*}
&Var(\epsilon|X)=\\[2ex]
&=\begin{pmatrix}
Var(\epsilon_1|X)&Cov(\epsilon_1,\epsilon_2|X)&\cdots&Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&Var(\epsilon_2|X)&  &Cov(\epsilon_2,\epsilon_n|X)\\
\vdots &&\ddots&\\
Cov(\epsilon_n,\epsilon_1|X)&Cov(\epsilon_n,\epsilon_2|X)&\cdots&Var(\epsilon_n|X)\\
\end{pmatrix}
\end{align*}
$$

The explicit form of the symmetric $(n\times n)$ matrix $Var(\epsilon|X)$ depends on our (hopefully correct) assumption on the error-term distribution (Assumption 4). 

In the following, we consider the two most prominent types of assumptions for the symmetric $(n\times n)$ matrix $Var(\epsilon|X)$:

* Spherical, i.e., homoskedastic and uncorrelated errors
* Heteroskedastic and uncorrelated errors

#### Case of Spherical (Homoskedastic and Uncorrelated) Errors 

If 
$$
\begin{align*}
Var(\epsilon|X)
&=
\begin{pmatrix}
\sigma^2 & 0        & \cdots & 0\\
0        & \sigma^2 & \cdots & 0\\
\vdots   & \vdots   & \ddots & 0\\
0        & 0        & \cdots & \sigma^2\\
\end{pmatrix}
=\sigma^2 I_n,
\end{align*}
$$
then 
$$
\begin{align*}
&Var(\hat\beta|X)=\\[2ex]
&=(X'X)^{-1}X' \left(Var(\epsilon|X)\right) X(X'X)^{-1}\\[2ex]
&=(X'X)^{-1}X' \left(\sigma^2 I_n \right) X(X'X)^{-1}\\[2ex]
&=\sigma^2\;(X'X)^{-1}X' \left( I_n \right) X(X'X)^{-1}\\[2ex]
&=\sigma^2\;\underbrace{(X'X)^{-1}X'X}_{I_{p+1}}\;(X'X)^{-1}\\[2ex]
&=\sigma^2\;(X'X)^{-1},
\end{align*}
$$
where the only unknown component is $\sigma^2=Var(\epsilon_i).$ 

We can estimate the homoskedastik error term variance $\sigma^2$ using the **R**esidual **S**tandard **E**rror:
$$
\begin{align*}
\hat\sigma = \operatorname{RSE}
&=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}\\[2ex]
&=\sqrt{ \frac{1}{n-(p+1)} \sum_{i=1}^n e_i^2}.
\end{align*}
$$

**Summing up:** 

In the case of spherical (homoskedastic and uncorrelated) error terms the standard error of $\beta_j$ is
$$
\operatorname{SE}(\hat\beta_j|X) = \sqrt{\left[\sigma^2 \left(X'X\right)^{-1}\right]_{(j,j)}}.
$$
The above expression is infeasible since $\sigma^2$ is typically unknown. We can estimate this infeasible population version using the empirical standard error
$$
\widehat{\operatorname{SE}}(\hat\beta_j|X) = \sqrt{\left[\hat{\sigma}^2 \left(X'X\right)^{-1}\right]_{(j,j)}}.
$$


This is the default version for computing the standard error in statistical software packages such as `R`. 

```{r}
set.seed(123)
n      <- 100                           # Sample size
X_1    <- runif(n, 0, 10)               # Predictor variable X_1
X_2    <- rnorm(n, -5, 2)               # Predictor variable X_2
error  <- rt(n, df = 10)*10             # True (usually unknown) error
Y      <- 1 + 5 * X_1 -5 * X_2 + error  # Y variable
lm_obj <- lm(Y ~ X_1 + X_2)             # OLS regression 

## Standard OLS output table:
summary(lm_obj)                         
```


#### Case of Heteroskedastic and Uncorrelated Errors 


If 
$$
\begin{align*}
Var(\epsilon|X)
&=
\begin{pmatrix}
\sigma_1^2 & 0          & \cdots & 0\\
0          & \sigma_2^2 & \cdots & 0\\
\vdots     & \vdots     & \ddots & 0\\
0          & 0          & \cdots & \sigma_n^2\\
\end{pmatrix}
&=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2),
\end{align*}
$$
then 
$$
\begin{align*}
&Var(\hat\beta|X)=\\[2ex]
&=(X'X)^{-1}X' \left(Var(\epsilon|X)\right) X(X'X)^{-1}\\[2ex]
&=(X'X)^{-1} \left(X'\left(\operatorname{diag}(\sigma_1^2,\dots,\sigma_n^2) \right) X\right) (X'X)^{-1}\\[2ex]
&=(X'X)^{-1} \left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right) (X'X)^{-1}\\[2ex]
\end{align*}
$$
Thus, the symmetric $(p+1)\times(p+1)$ variance-covariance matrix $Var(\hat\beta|X)$ keeps its "sandwich form", where the inner part of the sandwich
$$
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right)
$$
is typically unknown, since $\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2$ are typically unknown. 


There are different, so-called **Heteroskedasticity Consistent (HC)** estimators to estimate the unknown expression 
$$
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right).
$$  


HC-Type  | Formular
------|------------
HC0   | $\sum_{i=1}^ne_i^2X_iX_i'$
HC1   | $\sum_{i=1}^n\frac{n}{n-K}e_i^2X_iX_i'$
HC2   | $\sum_{i=1}^n\frac{e_{i}^{2}}{1-h_{i}}X_iX_i'$
HC3   | $\sum_{i=1}^n\frac{e_{i}^{2}}{\left(1-h_{i}\right)^{2}}X_iX_i'$
HC4   | $\sum_{i=1}^n\frac{e_{i}^{2}}{\left(1-h_{i}\right)^{\delta_{i}}}X_iX_i'$

HC3 is the most often used HC-estimator. 


::: {.callout-tip}
The statistic $h_i$ is simply the $i$th diagonal element of the projection matrix $P_X$
$$
h_i=[P_X]_{ii}
$$ 
and is called the $i$th **leverage statistic**, where 

* $1/n\leq h_i\leq 1$ and 
* $\bar{h}=n^{-1}\sum_{i=1}^nh_i=(p+1)/n$. 

Observations $X_i$ with leverage statistics $h_i$ that greatly exceed the average leverage value $(p+1)/n$ are referred to as "high leverage" observations. High leverage observations $X_i$ are observations that are far away from the predictor values of all other subjects. 

High leverage observations $X_i$ have the potential to distort the estimation results, $\hat\beta_n$. Indeed, a high leverage observation $X_i$ will have an distorting effect on the estimation results if the absolute value of the corresponding residual $|e_i|$ is unusually large---such observations are called **influential outliers**. Such observations increase the estimation uncertainty. 

General idea of the HC2-HC4 estimators is to increase the estimated variance in order to account for the effects of influential outliers. The residuals $e_i$ belonging to $X_i$ values that have a large leverage $h_i$ receive a higher weight and thus increase the value of $\widehat{E}(\epsilon^2_iX_iX_i').$ This strategy takes into account increased estimation uncertainties due to single influential outliers. 
:::


<!-- The estimator HC0 was suggested in the econometrics literature by @White1980 and is justified by asymptotic ($n\to\infty$) arguments. The estimators HC1, HC2 and HC3 were suggested by @MacKinnon_White_1985 to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, @Long_Ervin_2000 concludes that HC3 provides the best overall performance in finite samples. @Cribari_2004 suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large $h_i$ values). 
-->


**Summing up:** 

In the case of heteroskedastic and uncorrelated error terms the standard error of $\beta_j$ is
$$
\operatorname{SE}(\hat\beta_j|X) = \sqrt{\left[(X'X)^{-1} \left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right) (X'X)^{-1}\right]_{(j,j)}}.
$$
The above expression is the infeasible (since $\sigma^2$ is typically unknown) population version of the standard error. We can estimate this population version using the empirical standard error
$$
\widehat{\operatorname{SE}}(\hat\beta_j|X) = \sqrt{\left[(X'X)^{-1} \left(\textsf{HC}\right) (X'X)^{-1}\right]_{(j,j)}},
$$
where $\textsf{HC}$ is a placeholder for one the **H**eteroskedasticity **C**onsistent estimators HC1-HC4 given above.

The `sandwich`-package allows to compute these standard errors in `R` 

```{r}
set.seed(123)
n      <- 100                           # Sample size
X_1    <- runif(n, 0, 10)               # Predictor variable X_1
X_2    <- rnorm(n, -5, 2)               # Predictor variable X_2
error  <- rt(n, df = 10) * abs(X_2)     # True (usually unknown) heteroskedastic error
Y      <- 1 + 5 * X_1 -5 * X_2 + error  # Y variable


## Package for computing robust variance estimations
library("sandwich") # vcovHC(), 

## Package for producing an OLS output table (etc.)
suppressMessages(library("lmtest")) # coeftest

## Estimate the linear regression model parameters
lm_obj      <- lm(Y ~ X_1 + X_2)

vcovHC3_mat <- sandwich::vcovHC(lm_obj, type="HC3")

lmtest::coeftest(lm_obj, vcov = vcovHC3_mat)

## Note: The HC3-Robust SE estimates are: 
round(sqrt(diag(vcovHC3_mat)), digits = 5)
```

## Inference 

### Confidence Intervals for $\beta_j$

Given the estimate $\hat\beta_j$ and the estimate of the standard error 
$$
\widehat{\operatorname{SE}}(\hat\beta_j|X),
$$
we can construct the $(1-\alpha)\cdot 100\%$ confidence interval for the true (unknown) $\beta_j$-parameter: 
$$
\begin{align*}
&\operatorname{CI}^{1-\alpha}_{\beta_j}=\\[2ex]
=&\left[\hat\beta_j - q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j|X),\;
\hat\beta_j + q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j|X)\right]\\[2ex]
&\text{More compact notation:}\\[2ex]
=&\left[\hat\beta_j\; {\color{red}\pm} \; q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j|X)\right],
\end{align*}
$$
where 
$$
q^{t,n-(p+1)}_{1-\alpha/2}
$$
denotes the $(1-\alpha/2)$-quantile of the $t$-distribution with $\operatorname{df}=n-(p+1)$ degrees of freedom, and where $\alpha$ denotes the **significance level** with typical choices: 

* $\alpha = 0.05$ or
* $\alpha = 0.01$


::: {.callout-tip}
# Interpretation 


The confidence interval 
$$
\operatorname{CI}^{1-\alpha}_{\beta_j}
$$ 
is a **random confidence interval**. 

For a given realization of the training data random sample
$$
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{np}))
$$
we **obs**serve a specific realization 
$$
\operatorname{CI}^{1-\alpha}_{\beta_j,obs}.
$$ 


There is a $(1-\alpha)\cdot 100\%$ change (in resamplings from the training data random sample) that the **random confidence interval** 
$$
\operatorname{CI}^{1-\alpha}_{\beta_j}
$$ 
contains the true (fix) parameter value $\beta_j.$ 

To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:

* [Interactive visualization for interpreting confidence intervals](https://rpsychologist.com/d3/ci/)
:::

::: {.callout-caution}
Only the above frequentist point of view can be nicely interpreted.  


A given, observed confidence interval 
$$
\operatorname{CI}^{1-\alpha}_{\beta_j,obs},
$$
either contains the true parameter value or not and usually we do not know it, since we do not know the value of $\beta_j.$  

![](images/CI_meme.jpg)

::: 


### Confidence Intervals for Statistical Hypothesis Testing 

We can use the $(1-\alpha)\cdot 100\%$ confidence interval to do statistical hypothesis testing at the significance level $0<\alpha<1.$ Typical significance levels: 

* $\alpha=0.05$
* $\alpha=0.01$

Let us consider the following null-hypothesis $(H_0)$ that the true (usually unknown) value $\beta_j$ equals the **null-hypothetical value** $\beta^{(H_0)}_{j}$ versus the two-sided alternative hypothesis $(H_1)$ that the true (usually unknown) value $\beta_j$ does **not equal** the null-hypothetical value $\beta^{(H_0)}_{j}:$
$$
\begin{align*}
H_0:&\;\beta_j=\beta^{(H_0)}_{j}\\ 
H_1:&\;\beta_j\neq \beta^{(H_0)}_{j}
\end{align*}
$$

::: {.callout-tip}
# Classic No-Effect Null-Hypothesis
For the special case, where the null-hypothetical value equals zero 
$$
\beta^{(H_0)}_{j}=0
$$ 
we test the classic **no-effect null-hypothesis**.
:::


**Testing-Procedure:**

* If the observed (obs) realization of the confidence interval, $\operatorname{CI}^{1-\alpha}_{\beta_j,obs},$ **contains** the null-hypothetical value $\beta^{(H_0)}_{j},$ i.e.
$$
\begin{align*}
\beta^{(H_0)}_{j}&\in\operatorname{CI}^{1-\alpha}_{\beta_j,obs}\\[2ex]
\Leftrightarrow\beta^{(H_0)}_{j}&\in
\left[
  \hat{\beta}_{j,obs} \;\pm\; q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j)_{obs}
  \right],
\end{align*}
$$
then we **cannot reject the null hypothesis**  $\beta_j=\beta^{(H_0)}_{j}.$

* If, however, the observed (obs) realization of the confidence interval, $\operatorname{CI}^{1-\alpha}_{\beta_j,obs},$ does **not contain** the null-hypothetical value $\beta^{(H_0)}_{j},$ i.e.
$$
\begin{align*}
\beta^{(H_0)}_{j}&\not\in\operatorname{CI}^{1-\alpha}_{\beta_j,obs}\\[2ex]
\Leftrightarrow\beta^{(H_0)}_{j}&\not\in
\left[
  \hat{\beta}_{j,obs} \;\pm\; q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j)_{obs}
  \right],
\end{align*}
$$
then we **reject the null hypothesis** and adopt the alternative  $\beta_j\neq\beta^{(H_0)}_{j}.$



### Test Statistics for Statistical Hypothesis Testing 

Standard errors can also be used to construct test statistics for statistical hypothesis testing. In the following, we look at the $t$-test statistic. 

Choose a significance level $0<\alpha<1$ such as, for instance,

* $\alpha=0.05$ 
* $\alpha=0.01$

Let us (again) consider the null-hypothesis $(H_0)$ that the true (usually unknown) value $\beta_j$ equals the **null-hypothetical value** $\beta^{(H_0)}_{j}$ versus the two-sided alternative hypothesis $(H_1)$ that the true (usually unknown) value $\beta_j$ does not equal the null-hypothetical value $\beta^{(H_0)}_{j}:$
$$
\begin{align*}
H_0:&\;\beta_j=\beta^{(H_0)}_{j}\\[2ex] 
H_1:&\;\beta_j\neq \beta^{(H_0)}_{j}
\end{align*}
$$


The **random** $t$-test statistic is given by 
$$
T=\frac{\hat\beta_j - \beta^{(H_0)}_{j}}{\widehat{\operatorname{SE}}(\hat\beta_j)}
$$

Under the null-hypothesis, $\beta_j=\beta^{(H_0)}_{j},$ the $t$-test statistic is $t$-distributed with $\operatorname{df}=n-(p+1)$ degrees of freedom.
$$
T=\frac{\hat\beta_j - \beta^{(H_0)}_{j}}{\widehat{\operatorname{SE}}(\hat\beta_j)}\overset{H_0}{\sim} t_{n-(p+1)},
$$
where $t_{n-(p+1)}$ denotes the $t$-distribution with $\operatorname{df}=n-(p+1)$ degrees of freedom. 

For a given realization of the training data random sample
$$
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{np}))
$$
we **obs**serve a specific realization of the $t$-test statistic
$$
T_{obs}=\frac{\hat\beta_{j,obs} - \beta^{(H_0)}_{j}}{\widehat{\operatorname{SE}}(\hat\beta_j)_{obs}}
$$ 

#### **$p$-value:** {-}

The $p$-value is the probability of seeing a realization of the **random** $t$-test statistic, $T,$ which is more extreme than the observed value of the test-statistic, $T_{obs},$ given the null-hypothesis is true
$$
\begin{align*}
p_{obs}
& = P\left(|T|\geq|T_{obs}|\;\;|\;\; \text{$H_0$ is true}\right)\\[2ex]
& = 2\cdot\min\{P\left(T\geq T_{obs} \;\;|\;\; \text{$H_0$ is true}\right),\;\\ 
& \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; P\left(T\leq T_{obs} \;\;|\;\; \text{$H_0$ is true}\right)\}.
\end{align*}
$$


**Testing-Procedure:**
<!-- To do the statistical hypothesis test, we need to select a significance level $\alpha$ (e.g.,  $\alpha=0.05$ or $\alpha=0.01$).  -->

* If the observed realization of the $p$-value is larger than or equal to the significance level 
$$
p_{obs}\geq \alpha,
$$
then we **cannot reject the null hypothesis** $\beta_j=\beta^{(H_0)}_{j}.$ 

* If, however, the observed realization of the $p$-value is strictly smaller than the significance level 
$$
p_{obs}<\alpha,
$$
then we **reject the null hypothesis** and adopt the alternative hypothesis $\beta_j\neq \beta^{(H_0)}_{j}.$ 

::: {.callout-note}

# Equivalence of Confidence Intervals and Test-Statistics

It can be shown that a statistical hypothesis test based on the above confidence interval $\operatorname{CI}^{1-\alpha}_{\beta_j,obs}$ leads to exactly the same test decisions as a statistical hypothesis test based on the above $t$-test statistic $T_{obs}.$
::: 





### Probability of a Type I Error, Power and Consistency


Every statistical testing procedure (conducted using confidence intervals or test-statistics) involves the decision of 

* not rejecting $H_0$ 

versus 

* rejecting $H_0$

In applied research, we "aim" for rejecting $H_0.$ For instance, when we are able to reject the no-effect null-hypothesis 
$$
H_0: \beta_j = 0
$$
and thus able to adopt the alternative 
$$
H_1: \beta_j \neq 0,
$$
then we can state in a publication that variable $X_j$ has an effect.  

<!-- Cautions: 

1. If we cannot reject the null-hypothesis, A too strong desire for rejecting $H_0$ is the reason of a lot of scientific misconduct.  -->


If we reject the null-hypothesis even though the null-hypothesis is true, we conduct a **Type I Error** and thus may falsely claim that variable $X_j$ has an effect. Such a false claim can be fatal.  

Therefore, a statistical hypothesis test is only valid if it is able control the probability of conducting a type I error from above by the chosen significance level; i.e. if
$$
\underbrace{P(\text{reject }H_0 \;|\; H_0\text{ is true})}_{\text{Probability of a type I error}}\leq \alpha,
$$
where $\alpha$ is some small value like $\alpha = 0.05$ or $\alpha = 0.01.$ I.e., in $100$ resamples we expect to see at most $\alpha\cdot 100$ false rejections of $H_0.$


By choosing a small significance level like $\alpha = 0.05$ or $\alpha = 0.01,$ we make sure that we can be quite "sure" that we do not falsely reject $H_0.$


::: {.callout-caution}
A statistical testing procedure only controls the probability of a type I error (falsely rejecting $H_0,$ when $H_0$ is true), 
$$
\underbrace{P(\text{reject }H_0 \;|\; H_0\text{ is true})}_{\text{Probability of a type I error}}\leq \alpha,
$$
but not the probability of a type II error (falsely not rejecting $H_0,$ when $H_1$ is true)
$$
\underbrace{P(\text{not reject }H_0 \;|\; H_1\text{ is true})}_{\text{Probability of a type II error}}\leq \;\; {\color{red}?}
$$  

Therefore, when we cannot reject the null-hypothesis we cannot claim that the null-hypothesis is probably true, since we do not know wether the probability of a type II error (falsely not rejecting $H_0,$ when $H_1$ is true) is small. 

Indeed, even very large $p$-values $p_{obs}\approx 1 \gg \alpha$ can occur simply because a violation of a null-hypothesis is smaller than the involved estimation errors.  

The $p$-value is **not** the probability that the null-hypothesis is trueâ—

![](images/terminator.jpeg)
:::



Under the alternative, i.e., if for instance  
$$
H_1: \beta_j \neq 0
$$
is true, we want to be able to reject $H_0$ with a large as possible probability. The probability of rejecting a false null-hypothesis is called **Power**
$$
\underbrace{P(\text{reject }H_0 \;|\; H_1\text{ is true})}_{\text{Power}}
$$
We want that
$$
\underbrace{P(\text{reject }H_0 \;|\; H_1\text{ is true})}_{\text{Power}} \to 1\quad\text{as}\quad |\beta_j - 0| \to \infty
$$
for each given sample size $n,$ and that 
$$
\underbrace{P(\text{reject }H_0 \;|\; H_1\text{ is true})}_{\text{Power}} \to 1\quad\text{as}\quad n \to \infty
$$
for each given violation of the null-hypothesis $|\beta_j - 0|>0.$

A testing procedures that fulfills the latter property is is called **consistent.**




## Exercises

Prepare the following exercises of Chapter 3 in our course textbook `ISLR`: 

- Exercise 1
- Exercise 2
- Exercise 3
- Exercise 8
- Exercise 9

<!-- {{< include Ch4_LinearRegression_Solutions.qmd >}} -->