<!-- LTeX: language=en-US -->
# Parametric vs. Nonparametric Regression

The linear regression model as considered in @sec-linearRegCh is an example of a **parametric** regression model because it parametrizes the general regression model 
$$
Y_i = f(X_i) + \epsilon_i 
$$
using a linear model assumption (Assumption 1 in @sec-linearRegCh), such that
$$
f(X_i)=\beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}.
$$

**Advantages of parametric approaches:**

* Typically easy to fit 
* Simple interpretation
* Simple inference


**Disadvantages of parametric approaches:**

* The parametric model assumption can be far from true; i.e.
$$
f(X_i) \neq \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}
$$


**Alternative:** **Non-parametric methods** such as **K-nearest neighbors regression** since non-parametric approaches do not explicitly assume a parametric form for $f(X).$


## K-Nearest Neighbors (K-NN) Regression 

Let 
$$
Y_i = f(X_i) + \epsilon_i 
$$
denote the **general regression model.** 


In the following, we do not assume a certain parametric model form for $f(x),$ but only make the **qualitative assumption** that $f$ is a sufficiently **smooth** function, such that 
$$
|f(x_1)-f(x_2)|\approx 0\quad\text{if}\quad d(x_1,x_2)\approx 0,
$$
where $d(x_1,x_2)$ measures the distance (e.g. $d(x_1,x_2)=||x_1-x_2||$) between the points $x_1$ and $x_2.$


Let $x_0\in\mathbb{R}^p$ denote a certain (multivariate) predictor value at which we want to estimate 
$$
f(x_0).
$$

**KNN regression** estimates $f(x_0)$ by 
$$
\hat{f}_K(x_0),
$$ 
where $\hat{f}_K(x_0)$ is computed in two steps: 

1. Compute the distances between $x_0$ and all training data predictor values $X_1,\dots,X_{n_{Train}}$
$$
d(x_0,X_1),d(x_0,X_2)\dots,d(x_0,X_{n_{Train}}).
$$
Use these distances to identify the $K$ training data predictor values $X_1,\dots,X_{n_{Train}}$ that are closest to $x_0$ and collect their indices the index set $\mathcal{N}_0,$ where
$$
\begin{align*}
\mathcal{N}_0 
& =\{i\in\{1,2,\dots,n_{Train}\} \; |\; d(x_0,X_i)\text{ is one of the $K$ smallest distances}\}
\end{align*}
$$ 
such that 
     * $\mathcal{N}_0\subset\{1,2,\dots,n_{Train}\}$ 
     * $|\mathcal{N}_0|=K$ (number of elements in $\mathcal{N}_0$)
2. Estimate $f(x_0)$ using the sample average of all the training responses $Y_i$ with $i\in\mathcal{N}_0,$ i.e. 
$$
\hat{f}_K(x_0)=\frac{1}{K}\sum_{i\in\mathcal{N}_0}Y_i.
$$

The above two steps are then repeated for all predictor values $x_0\in\mathbb{R}^p$ of interest.


The performance of the estimator $\hat{f}_K(x_0)$ depends on 

* the choice of $K$ and 
* the choice of distance $d$ 

For real valued predictors, $X_i\in\mathbb{R}^p$ a usual choice is the **Euclidian distance** 
$$
d_E(x_0, X_i) = ||x_0 - X_i||^2 = \sum_{j=1}^p (x_{0j} - X_{ij})^2.
$$

::: {.callout-important}
# Use Standardized Predictors! 
Typically, it is important to compute the distances with respect to the **standardized** (centering, and scaling to unit variance) predictor variables; i.e.
$$
d^*_E(x_0, X_i) = ||x^*_0 - X^*_i||^2 = \sum_{j=1}^p (x^*_{0j} - X^*_{ij})^2,
$$
where 
$$
x^*_{0j} = \frac{x_{0j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
$$ 
and 
$$
X^*_{ij} = \frac{X_{ij} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
$$ 
with $\bar{X}_{j} = \frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}X_{ij}.$

Otherwise, the distance values could be dominated by one of the $p$ predictors. 

E.g. when one predictor is age (values between $0$ and $99$) and another predictor is yearly income (values between $0$ and $12,000,000$), then the differences in income will dominate the differences in age only because of the different scales.
<!-- 
$$
\frac{X_{1j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}},\dots,\frac{X_{n_{Train}j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
$$  -->
:::

The problem is now, to find the optimal value for $K.$ 

**Idea:** Choose $K$ by minimizing the mean squared (prediction) error.


## Local Mean Squared (Prediction) Error (MSE) {#sec-mqfit}

A fair and reliable assessment of the model accuracy requires **testing data**, i.e., data which comes from the same data generating process as the **training data**, but which was not used to compute (train) the estimator. 


<!-- In fact, a very flexible (e.g. non-parametric) estimation method will tend to overfit the training data such that $y_i\approx \hat{f}(x_i)$ for all $i=1,\dots,n$ resulting in a training MSE that is close to zero since $\hat{f}(x_i)$ fits also the errors $\epsilon_i.$ -->


<!-- **Example:** Suppose that we are interested in developing an algorithm to predict a stockâ€™s price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don't really care how well our method predicts last week's stock price. We instead care about how well it will predict tomorrow's price
or next month's price.  -->

<!-- **Example:** Suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements.  -->

<!-- In general, however, we do not really care how well the method works on the training data. We are interested in the accuracy of the predictions that we obtain when we apply our method to **previously unseen test data**. 

Thus, we want to choose the method that gives the **lowest *test* MSE**, as opposed to the lowest *training* MSE.  -->


#### **Local Test Data MSE** {-}

Let $\hat{f}$ be computed from the training data $\{(X_1,Y_1),\dots,(X_n,Y_{n_{Train}})\}.$ And let 
$$
\{(x_{0},Y^{Test}_{1}),(x_{0},Y^{Test}_{2})\dots,(x_{0},Y^{Test}_{n_{Test}})\}
$$
denote a specific set of $n_{Test}$ **test data points** $Y^{Test}_{1},\dots,Y^{Test}_{n_{Test}}$ for a **specific predictor value** $x_0.$ 


::: {.callout-tip}
This type of $x_0$-specific test data is a realization of a **conditional random sample** given $X=x_0,$
$$
(x_{0},Y^{Test}_{i})\overset{\text{iid}}{\sim}(X,Y)|X=x_0,\quad i=1,\dots,n_{Test}.
$$ 
I.e, the test data $(x_{0},Y^{Test}_{i}),$ is generated using iid realization from 
$$
Y^{Test}_{i} = f(x_0) +  \epsilon^{Test}_{i}, \quad i=1,\dots,n_{Test}.
$$
This test data random sample is independent of the training data random sample whose realization was used to compute $\hat{f}.$
:::

Then, the **point-wise (local) test MSE** at $X=x_0$ is given by,
\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}(x_0)= \frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y^{Test}_{i} - \hat{f}(x_{0})\right)^2.
\end{align*}



This **local test MSE** is an **estimator** of the population version of the local Mean Squared (Prediction) Error of $\hat{f}$
$$
\begin{align*}
\operatorname{MSE}(x_0) 
&= E\left[(Y - \hat{Y})^2|X=x_0\right]\\
&= E\left[(Y - \hat{f}(X))^2|X=x_0\right].
\end{align*}
$$

**KNN-Regression**: 

For the KNN-regression estimator $\hat{f}_K(x_{0})$ we have that

\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}(x_0,K)= \frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y^{Test}_{i} - \hat{f}_K(x_{0})\right)^2.
\end{align*}

which estimates the population counterpart
\begin{align*}
\operatorname{MSE}(x_0,K) 
&= E\left[(Y - \hat{f}_K(X))^2|X=x_0\right].
\end{align*}


Generally, we'll find a minimum of $\operatorname{MSE}(x_0,K)$ with respect to the **smoothing parameter** $K$. 


The minimum of $\operatorname{MSE}(x_0,K)$ finds the optimal compromize between the squared bias of $\hat{f}(x_0)$ and the variance of $\hat{f}(x_0).$


### The Local Bias-Variance Trade-Off

One can show that 
\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}(x_0)= \frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y^{Test}_{i} - \hat{f}(x_{0})\right)^2.
\end{align*}
is an **unbiased estimator** of the true (unknown) Mean Squared (Prediction) Error of $\hat{f}(x_0)$
$$
\begin{align*}
\operatorname{MSE}(x_0) 
& = E\left[(Y - \hat{Y})^2|X=x_0\right]\\
&=\underbrace{E\left[\left(f(x_0)-\hat{f}(x_0)\right)^2\right]+\sigma^2}_{\text{Mean Squared Prediction Error of $\hat{f}(x_0)$}},
\end{align*}
$$ 
i.e., that
$$
E\left[\widehat{\operatorname{MSE}}_{test}(x_0)\right]=\underbrace{E\left[\left(f(x_0)-\hat{f}(x_0)\right)^2\right]+\sigma^2}_{=\operatorname{MSE}(x_0)}.
$${#eq-MSEUnbiased}


::: {.callout collapse="true"}

# Proof of @eq-MSEUnbiased:
$$
\begin{align*}
E\left[\widehat{\operatorname{MSE}}_{Test}(x_0)\right] 
& =E\left[\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i}^{Test}- \hat{f}(x_0)\right)^2\right]\\[2ex]
& \text{By the linearity of $E()$:}\\[2ex]
& =\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}E\left[\left(Y_{i}^{Test}- \hat{f}(x_0)\right)^2\right]\\[2ex]
%& =\frac{1}{n_{Test}}\,\sum_{i=1}^{n_{Test}}\,E\left[\left(Y_{1}^{Test}- \hat{f}(x_0)\right)^2\right]\\[2ex]
& \text{Using that $Y_{i}^{Test}$ and $\hat{f}$ are iid across $i=1,\dots,n_{Test}$:}\\[2ex]
& =\frac{1}{n_{Test}}\,E\left[\left(Y^{Test}- \hat{f}(x_0)\right)^2\right]\,\sum_{i=1}^{n_{Test}} 1\\[2ex]
& =\frac{1}{n_{Test}}\,n_{Test}\,E\left[\left(Y^{Test}- \hat{f}(x_0)\right)^2\right]\\[2ex]
& =E\left[\left(Y^{Test}- \hat{f}(x_0)\right)^2\right]\\[2ex]
& \text{Using that}\;Y^{Test}=f(x_0)+\epsilon^{Test}\\[2ex]
& =E\left[\left(f(x_0) + \epsilon^{Test} - \hat{f}(x_0)\right)^2\right]\\[2ex]
& =E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2 +2\left(f(x_0)- \hat{f}(x_0)\right)\epsilon^{Test} + (\epsilon^{Test})^2 \right]\\[2ex]
& =E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2\right]\\[2ex] 
&+ \underbrace{2E\left[\left(f(x_0)- \hat{f}(x_0)\right)\right]\overbrace{E\left[\epsilon^{Test}\right]}^{=0}}_{\text{using independence between training (in $\hat{f}$) and testing data}}\\[2ex] 
&+ \underbrace{E\left[(\epsilon^{Test})^2 \right]}_{=Var(\epsilon^{Test})}\\[2ex]
& =\underbrace{E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2\right]}_{\text{Mean Squared Estimation Error of $\hat{f}(x_0)$}}+0+Var(\epsilon^{Test})\\[2ex] 
& =\underbrace{E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2\right]}_{\text{reducable}}+\overbrace{\underbrace{Var(\epsilon^{Test})}_{\text{irreducable}}}^{=\sigma^2}\\[2ex] 
\end{align*}
$$
:::


<!-- The **expected MSE** at $x_0,$ 
$$
E\left[\operatorname{MSE}_{test}(x_0)\right],
$$ 
refers to the average test MSE that we would obtain if we repeatedly estimated $f$ using training data set, and evaluated each at $x_0.$  
-->

<!-- 
::: {.callout-note}
A computed value of $\operatorname{MSE}_{test}(x_0)$ (as done in the coding challenge) is not able to consistently approximate $E\left[\operatorname{MSE}_{test}(x_0)\right].$

However, to get information about Bias and Variance of a method, we need to approximate $E\left[\operatorname{MSE}_{test}(x_0)\right].$ This will be (among others) the topic of  @sec-resamplingmethods.
::: 
-->


The mean squared estimation error of $\hat{f}(x_0)$ can be **decomposed** into a **variance** component and a **squared bias** component, i.e.
$$
E\left[\left(f(x_0) - \hat{f}(x_0)\right)^2\right] = 
Var\left(\hat{f}(x_0)\right) + \left[\operatorname{Bias}\left(\hat{f}(x_0)\right)\right]^2.
$${#eq-MSEBiasVar}

::: {.callout collapse="true"}

# Proof of @eq-MSEBiasVar:

<br>

$$
\begin{align*}
&   E\left[\left(f(x_0) - \hat{f}(x_0)\right)^2\right]\\[2ex]
& = E\left[\left(\hat{f}(x_0) - f(x_0)\right)^2\right]\\[2ex]
&\text{Adding $0=E[\hat{f}(x_0)] - E[\hat{f}(x_0)]$ yields}\\[2ex]
& = E\left[\left(\left\{\hat{f}(x_0) - E[\hat{f}(x_0)]\right\} - \left\{ f(x_0)- E[\hat{f}(x_0)]\right\}\right)^2\right]\\[2ex]
& = \overbrace{E\left[\left\{\hat{f}(x_0) - E[\hat{f}(x_0)]\right\}^2\right]}^{=Var\left(\hat{f}(x_0)\right)} + \overbrace{E\left[\left\{ f(x_0)- E[\hat{f}(x_0)]\right\}^2\right]}^{=\left\{E[\hat{f}(x_0)] - f(x_0) \right\}^2}\\[2ex]
& \;\; -2\;\; \underbrace{E\left[\left\{\hat{f}(x_0) - E[\hat{f}(x_0)]\right\} \cdot 
                 \left\{ f(x_0)- E[\hat{f}(x_0)]\right\}\right]}_{=E\left[\hat{f}(x_0)f(x_0)
                                                                         -\hat{f}(x_0)E\left[\hat{f}(x_0)\right]
                                                                         -     f(x_0) E\left[\hat{f}(x_0)\right]
                                                                         +\left(E\left[\hat{f}(x_0)\right]\right)^2\right]}\\[2ex]
& = Var\left(\hat{f}(x_0)\right) + \Big\{\;\overbrace{E[\hat{f}(x_0)] - f(x_0)}^{=\operatorname{Bias}\left(\hat{f}(x_0)\right)}\; \Big\}^2\\[2ex]
& \;\; -2\;\; \underbrace{E\left[\hat{f}(x_0)f(x_0) -\hat{f}(x_0)E\left[\hat{f}(x_0)\right]
                                                                         -     f(x_0) E\left[\hat{f}(x_0)\right]
                                                                         +\left(E\left[\hat{f}(x_0)\right]\right)^2\right]}_{= 0
                                                                         }\\[2ex]                  
% = E\left[\hat{f}(x_0)\right]f(x_0) - \left(E\left[\hat{f}(x_0)\right]\right)^2 - f(x_0) E\left[\hat{f}(x_0)\right]+\left(E\left[\hat{f}(x_0)\right]\right)^2                                                                          
& = \underbrace{Var\left(\hat{f}(x_0)\right) + \left[\operatorname{Bias}\left(\hat{f}(x_0)\right)\right]^2}_{\text{reducible}} 
\end{align*}
$$
:::

#### **Variance of $\hat{f}$ at $x_0$** {-}
$$
Var(\hat{f}(x_0))=E\left[\left(\hat{f}(x_0) - E\left[\hat{f}(x_0)\right]\right)^2\right]
$$

::: {.callout-tip}
# Variance
The variance of $\hat{f}$ at $x_0$ refers to the amount by which $\hat{f}(x_0)$ would change if we estimated it using a different training data set. Generally, different training data sets will result in a different $\hat{f}(x_0).$ Ideally the estimate for $f$ should not vary too much between training sets. If a method has high variance then small changes in the training data can result in large changes in $\hat{f}(x_0).$ 

ðŸ¤“ In general, more flexible statistical methods (e.g., KNN-regression with small $K$) have higher variance---and vice versa.
:::



#### **Bias of $\hat{f}$ at $x_0$** {-}
$$
\operatorname{Bias}(\hat{f}(x_0))=E\left[\hat{f}(x_0)\right] - f(x_0)
$$

::: {.callout-tip}
# Bias
The bias of $\hat{f}$ at $x_0$ refers to the error that is introduced by approximating $f(x_0)$ using a **nonparametric** estimation approach. 

ðŸ¤“ In general, more flexible statistical methods (e.g., KNN-regression with small $K$) have smaller bias---and vice versa. 
:::

Note that 
$$
Var\left(\hat{f}(x_0)\right)\geq 0
$$ 
and that 
$$
\left[\operatorname{Bias}\left(\hat{f}(x_0)\right)\right]^2\geq 0.
$$
Thus, the expected test MSE can never lie below of $Var(\epsilon),$ i.e.
$$
\begin{align*}
E\left[\widehat{\operatorname{MSE}}_{test}(x_0)\right] 
&=\operatorname{MSE}(x_0) \\
&=E\left[\left(f(x_0)-\hat{f}(x_0)\right)^2\right] + \sigma^2\\
&=Var\left(\hat{f}(x_0)\right)+\left[\operatorname{Bias}\left(\hat{f}(x_0)\right)\right]^2+ \sigma^2\\
& \geq \sigma^2 = Var\left(\epsilon\right).
\end{align*}
$$


### Local MSE, Variance, and Bias of KNN-Regression

The left panel of Figure 3.16 shows the estimation result for $K=1$ and the right panel for $K=9.$


**Case $K=1:$** KNN-regression **interpolates** all the (yellow) training data points.


**Case $K=9:$** KNN-regression **smoothes** the (yellow) training data points.


![](images/Fig_3_16.png)


**A small value for $K$** provides a very flexible fit, which will have 

   * small bias 
    $$
    |\operatorname{Bias}(\hat{f}_K(x_0))| = |E(\hat{f}_K(x_0)) - f(x_0)| =\;\text{small}
    $$
   * large variance
    $$
    Var(\hat{f}_K(x_0)) = \text{large}
    $$ 



::: {.callout-tip}

# Small $K$

The **small bias** is due to the fact that the estimator $\hat{f}_K(x_0)$ only uses a few ($K$ small) and thus **very close** and thus **very good** training data neighbors with 
$$
|f(x_0) - f(X_i^{Train})|\approx 0\quad\text{since}\quad d(x_0, X_i^{Train})\approx 0.
$$
Thus, if we were able to average multiple estimation results $\hat{f}(x_0)$ computed from many different (independent) training data sets, then this average would be close to $f(x_0),$ i.e. $E(\hat{f}(x_0))\approx f(x_0)$ (small bias). 


The **large variance** is due to the fact that the estimator $\hat{f}_K(x_0)$ depends on a small number of $K$ training data points such that the law of larger numbers had no chance to reduce variance yet. Thus the estimation result $\hat{f}(x_0)$ would change a lot if we re-estimated it using a different training data set.
:::


**A large value of $K$** provides a less flexible fit, which will have 

   * large bias
    $$
    |\operatorname{Bias}(\hat{f}_K(x_0))| = |E(\hat{f}_K(x_0)) - f(x_0)| =\;\text{large}
    $$ 
   * small variance
    $$
    Var(\hat{f}_K(x_0)) = \;\text{small}
    $$ 


::: {.callout-tip}

# Large $K$

The **large bias** is due to the fact that the estimator $\hat{f}_K(x_0)$ also uses **more distant** and thus **rather bad** training data neighbors with 
$$
|f(x_0) - f(X_i^{Train})|\not\approx 0\quad\text{since}\quad d(x_0, X_i^{Train})\not\approx 0.
$$
Thus, if we were able to average multiple estimation results $\hat{f}(x_0)$ computed from many different (independent) training data sets, then this average may not be close to $f(x_0),$ i.e. $E(\hat{f}(x_0))\not \approx f(x_0)$ (large bias, potentially). 


The **small variance** is due to the fact that the estimator $\hat{f}_K(x_0)$ depends on a larger number of $K$ traing data points such that the law of larger numbers has a chance to reduce variance. Thus the estimation result $\hat{f}(x_0)$ would not change a lot if we re-estimated it using a different training data set.
:::

#### **Locally Optimal Smoothing Parameter $K$** {-}

**An optimal value of $K$** can be chosen **locally** for every $x_0$ of interest by choosing that value of $K$ that **minimizes** 

\begin{align*}
\widehat{\operatorname{MSE}}_{Test}(x_0,K)
& =\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i}^{Test}- \hat{f}_K(x_0)\right)^2
\end{align*}
with respect to $K=1,2,\dots.$



## In Class Coding Exercises 

Consider the following multiple linear regression model:
\begin{align*}
Y_i 
& = f(x)+ \epsilon_i\\ 
& = 5+ 3\sin(2\pi x) + \epsilon_i,
\end{align*}
where the training data has a sample size of $n_{Train}=100,$ the error term is iid normal 
$$
\epsilon_i\overset{\text{iid}}{\sim}\mathcal{N}(0,1),
$$
and the predictor values are deterministic and equidistant, i.e.,
$$
X_i=\frac{i}{n},\quad i=1,\dots,n_{Train}.
$$

**Problems:**

(a) Programm KNN-Regression Estimator $\hat{f}_K(x_0).$ Compute the KNN-Regression estimation $\hat{f}_K(x_0)$ for $K=5$ and $x0 = 0.2.$ Compare the result with the true regression function value $f(0.2).$

(b) Compute the KNN-Regression estimation for $K=5$ and a *dense* $x$-grid between 0 and 1 (e.g. `seq(0,1,len=500)`) and compare the results with the true regression function values using a plot.

(c) Generate $200$ estimation results $\hat{f}_K(x_0)$ for $x0 = 0.2$ using $200$ independent training data set. Plot the results along with their sample means and the true $f(0.2)$ value using plots for different values of $K=1,\dots,K.$ 


## Under Construction



## Assessing Model Accuracy of Nonparametric Regression

A fair and reliable assessment of the model accuracy requires **testing data**, i.e., data which comes from the same data generating process as the **training data**, but which was not used to compute (train) the estimator. 

Let 
$$
\{(X_{1}^{Test},Y_{1}^{Test}),(X_{2}^{Test},Y_{2}^{Test}),\dots,(X_{n_{Test}}^{Test},Y_{n_{Test}}^{Test})\},
$${#eq-trainingsample}
denote the **test data random sample**, where 
$$
(X_{i}^{Test},Y_{i}^{Test})\overset{\text{iid}}{\sim}(X,Y),\quad i=1,\dots,n_{Test}.
$$ 
with $(X,Y)$ being defined by the general regression model 
$$
Y=f(X)+\epsilon.
$$ 


That is, the new test data is a **random sample**, which ...

1. is independent of the training data random sample 
2. has the same distribution as the training data random sample

The observed realization 
$$
\{(X_{1,obs}^{Test},Y_{1,obs}^{Test}),(X_{2,obs}^{Test},Y_{2,obs}^{Test}),\dots,(X_{n_{Test},obs}^{Test},Y_{n_{Test},obs}^{Test})\},
$$
of the test data random sample is used to check the accuracy of the estimate $\hat{f}.$ 

::: {.callout-important}
In the following, we will often supress the subscript "obs", since often both points of view, the random variables points of view, and the observed realizations points of view make sense.
:::


#### **Global Training Data MSE** {-}

A commonly used measure for the model fit is the mean squared (prediction) error (MSE). 

The global **training data MSE** is given by
\begin{align*}
\widehat{\operatorname{MSE}}_{\text{train}}=\frac{1}{n_{Train}}\sum_{i=1}^n\left(Y_i - \hat{f}(X_i)\right)^2,
\end{align*}
where 

* $\hat{f}$ is computed from the training data
* $\hat{f}(x_i)$ is the prediction that $\hat{f}$ gives for the $i$th training data observation. 


In general, however, we do not really care how well the method works on the training data. We are interested in the accuracy of the predictions that we obtain when we apply our method to **previously unseen test data**. 

Thus, we want to choose the method that gives the **lowest *test* MSE**, as opposed to the lowest *training* MSE. 



#### **Global Test Data MSE** {-}


Typically, we do not have access to such $x_0$-specific test data. 


Moreover, we typically we want a **global** measure of fit, i.e. for all predictor values in the range of $X.$

Let 
$$
\{(X^{Test}_{1},Y^{Test}_{1}),(X^{Test}_{2},Y^{Test}_{2})\dots,(X_{n_{Test}},Y_{n_{Test}}^{Test})\}
$$
denote the **test data** with different predictor values $X_{1}^{Test},\dots,X_{n_{Test}}^{Test}.$ 

::: {.callout-tip}
This type of test data is a realization of a random sample 
$$
(X^{Test}_{i},Y^{Test}_{i})\overset{\text{iid}}{\sim}(X,Y),\quad i=1,\dots,n_{Test}.
$$ 
This test data random sample is independent of the training data random sample whose realization was used to compute $\hat{f}.$
:::


Then, the **global test MSE** is given by,
$$
\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}=\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i}^{Test} - \hat{f}(X_{i}^{Test})\right)^2.
\end{align*}
$$

The **global test MSE** is an **estimator** of the population version of the global Mean Squared (Prediction) Error of $\hat{f}$
$$
\begin{align*}
\operatorname{MSE} 
&= E\left[(Y - \hat{Y})^2\right]\\
&= E\left[(Y - \hat{f}(X))^2\right].
\end{align*}
$$





### The Global Bias-Variance Trade-Off

using, e.g., cross-validation; see @sec-resamplingmethods. 




The overall, i.e., **global MSE**  can be computed by averaging the local $\operatorname{MSE}(x_0)$ over all possible values of $x_0,$ i.e.
$$
\operatorname{MSE} = E(\operatorname{MSE}(X)) = \int f_X(x)\operatorname{MSE}(x) dx,
$$
where $f_X$ denotes the design density of the predicors $X.$


We can **estimate** the global MSE using 
$$
\widehat{\operatorname{MSE}}_{test} = \frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i}^{Test}-\hat{f}(X_{i}^{Test})\right)^2.
$$






Generally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of $f$ and vice versa. 


**Figure 3.17** provides an example with data generated from a one-dimensional linear regression model: 

* black solid lines: true $f(x)$
* blue curves: KNN fits $\hat{f}_K(x)$ using $K = 1$ (left plot) and $K = 9$ (right plot). 

Observations: 

* The KNN fit $\hat{f}_K(x)$ using $K = 1$ is far too wiggly
* The KNN fit $\hat{f}_K(x)$ using $K = 9$ is much closer to the true $f(X).$ 

However, since the true regression function is here linear, it is hard for a non-parametric approach to compete with simple linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. 
![](images/Fig_3_17.png)


The blue dashed line in the left-hand panel of **Figure 3.18** represents the simple linear regression fit to the same data. It is almost perfect. 

The right-hand panel of **Figure 3.18** reveals that linear regression outperforms KNN for this data across different choices of $K=1,2,\dots,10.$ 
![](images/Fig_3_18.png)


**Figure 3.19** displays a non-linear situations in which KNN performs much better than simple linear regression. 
![](images/Fig_3_19.png)


#### Curse of Dimensionality {-}

Unfortunately, in higher dimensions, KNN often performs worse than simple/multiple linear regression, since non-parametric approaches suffer from the **curse of dimensionality**. 


**Figure 3.20** considers the same strongly non-linear situation as in the second row of **Figure 3.19**, except that we have added additional noise (i.e. redundant) predictors that are not associated with the response. 

* When $p = 1$ or $p = 2,$ KNN outperforms linear regression. 
* But for $p = 3$ the results are mixed, and for $p\geq 4$ linear regression is superior to KNN. 
![](images/Fig_3_20.png)

Observations: 

* When $p=1$, a sample size of $n=50$ can provide enough information to estimate $f(X)$ accurately using non-parametric methods since the $K$ nearest neighbors can actually be close to a given test observation $x_0.$ 
* However, when spreading the $n=50$ data points over a large number of, for instance, $p=20$ dimensions, the $K$ nearest neighbors tend to become far away from $x_0$ causing a large bias.  










## Under Revision
















### Population Version of the Mean Squared (Prediction) Error 

Consider a **given** estimate $\hat{f}$ and a **given** predictor $X,$ which yields a **given** prediction $\hat{Y} = \hat{f}(X).$ That is, assume for a moment that both $\hat{f}$ and $X$ are **fixed**, so that the only variability comes from $\epsilon.$ Then, it is easy to show that
$$
\begin{align*}
\overbrace{E\left[(Y - \hat{Y})^2\right]}^{\text{Mean Squared (Prediction) Error}}
=\underbrace{\left(f(X) -\hat{f}(X)\right)^2}_{\text{reducable}} + \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}}, 
\end{align*}
$${#eq-MSEDecompFixed}
where 

* $E\left[(Y - \hat{Y})^2\right]$ represents the expected value, of the squared difference between the predicted $\hat{Y}=\hat{f}(X)$ and actual value of $Y,$ 
* and $Var(\epsilon)$ represents the variance associated with the error term $\epsilon.$


Derivation of @eq-MSEDecompFixed for a **given** $\hat{f}$ and a **given** $X;$ i.e. only $\epsilon$ is random: 
$$
\begin{align*}
&E\left[(Y - \hat{Y})^2\right]\\[2ex] 
&\text{[using $Y=f(X)+\epsilon$]}\\[2ex]
&=E\left[(f(X) + \epsilon - \hat{f}(X))^2\right]\\[2ex]
&=E\left[((f(X)- \hat{f}(X)) + \epsilon )^2\right]\\[2ex]
&\text{[binomial formula]}\\[2ex]
&=E\left[\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right)\epsilon + \epsilon^2\right]\\[2ex] 
&\text{[using that $X$ and $\hat{f}$ are fixed]}\\[2ex]
% &=E\left[\left(f(X) -\hat{f}(X)\right)^2\right] - 2E\left[\left(f(X) -\hat{f}(X)\right)\epsilon\right] + E\left[\epsilon^2\right] \\[2ex] 
&=\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right) E\left[\epsilon\right] + E\left[\epsilon^2\right] \\[2ex] 
&\text{[using that $E(\epsilon)=0$ and $E(\epsilon^2)=Var(\epsilon)=\sigma^2$]}\\[2ex]
&=\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right) \cdot 0 + Var\left(\epsilon\right) \\[2ex]
&=\underbrace{\left(f(X) -\hat{f}(X)\right)^2}_{\text{reducable}} + \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}}
\end{align*}
$$

Thus, the variance of the irreducible prediction error equals the **lowest possible value** of the mean squared (prediction) error, i.e. 
$$
E\left[(Y - \hat{Y})^2\right]\geq Var\left(\epsilon\right). 
$$


The same can be done for the local version of the MSE: 
$$
\begin{align*}
&E\left[(Y - \hat{Y})^2|X=x_0\right]\\[2ex] 
&\text{[using $Y=f(X)+\epsilon$]}\\[2ex]
&=E\left[(f(X) + \epsilon - \hat{f}(X))^2\right]\\[2ex]
&=E\left[((f(X)- \hat{f}(X)) + \epsilon )^2\right]\\[2ex]
&\text{[binomial formula]}\\[2ex]
&=E\left[\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right)\epsilon + \epsilon^2\right]\\[2ex] 
&\text{[using that $X$ and $\hat{f}$ are fixed]}\\[2ex]
% &=E\left[\left(f(X) -\hat{f}(X)\right)^2\right] - 2E\left[\left(f(X) -\hat{f}(X)\right)\epsilon\right] + E\left[\epsilon^2\right] \\[2ex] 
&=\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right) E\left[\epsilon\right] + E\left[\epsilon^2\right] \\[2ex] 
&\text{[using that $E(\epsilon)=0$ and $E(\epsilon^2)=Var(\epsilon)=\sigma^2$]}\\[2ex]
&=\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right) \cdot 0 + Var\left(\epsilon\right) \\[2ex]
&=\underbrace{\left(f(X) -\hat{f}(X)\right)^2}_{\text{reducable}} + \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}}
\end{align*}
$$




For a really good estimate $\hat{f}$ of $f,$ we expect that $\hat{f}(X)\approx f(X)$ such that
$$
E\left[(Y - \hat{Y})^2\right] \approx \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}}.
$$
For a bad estimate $\hat{f}$ of $f,$ however, we expect that  
$$
E\left[(Y - \hat{Y})^2\right] \gg \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}}.
$$

Connecting the **global test MSE** with the population MSE:


The unknown MSE $E\left[(Y - \hat{Y})^2\right]$ can be estimated using the **global test MSE:**
$$
E\left[(Y - \hat{Y})^2\right] \approx \widehat{\operatorname{MSE}}_{\text{test}}.
$$

Thus, if $\hat{f}$ is a really good estimate of $f,$ i.e. if $\hat{f}(X_i^{Test})\approx f(X_i^{Test}),$ then 
$$
\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}
&=\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i} - \hat{f}(X_{i}^{Test})\right)^2\\
&\approx\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i}^{Test} - f(X_{i}^{Test})\right)^2\\
&=\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}(\epsilon_{i}^{Test})^2\\
&=\hat{\sigma}^2\\ 
&\approx \sigma^2. 
\end{align*}
$$
For a bad estimate $\hat{f}$ of $f,$ however, we expect that  
$$
\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}
&=\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i} - \hat{f}(X_{i}^{Test})\right)^2\\
&\gg \sigma^2. 
\end{align*}
$$


::: {.callout-important}

The training data MSE is **not able** to estimate the mean squared (prediction) error 
$$
E\left[(Y - \hat{Y})^2\right].
$$
:::


<br>

### Global Training and Test MSE in Nonparametric Smoothing Spline Regression {-}

Figure 2.9 shows training and test MSEs for smoothing spline (`R` command `smooth.spline()`) estimates $\hat{f}$ in the case of 

* a moderately complex $f$ 
* a moderate signal-to-noise ratio $\frac{Var(f(X))}{Var(\epsilon)}$

![](images/Fig_2_9.png)

<br>

Figure 2.10 shows training and test MSEs for smoothing spline estimates $\hat{f}$ in the case of 

* a very simple $f$ 
* a moderate signal-to-noise ratio $\frac{Var(f(X))}{Var(\epsilon)}$

![](images/Fig_2_10.png)

<br>

Figure 2.11 shows training and test MSEs for smoothing spline estimates $\hat{f}$ in the case of 

* a moderately complex $f$
* a very large signal-to-noise ratio $\frac{Var(f(X))}{Var(\epsilon)}$

![](images/Fig_2_11.png)

<br>




In practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably more difficult because usually no test data are available. 

As the three examples in Figures 2.9, 2.10, and 2.11 of our textbook illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably. 

Throughout this book, we discuss a variety of approaches that can be used in practice to estimate the minimum point of the test MSE. 

One important method is **cross-validation**, which is a method for estimating the test MSE using the training data.





<!-- $$
E\left[E[(Y_0- \hat{f}(X))^2|X]\right]
$$ -->





![](images/Fig_2_12.png)



































