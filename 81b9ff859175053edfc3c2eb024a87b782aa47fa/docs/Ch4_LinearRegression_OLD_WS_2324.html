<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>5&nbsp; Linear Regression (NEW) – Computer-Aided Statistical Analysis (B.Sc.)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch5_Classification.html" rel="next">
<link href="./Ch4_LinearRegression.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch4_LinearRegression_OLD_WS_2324.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Regression (NEW)</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer-Aided Statistical Analysis (B.Sc.)</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Intro2R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title"><code>R</code>-Lab: Introduction to <code>R</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_StatLearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_MatrixAlgebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_LinearRegression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_LinearRegression_OLD_WS_2324.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Regression (NEW)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_Classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_ResamplingMethods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#sec-LinModAssumptions" id="toc-sec-LinModAssumptions" class="nav-link active" data-scroll-target="#sec-LinModAssumptions"><span class="header-section-number">5.1</span> Assumptions</a></li>
  <li><a href="#deriving-the-expression-of-the-ols-estimator" id="toc-deriving-the-expression-of-the-ols-estimator" class="nav-link" data-scroll-target="#deriving-the-expression-of-the-ols-estimator"><span class="header-section-number">5.2</span> Deriving the Expression of the OLS Estimator</a></li>
  <li><a href="#assessing-the-accuracy-of-the-model-fit-hatf" id="toc-assessing-the-accuracy-of-the-model-fit-hatf" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-model-fit-hatf"><span class="header-section-number">5.3</span> Assessing the Accuracy of the Model fit <span class="math inline">\(\hat{f}\)</span></a></li>
  <li>
<a href="#assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" id="toc-assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-coefficient-estimators-hatbeta"><span class="header-section-number">5.4</span> Assessing the Accuracy of the Coefficient Estimators <span class="math inline">\(\hat{\beta}\)</span></a>
  <ul class="collapse">
<li><a href="#bias-of-hatbeta" id="toc-bias-of-hatbeta" class="nav-link" data-scroll-target="#bias-of-hatbeta"><span class="header-section-number">5.4.1</span> Bias of <span class="math inline">\(\hat{\beta}\)</span></a></li>
  <li><a href="#standard-error-of-hatbeta_j" id="toc-standard-error-of-hatbeta_j" class="nav-link" data-scroll-target="#standard-error-of-hatbeta_j"><span class="header-section-number">5.4.2</span> Standard Error of <span class="math inline">\(\hat{\beta}_j\)</span></a></li>
  </ul>
</li>
  <li>
<a href="#sec-linearRegCh" id="toc-sec-linearRegCh" class="nav-link" data-scroll-target="#sec-linearRegCh"><span class="header-section-number">6</span> Linear Regression</a>
  <ul class="collapse">
<li>
<a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression"><span class="header-section-number">6.1</span> Simple Linear Regression</a>
  <ul class="collapse">
<li><a href="#estimating-the-regression-coefficients" id="toc-estimating-the-regression-coefficients" class="nav-link" data-scroll-target="#estimating-the-regression-coefficients"><span class="header-section-number">6.1.1</span> Estimating the Regression Coefficients</a></li>
  <li><a href="#assessing-the-accuracy-of-the-coefficient-estimates" id="toc-assessing-the-accuracy-of-the-coefficient-estimates" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-coefficient-estimates"><span class="header-section-number">6.1.2</span> Assessing the Accuracy of the Coefficient Estimates</a></li>
  <li><a href="#assessing-the-accuracy-of-the-model" id="toc-assessing-the-accuracy-of-the-model" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-model"><span class="header-section-number">6.1.3</span> Assessing the Accuracy of the Model</a></li>
  </ul>
</li>
  <li>
<a href="#sec-MultLinReg" id="toc-sec-MultLinReg" class="nav-link" data-scroll-target="#sec-MultLinReg"><span class="header-section-number">6.2</span> Multiple Linear Regression</a>
  <ul class="collapse">
<li><a href="#estimating-the-regression-coefficients-1" id="toc-estimating-the-regression-coefficients-1" class="nav-link" data-scroll-target="#estimating-the-regression-coefficients-1"><span class="header-section-number">6.2.1</span> Estimating the Regression Coefficients</a></li>
  <li><a href="#inference-on-beta_1dotsbeta_p" id="toc-inference-on-beta_1dotsbeta_p" class="nav-link" data-scroll-target="#inference-on-beta_1dotsbeta_p"><span class="header-section-number">6.2.2</span> Inference on <span class="math inline">\(\beta_1,\dots,\beta_p\)</span></a></li>
  </ul>
</li>
  <li>
<a href="#other-considerations-in-the-regression-model" id="toc-other-considerations-in-the-regression-model" class="nav-link" data-scroll-target="#other-considerations-in-the-regression-model"><span class="header-section-number">6.3</span> Other Considerations in the Regression Model</a>
  <ul class="collapse">
<li><a href="#qualitative-predictors" id="toc-qualitative-predictors" class="nav-link" data-scroll-target="#qualitative-predictors"><span class="header-section-number">6.3.1</span> Qualitative Predictors</a></li>
  <li><a href="#extensions-of-the-linear-model" id="toc-extensions-of-the-linear-model" class="nav-link" data-scroll-target="#extensions-of-the-linear-model"><span class="header-section-number">6.3.2</span> Extensions of the Linear Model</a></li>
  <li><a href="#potential-problems" id="toc-potential-problems" class="nav-link" data-scroll-target="#potential-problems"><span class="header-section-number">6.3.3</span> Potential Problems</a></li>
  </ul>
</li>
  <li><a href="#comparison-linear-regression-vs.-k-nn-regression" id="toc-comparison-linear-regression-vs.-k-nn-regression" class="nav-link" data-scroll-target="#comparison-linear-regression-vs.-k-nn-regression"><span class="header-section-number">6.4</span> Comparison: Linear Regression vs.&nbsp;K-NN Regression</a></li>
  <li>
<a href="#self-study-r-lab-linear-regression" id="toc-self-study-r-lab-linear-regression" class="nav-link" data-scroll-target="#self-study-r-lab-linear-regression"><span class="header-section-number">6.5</span> Self-Study <code>R</code>-Lab: Linear Regression</a>
  <ul class="collapse">
<li><a href="#libraries" id="toc-libraries" class="nav-link" data-scroll-target="#libraries"><span class="header-section-number">6.5.1</span> Libraries</a></li>
  <li><a href="#simple-linear-regression-1" id="toc-simple-linear-regression-1" class="nav-link" data-scroll-target="#simple-linear-regression-1"><span class="header-section-number">6.5.2</span> Simple Linear Regression</a></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">6.5.3</span> Multiple Linear Regression</a></li>
  <li><a href="#interaction-terms" id="toc-interaction-terms" class="nav-link" data-scroll-target="#interaction-terms"><span class="header-section-number">6.5.4</span> Interaction Terms</a></li>
  <li><a href="#non-linear-transformations-of-the-predictors" id="toc-non-linear-transformations-of-the-predictors" class="nav-link" data-scroll-target="#non-linear-transformations-of-the-predictors"><span class="header-section-number">6.5.5</span> Non-linear Transformations of the Predictors</a></li>
  <li><a href="#qualitative-predictors-1" id="toc-qualitative-predictors-1" class="nav-link" data-scroll-target="#qualitative-predictors-1"><span class="header-section-number">6.5.6</span> Qualitative Predictors</a></li>
  <li><a href="#writing-functions" id="toc-writing-functions" class="nav-link" data-scroll-target="#writing-functions"><span class="header-section-number">6.5.7</span> Writing Functions</a></li>
  </ul>
</li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">6.6</span> Exercises</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title">
<span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Regression (NEW)</span>
</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><section id="sec-LinModAssumptions" class="level2" data-number="5.1"><h2 data-number="5.1" class="anchored" data-anchor-id="sec-LinModAssumptions">
<span class="header-section-number">5.1</span> Assumptions</h2>
<p>The (multiple) linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:</p>
<p><strong>Assumption 1: Model and Sampling</strong></p>
<p><strong>Part (a): Linear Model</strong></p>
<p><span id="eq-LinMod"><span class="math display">\[
\begin{align}
  Y_i= \underbrace{\sum_{k=0}^p\beta_k X_{ik}}_{=f(X_i)}+\epsilon_i, \quad i=1,\dots,n,
\end{align}
\qquad(5.1)\]</span></span> where <span class="math display">\[
X_{i0}=1
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span> is called “dependent variable” or “outcome variable” or “regressand” or</li>
<li>
<span class="math inline">\(X_{ik}\)</span> is called the <span class="math inline">\(k\)</span>th “predictor variable” or “regressor” or “explanatory variable” or “control variable.” Each of these names emphasizes a slightly different perspective on <span class="math inline">\(X_{ik}.\)</span>
</li>
</ul>
<p>It is convenient to write <a href="#eq-LinMod" class="quarto-xref">Equation&nbsp;<span>5.1</span></a> using matrix notation <span class="math display">\[
\begin{eqnarray*}
  Y_i&amp;=&amp;\underset{(1\times (p+1))}{X_i'}\underset{((p+1)\times 1)}{\beta} +\epsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}
\]</span> where <span class="math display">\[
  X_i=\left(\begin{matrix}X_{i0}\\ \vdots\\  X_{ip}\end{matrix}\right)
  \quad\text{and}\quad
\beta=\left(\begin{matrix}\beta_0\\ \vdots\\ \beta_p\end{matrix}\right).
\]</span> Stacking all individual rows <span class="math inline">\(i=1,\dots,n\)</span> leads to <span class="math display">\[
\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&amp;=&amp;\underset{(n\times (p+1))}{X}\underset{((p+1)\times 1)}{\beta} + \underset{(n\times 1)}{\epsilon},
\end{eqnarray*}
\]</span> where <span class="math display">\[
\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right),\quad X=\left(\begin{matrix}X_{10}&amp;\dots&amp;X_{1(p+1)}\\\vdots&amp;\ddots&amp;\vdots\\ X_{n0}&amp;\dots&amp;X_{n(p+1)}\\\end{matrix}\right),\quad\text{and}\quad \epsilon=\left(\begin{matrix}\epsilon_1\\ \vdots\\ \epsilon_n\end{matrix}\right).
\end{equation*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Simple Linear Regression and Polynomial Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>The special case of <span class="math inline">\(p=1\)</span> <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i
\]</span> is called the <strong><em>simple</em> linear regression model</strong>. With the simple linear regression model, only straight line fits are possible.</p>
<p>By contrast, with the multiple linear regression model, we can also fit polynomials. For instance, we can define <span class="math display">\[
X_{i2} := X_{i1}^2
\]</span> which leads to a quadratic regression model (often used for life-cycle analyses that include the predictor <code>Age</code><span class="math inline">\(_i=X_{i1}\)</span>) <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i1}^2 + \epsilon_i.
\]</span> Of course, further predictor variables <span class="math inline">\(X_{i2},\dots,X_{ip}\)</span> can (and should) be added to this model.</p>
<p>The same logic applies to polynomials with higher polynomial degrees <span class="math inline">\((\geq 2).\)</span> Large polynomial degrees, however, can lead to unstable estimation results.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The assumption <span class="math inline">\(f(X_i) = X_i'\beta\)</span> may be a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple.</p>
</div>
</div>
<p><strong>Part (b): Random Sample</strong></p>
<p>We assume that the observed data points <span class="math display">\[
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{n(p+1)}))
\]</span> are a realization of the <strong>training data random sample</strong> <span class="math display">\[
((Y_{1},X_{10},\dots,X_{1(p+1)}),\dots,(Y_{n},X_{n0},\dots,X_{n(p+1)})).
\]</span></p>
<p>That is, the <span class="math inline">\(i\)</span>th observed <span class="math inline">\(p+2\)</span> dimensional data point <span class="math display">\[
(y_{i},x_{i0},\dots,x_{i(p+1)})\in\mathbb{R}^{p+2}
\]</span> is a realization of a <span class="math inline">\(p+2\)</span> dimensional random variable <span class="math display">\[
(Y_{i},X_{i0},\dots,X_{i(p+1)})\in\mathbb{R}^{p+2},
\]</span> where</p>
<ol type="1">
<li>
<span class="math inline">\((Y_{i},X_{i0},\dots,X_{i(p+1)})\)</span> has the identical <span class="math inline">\(p+2\)</span> dimensional distribution for all <span class="math inline">\(i=1,\dots,n.\)</span>
</li>
<li>
<span class="math inline">\((Y_{i},X_{i0},\dots,X_{i(p+1)})\)</span> is independent of <span class="math inline">\((Y_{j},X_{j0},\dots,X_{j(p+1)})\)</span> for all <span class="math inline">\(i\neq j=1,\dots,n.\)</span>
</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Due to <a href="#eq-LinMod" class="quarto-xref">Equation&nbsp;<span>5.1</span></a>, this i.i.d. assumption is equivalent to assuming that the multivariate random variables <span class="math display">\[
(\epsilon_i,X_{i0},\dots,X_{i(p+1)})\in\mathbb{R}^{p+2}
\]</span> are i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Remark:</strong> Often, we do not use a different notation for observed realizations <span class="math inline">\((y_{i},x_{i0},\dots,x_{i(p+1)})\in\mathbb{R}^{p+2}\)</span> and for the corresponding random variable <span class="math inline">\((Y_{i},X_{i0},\dots,X_{i(p+1)})\in\mathbb{R}^{p+2}\)</span> since often both interpretations (random variable and its realizations) can make sense in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.</p>
</div>
</div>
<p><strong>Assumption 2: Exogeneity</strong> <span id="eq-assExogen"><span class="math display">\[
E(\epsilon_i|X_i)=0,\quad i=1,\dots,n
\qquad(5.2)\]</span></span></p>
<p>This assumption demands that the mean of the random error term <span class="math inline">\(\epsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_i\)</span>. This exogeneity assumption is also called</p>
<ul>
<li>“orthogonality assumption” or</li>
<li>“mean independence assumption.”</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Together with the random sample assumption (Assumption 1, Part (b)) <a href="#eq-assExogen" class="quarto-xref">Equation&nbsp;<span>5.2</span></a> even implies <strong>strict exogeneity</strong> <span class="math display">\[
E(\epsilon|X) = \underset{(n\times 1)}{0},
\]</span> since we have independence across <span class="math inline">\(i=1,\dots,n\)</span>. Under strict exogeneity, the mean of the random error <strong>vector</strong> <span class="math inline">\(\epsilon\in\mathbb{R}^n\)</span> is zero irrespective of the realizations of the <span class="math inline">\((n\times (p+1))\)</span>-dimensional random predictor matrix <span class="math inline">\(X.\)</span></p>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Independence between error term and predictors
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let</p>
<ul>
<li>
<span class="math inline">\(E(\epsilon_i)=0\)</span> and</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> be independent of <span class="math inline">\(X_i\)</span>
</li>
</ul>
<p>Here the assumption of exogeneity is fulfilled since by the independence between <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_i\)</span> we have that <span class="math display">\[
E(\epsilon_i|X_i) = E(\epsilon_i)
\]</span> and by assumption <span class="math inline">\(E(\epsilon_i)=0\)</span> such that <span class="math display">\[
E(\epsilon_i|X_i) = 0.
\]</span></p>
<p>Note: The assumption <span class="math inline">\(E(\epsilon_i)=0\)</span> is not critical (i.e.&nbsp;not restrictive) due to the intercept term in <a href="#eq-assExogen" class="quarto-xref">Equation&nbsp;<span>5.2</span></a>.</p>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Heteroskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let</p>
<ul>
<li>
<span class="math inline">\(\epsilon_i\sim\mathcal{N}(0,\sigma_i^2),\)</span> where</li>
<li><span class="math inline">\(\sigma_i = |X_{i1}|\)</span></li>
</ul>
<p>Here the assumption of exogeneity is fulfilled since realizations of <span class="math inline">\(X_i\)</span> do not affect the mean of <span class="math inline">\(\epsilon_i,\)</span> thus <span class="math display">\[
E(\epsilon_i|X_i) = 0.
\]</span></p>
<p>However, <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_i\)</span> are not independent of each other, since the conditional variance of <span class="math inline">\(\epsilon_i\)</span> is a function of <span class="math inline">\(X_{i1}\)</span> <span class="math display">\[
Var(\epsilon_i|X_i) = |X_{i1}|^2.
\]</span></p>
</div>
</div>
<p><strong>Assumption 3: Rank Condition (no perfect multicollinearity)</strong></p>
<p><span class="math display">\[
\begin{align*}
\operatorname{rank}(X)&amp;=(p+1)\quad\text{a.s.}\\
\Leftrightarrow P\big(\operatorname{rank}(X)&amp;=(p+1)\big)=1
\end{align*}
\]</span> This assumption demands that, with probability one, no predictor variable <span class="math inline">\(X_{k}\in\mathbb{R}^n\)</span> is linearly dependent of the others. (This is the literal translation of the “almost surely (a.s.)” concept.)</p>
<p><strong>Note:</strong> The assumption implies that <span class="math inline">\(n\geq (p+1),\)</span> since <span class="math display">\[
\operatorname{rank}(X)\leq \min\{n,(p+1)\}\quad(a.s.)
\]</span></p>
<p>This rank assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation). The violation of this assumption harms any economic interpretation since we cannot disentangle the explanatory variables’ individual effects on <span class="math inline">\(Y\)</span>. Therefore, this assumption is also often called an <strong>identification assumption</strong>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Under Assumption 3, we have that <span class="math display">\[
\operatorname{rank}(X)=(p+1)\quad\text{(a.s.)}
\]</span></p></li>
<li><p>This implies that the <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix <span class="math inline">\(X'X\)</span> has full rank, i.e.&nbsp;that <span class="math display">\[
\operatorname{rank}(X'X)=(p+1)\quad\text{(a.s.)}
\]</span></p></li>
<li><p>Thus <span class="math inline">\((X'X)\)</span> is invertible; i.e.&nbsp;there exists a <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix <span class="math inline">\((X'X)^{-1}\)</span> such that <span class="math display">\[
(X'X)(X'X)^{-1} = (X'X)^{-1}(X'X) = I_{(p+1)}.
\]</span></p></li>
</ul>
</div>
</div>
<p><strong>Assumption 4: Error distribution</strong></p>
<p>There are different more or less restrictive assumptions. Some of the most common ones are the following:</p>
<ul>
<li><p><strong>Conditional distribution with sufficiently many moments:</strong> <span class="math display">\[
\epsilon_i|X_i \sim f_{\epsilon|X}
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for any distribution <span class="math inline">\(f_{\epsilon|X}\)</span> with two (or more) finite moments.</p></li>
<li><p><strong>Conditional normal distribution:</strong> <span class="math display">\[
\epsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span>.</p></li>
<li><p><strong>Independence between error and predictors:</strong> <span class="math inline">\(\epsilon_i\sim f_\epsilon\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> such that <span class="math inline">\(f_\epsilon=f_{\epsilon|X}\)</span> and such that <span class="math inline">\(f_\epsilon\)</span> has two (or more) finite moments.</p></li>
<li><p><strong>Independence between error and predictors and normality:</strong> As above, but with <span class="math inline">\(f_\epsilon=\mathcal{N}(0,\sigma^2)\)</span>.</p></li>
<li>
<p><strong>Spherical errors:</strong> The conditional distributions of <span class="math inline">\(\epsilon_i|X_i\)</span> may generally depend on <span class="math inline">\(X_i\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> but only such that <span class="math display">\[
E(\epsilon|X)=\underset{(n\times 1)}{0}
\]</span> and <span class="math display">\[
\begin{align*}
&amp;\underset{(n\times n)}{Var\left(\epsilon|X\right)}=\\[2ex]
&amp; = \left(\begin{matrix}
Var(\epsilon_1|X)&amp;Cov(\epsilon_1,\epsilon_2|X)&amp;\dots&amp;Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&amp;Var(\epsilon_2|X)&amp;\dots&amp;Cov(\epsilon_2,\epsilon_n|X)\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
Cov(\epsilon_n,\epsilon_1|X)&amp;Cov(\epsilon_n,\epsilon_2|X)&amp;\dots&amp;Var(\epsilon_n|X)
\end{matrix}\right)\\[2ex]
&amp; = \left(\begin{matrix}
\sigma^2&amp;0&amp;\dots&amp;0\\
0&amp;\sigma^2&amp;\dots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\dots&amp;\sigma^2
\end{matrix}\right)
= \sigma^2 I_n,
\end{align*}
\]</span> where <span class="math inline">\(I_n\)</span> denotes the <span class="math inline">\((n\times n)\)</span> identity matrix with ones on the diagonal and zeros else. Thus, under the spherical errors assumption, one has, for all possible realizations of <span class="math inline">\(X\)</span>, that:</p>
<ul>
<li>
<strong>uncorrelated:</strong> <span class="math inline">\(Cov(\epsilon_i,\epsilon_j|X)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and all <span class="math inline">\(j=1,\dots,n\)</span> such that <span class="math inline">\(i\neq j\)</span>
</li>
<li>
<strong>homoskedastic:</strong> <span class="math inline">\(Var(\epsilon_i|X)=\sigma^2\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>
</li>
</ul>
</li>
</ul>
<p>All four Assumptions 1-4 must hold for doing inference using the (multiple) linear regression model.</p>
<section id="homoskedastic-versus-heteroskedastic-error-terms" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="homoskedastic-versus-heteroskedastic-error-terms">Homoskedastic versus Heteroskedastic Error Terms</h4>
<p>The i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\((X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+1}\)</span>. That is, the error term <span class="math inline">\(\epsilon_i\)</span> can have a conditional distribution which depends on <span class="math inline">\((X_{i0},\dots,X_{i(p+1)}).\)</span></p>
<p>The exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of <span class="math inline">\(\epsilon_i\)</span> is independent of <span class="math inline">\(X_i\)</span>. Besides this, dependencies between <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_{i0},\dots,X_{i(p+1)}\)</span> are allowed. For instance, the variance of <span class="math inline">\(\epsilon_i\)</span> can be a function of <span class="math inline">\(X_{i0},\dots,X_{i(p+1)}.\)</span> If this is the case, <span class="math inline">\(\epsilon_i\)</span> is said to be <strong>“heteroskedastic.”</strong></p>
<ul>
<li><p><strong>Heteroskedastic error terms:</strong> The conditional variances <span class="math display">\[
Var(\epsilon_i|X_i=x_i)=\sigma^2(x_i)
\]</span> are a non-constant function <span class="math inline">\(\sigma^2(x_i)&gt;0\)</span> of the realizations <span class="math inline">\(X_i=x_i.\)</span></p></li>
<li><p><strong>Homoskedastic error terms:</strong> The conditional variances <span class="math display">\[
Var(\epsilon_i|X_i=x_i)=\sigma^2
\]</span> are constant <span class="math inline">\(\sigma^2&gt;0\)</span> for every possible realization <span class="math inline">\(X_i=x_i.\)</span></p></li>
</ul>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Heteroskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\epsilon_i|X_i\sim \mathcal{U}[-0.5|X_{i2}|, 0.5|X_{i2}|],
\]</span> with <span class="math display">\[
X_{i2}\sim \mathcal{U}[-4,4]
\]</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> where <span class="math inline">\(\mathcal{U}[a,b]\)</span> denotes the uniform distribution over <span class="math inline">\([a,b].\)</span></p>
<p>This error term is mean independent of <span class="math inline">\(X_i\)</span> since <span class="math inline">\(E(\epsilon_i|X_i)=0\)</span>, but it has a heteroskedastic conditional variance since <span class="math display">\[
Var(\epsilon_i|X_i)=\frac{1}{12}X_{i2}^2
\]</span> depends on <span class="math inline">\(X_{i2}.\)</span></p>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Homoskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\epsilon_i\sim{\mathcal N} (0, \sigma^2)
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span> Here, the conditional variance of the error terms <span class="math inline">\(\epsilon_i\)</span> given <span class="math inline">\(X_i\)</span> <span class="math display">\[
Var(\epsilon_i|X_i)=Var(\epsilon_i)=\sigma^2
\]</span> are equal to the constant <span class="math inline">\(\sigma^2&gt;0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for every possible realization of <span class="math inline">\(X_i.\)</span></p>
</div>
</div>
</section></section><section id="deriving-the-expression-of-the-ols-estimator" class="level2" data-number="5.2"><h2 data-number="5.2" class="anchored" data-anchor-id="deriving-the-expression-of-the-ols-estimator">
<span class="header-section-number">5.2</span> Deriving the Expression of the OLS Estimator</h2>
<p>We derive the expression for the OLS estimator <span class="math display">\[
\hat\beta=(\hat\beta_0,\dots,\hat\beta_p)'\in\mathbb{R}^{p+1}
\]</span> as the vector-valued minimizing argument of the sum of squared residuals, <span class="math display">\[
\operatorname{RSS}(b)=\sum_{i=1}^n\big(\underbrace{Y_i-X_i'b}_{\text{$i$th residual}}\big)^2
\]</span> with <span class="math inline">\(b\in\mathbb{R}^K\)</span>, for a given sample <span class="math display">\[
((Y_1,X_1),\dots,(Y_n,X_n)).
\]</span></p>
<p>Using matrix/vector notation we can write <span class="math inline">\(S_n(b)\)</span> as <span class="math display">\[
\begin{align*}
\operatorname{RSS}(b)
&amp;=\sum_{i=1}^n(Y_i-X_i'b)^2\\[2ex]
&amp;=(Y-X b)^{\prime}(Y-X b)\\[2ex]
&amp;=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}
\]</span> To find the minimizing argument <span class="math display">\[
\hat\beta = \arg\min_{b\in\mathbb{R}^{p+1}}\operatorname{RSS}(b)
\]</span> we compute the vector containing all partial derivatives <span class="math display">\[
\begin{align*}
\underset{((p+1)\times 1)}{\frac{\partial \operatorname{RSS}(b)}{\partial b}} &amp;=-2\left(X^{\prime}Y -X^{\prime} Xb\right).
\end{align*}
\]</span> Setting each partial derivative to zero leads to <span class="math inline">\((p+1)\)</span> linear equations (“normal equations”) in <span class="math inline">\((p+1)\)</span> unknowns. This linear system of equations defines the OLS estimates, <span class="math inline">\(\hat{\beta}\)</span>, for a given dataset: <span class="math display">\[
\begin{align*}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)
&amp;=\underset{((p+1)\times 1)}{0}\\[2ex]
X^{\prime} X\hat{\beta}
&amp;=\underset{((p+1)\times 1)}{X^{\prime}Y}.
\end{align*}
\]</span> From our full rank assumption (Assumption 3) it follows that <span class="math inline">\(X^{\prime}X\)</span> is an invertible <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix which allows us to solve the equation system by <span class="math display">\[
\begin{align*}
\underset{((p+1)\times 1)}{\hat{\beta}} &amp;=\left(X^{\prime} X\right)^{-1} X^{\prime} Y.
\end{align*}
\]</span></p>
<p>The following codes computes the estimate <span class="math inline">\(\hat{\beta}\)</span> for a given dataset with <span class="math inline">\(X_i\in\mathbb{R}^{p+1}\)</span>, <span class="math inline">\(p=2\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Some given data</span></span>
<span><span class="va">X_1</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.9</span>,<span class="fl">0.8</span>,<span class="fl">1.1</span>,<span class="fl">0.1</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">4.4</span>,<span class="fl">4.6</span>,<span class="fl">1.6</span>,<span class="fl">5.5</span>,<span class="fl">3.4</span><span class="op">)</span></span>
<span><span class="va">X_2</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">66</span>, <span class="fl">62</span>, <span class="fl">64</span>, <span class="fl">61</span>, <span class="fl">63</span>, <span class="fl">70</span>, <span class="fl">68</span>, <span class="fl">62</span>, <span class="fl">68</span>, <span class="fl">66</span><span class="op">)</span></span>
<span><span class="va">Y</span>       <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.7</span>,<span class="op">-</span><span class="fl">1.0</span>,<span class="op">-</span><span class="fl">0.2</span>,<span class="op">-</span><span class="fl">1.2</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">3.4</span>,<span class="fl">0.0</span>,<span class="fl">0.8</span>,<span class="fl">3.7</span>,<span class="fl">2.0</span><span class="op">)</span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="st">"X_1"</span> <span class="op">=</span> <span class="va">X_1</span>, <span class="st">"X_2"</span> <span class="op">=</span> <span class="va">X_2</span>, <span class="st">"Y"</span> <span class="op">=</span> <span class="va">Y</span><span class="op">)</span></span>
<span><span class="co">## Compute the OLS estimation</span></span>
<span><span class="va">lmobj</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span>, data <span class="op">=</span> <span class="va">dataset</span><span class="op">)</span></span>
<span><span class="co">## Plot sample regression surface</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"scatterplot3d"</span><span class="op">)</span> <span class="co"># library for 3d plots</span></span>
<span><span class="va">plot3d</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/scatterplot3d/man/scatterplot3d.html">scatterplot3d</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">X_1</span>, y <span class="op">=</span> <span class="va">X_2</span>, z <span class="op">=</span> <span class="va">Y</span>,</span>
<span>            angle <span class="op">=</span> <span class="fl">33</span>, scale.y <span class="op">=</span> <span class="fl">0.8</span>, pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>            color <span class="op">=</span><span class="st">"red"</span>, </span>
<span>            xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,</span>
<span>            ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>,</span>
<span>            main <span class="op">=</span><span class="st">"OLS Regression Surface"</span><span class="op">)</span></span>
<span><span class="va">plot3d</span><span class="op">$</span><span class="fu">plane3d</span><span class="op">(</span><span class="va">lmobj</span>, lty.box <span class="op">=</span> <span class="st">"solid"</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.5</span><span class="op">)</span>, draw_polygon<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="special-case-simple-linear-regression-model" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="special-case-simple-linear-regression-model">Special Case: Simple Linear Regression Model</h4>
<p>For a given observed realization of the training data random sample <span class="math display">\[
(x_1,y_1),\dots,(x_n,y_n)
\]</span> we choose <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> such that the <strong>R</strong>esidual <strong>S</strong>um of <strong>S</strong>quares criterion is minimized: <span class="math display">\[
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat{\beta}_0,\hat{\beta_1})
&amp; = e_1^2 + \dots + e_n^2\\[2ex]
&amp;=\sum_{i=1}^n\left(y_i - \left(\hat\beta_0 + \hat\beta_1x_i\right)\right)^2\\[2ex]
&amp;=\sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2
\end{align*}
\]</span> The minimizers are <span class="math display">\[
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
\]</span> and <span class="math display">\[
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
\]</span> where <span class="math inline">\(\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i\)</span> and <span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i\)</span>.</p>
<p><img src="images/Fig_3_1.png" class="img-fluid"></p>
<!-- 
![](images/Fig_3_2.png) 
-->
</section><section id="some-quantities-of-interest" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="some-quantities-of-interest">Some Quantities of Interest</h4>
<p><strong>Predicted values and residuals.</strong></p>
<ul>
<li><p>The (OLS) <strong>predicted values</strong>: <span class="math display">\[
\hat{Y}_i=X_i'\hat\beta, \quad i=1,\dots,n
\]</span> The <span class="math inline">\((n\times 1)\)</span> vector of predicted values <span class="math display">\[
\begin{align*}
\hat{Y} = \left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)
&amp;=X\hat{\beta}\\[-2ex]
&amp;=\underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&amp;=P_X Y
\end{align*}
\]</span></p></li>
<li><p>The (OLS) <strong>residuals</strong>: <span class="math display">\[
e_i=Y_i-\hat{Y}_i, \quad i=1,\dots,n
\]</span> The <span class="math inline">\((n\times 1)\)</span> vector of residuals <span class="math display">\[
\begin{align*}
e =
\left(\begin{matrix}e_1\\e_2\\ \vdots\\ e_n\end{matrix}\right)
&amp;=
\left(\begin{matrix}Y_1\\[.5ex]Y_2\\[.5ex] \vdots\\[.5ex] Y_n\end{matrix}\right)-
\left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)\\[2ex]
&amp;=Y - \hat{Y}\\[2ex]
%&amp;=Y - X\hat{\beta}\\[-2ex]
%&amp;=Y - \underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&amp;=Y - P_X Y\\[2ex]
&amp;=\underbrace{(I_n - P_X)}_{=M_X} Y\\[2ex]
&amp;=M_XY
\end{align*}
\]</span></p></li>
</ul>
<p><strong>Projection matrices.</strong></p>
<p>The matrix <span class="math display">\[
P_X=X(X'X)^{-1}X'
\]</span> is the <span class="math inline">\((n\times n)\)</span> <strong>projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the column space spanned by the column vectors of <span class="math inline">\(X\)</span> and <span class="math display">\[
M_X=I_n-X(X'X)^{-1}X'=I_n-P_X
\]</span> is the associated <span class="math inline">\((n\times n)\)</span> <strong>orthogonal projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the vector space that is orthogonal to that spanned by the column vectors of <span class="math inline">\(X.\)</span></p>
</section></section><section id="assessing-the-accuracy-of-the-model-fit-hatf" class="level2" data-number="5.3"><h2 data-number="5.3" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-model-fit-hatf">
<span class="header-section-number">5.3</span> Assessing the Accuracy of the Model fit <span class="math inline">\(\hat{f}\)</span>
</h2>
<p>The larger the proportion of the explained variance, the better is the fit of the estimated model <span class="math inline">\(\hat{f}\)</span> to the training data. This motivates the definition of the so-called <span class="math inline">\(R^2\)</span> coefficient of determination: <span class="math display">\[
\begin{eqnarray*}
R^2
%&amp;=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&amp;=1-\frac{\sum_{i=1}^ne_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&amp;=1-\frac{\operatorname{RSS}}{\operatorname{TSS}}
\end{eqnarray*}
\]</span> with <span class="math display">\[
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat\beta)=\sum_{i=1}^n\left(y_i-x_i'\hat\beta\right)^2=\sum_{i=1}^ne_i^2
\end{align*}
\]</span> and <span class="math display">\[
\begin{align*}
\operatorname{TSS}=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2.
\end{align*}
\]</span></p>
<p><span class="math inline">\(\operatorname{TSS}\)</span> “Total Sum of Squares”</p>
<p><span class="math inline">\(\operatorname{RSS}\)</span> “Residual Sum of Squares”</p>
<ul>
<li><p>Obviously, we have that <span class="math inline">\(0\leq R^2\leq 1\)</span>.</p></li>
<li><p>The closer <span class="math inline">\(R^2\)</span> lies to <span class="math inline">\(1\)</span>, the better is the fit of the model to the observed training data.</p></li>
</ul>
<p>In tendency an accurate model has …</p>
<ul>
<li><p>a low residual standard error <span class="math inline">\(\operatorname{RSE}\)</span> <span class="math display">\[
\operatorname{RSE}=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}
\]</span></p></li>
<li><p>a high <span class="math inline">\(R^2\)</span></p></li>
</ul>
<p><span class="math display">\[
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
\]</span> where <span class="math inline">\(0\leq R^2\leq 1.\)</span></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Cautionary Note Nr 1:</strong> Do not forget that there is a <strong>irreducible error</strong> <span class="math inline">\(Var(\epsilon)=\sigma^2&gt;0\)</span>. Thus</p>
<ul>
<li>very low <span class="math inline">\(\operatorname{RSE}\)</span> values <span class="math inline">\((\operatorname{RSE}\approx 0)\)</span> and</li>
<li>very high <span class="math inline">\(R^2\)</span> values <span class="math inline">\((R^2\approx 1)\)</span>
</li>
</ul>
<p>can be warning signals indicating overfitting. While overfitting typically does not happen with a simple linear regression model, it can happen with a multiple linear regression model.</p>
<p><strong>Cautionary Note Nr 2:</strong> The <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\operatorname{RSE}\)</span> are only based on <strong>training data</strong>. In <a href="Ch2_StatLearning.html" class="quarto-xref"><span>Chapter 2</span></a>, we have seen that a proper assessment of the model accuracy needs to take into account <strong>test data</strong>.</p>
</div>
</div>
<section id="r2-and-correlation-coefficient" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="r2-and-correlation-coefficient">
<span class="math inline">\(R^2\)</span> and correlation coefficient</h4>
<p>In the case of the simple linear regression model, <span class="math inline">\(R^2\)</span> equals the squared sample correlation coefficient between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, <span class="math display">\[
R^2 = r_{yx_1}^2,
\]</span> where <span class="math display">\[
r_{yx_1}=\frac{\sum_{i=1}^n(x_{i1}-\bar{x}_1)(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_{i1}-\bar{x}_1)^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}},
\]</span> where <span class="math inline">\(\bar{x}_1=n^{-1}\sum_{i=1}^nx_{i1}.\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the multiple linear regression model <span class="math inline">\(Y_i=\beta_0+\sum_{j=1}^p\beta_jX_{ij}+\epsilon_i,\)</span> the <span class="math inline">\(R^2\)</span> equals the squared correlation between response and the fitted values: <span class="math display">\[
R^2=r^2_{y\hat{y}}
\]</span> with <span class="math display">\[
r_{y\hat{y}}=\frac{\sum_{i=1}^n(y_i-\bar{y})(\hat{y}_i-\bar{\hat{y}})}{\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}\sqrt{\sum_{i=1}^n(\hat{y}_i-\bar{\hat{y}})^2}},
\]</span> where <span class="math inline">\(\bar{y}=n^{-1}\sum_{i=1}^ny_{i}.\)</span></p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>A high/low <span class="math inline">\(R^2\)</span> value only means that the predictors have high/low <em>predictive power</em> with respect to the training data.</p></li>
<li><p>A high/low <span class="math inline">\(R^2\)</span> does not mean a validation/falsification of the estimated model. Any econometric model needs a plausible explanation from relevant economic theory.<br></p></li>
</ul>
</div>
</div>
<p>The most often criticized disadvantage of the <span class="math inline">\(R^2\)</span> is that additional regressors (relevant or not) will increase the <span class="math inline">\(R^2\)</span>. The below <code>R</code>-codes demonstrates this problem.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>     <span class="op">&lt;-</span> <span class="fl">100</span>                  <span class="co"># Sample size</span></span>
<span><span class="va">X</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>      <span class="co"># Relevant X variable</span></span>
<span><span class="va">X_ir</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">5</span>, <span class="fl">20</span><span class="op">)</span>      <span class="co"># Irrelevant X variable</span></span>
<span><span class="va">error</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">*</span><span class="fl">10</span>    <span class="co"># True (usually unknown) error</span></span>
<span><span class="va">Y</span>     <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">error</span>    <span class="co"># Y variable</span></span>
<span><span class="va">lm1</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>     <span class="co"># Correct OLS regression </span></span>
<span><span class="va">lm2</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">+</span><span class="va">X_ir</span><span class="op">)</span><span class="op">)</span><span class="co"># OLS regression with X_ir </span></span>
<span><span class="va">lm1</span><span class="op">$</span><span class="va">r.squared</span> <span class="op">&lt;</span> <span class="va">lm2</span><span class="op">$</span><span class="va">r.squared</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>So, <span class="math inline">\(R^2\)</span> increases here even though <code>X_ir</code> is a completely irrelevant explanatory variable.</p>
<p>Because of this, the <span class="math inline">\(R^2\)</span> cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called <strong>adjusted</strong> <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\overline{R}^2,\)</span> defined as <span class="math display">\[
\begin{eqnarray*}
  \overline{R}^2&amp;=&amp;1-\frac{\frac{1}{n-(p+1)}\sum_{i=1}^ne^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}\leq R^2%\\
\end{eqnarray*}
\]</span> The adjustment is in terms of the degrees of freedom <span class="math inline">\(n-(p+1)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lm1</span><span class="op">$</span><span class="va">adj.r.squared</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># model without X_ir</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.569</code></pre>
</div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lm2</span><span class="op">$</span><span class="va">adj.r.squared</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># model with X_ir</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.564</code></pre>
</div>
</div>
</section></section><section id="assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" class="level2" data-number="5.4"><h2 data-number="5.4" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-coefficient-estimators-hatbeta">
<span class="header-section-number">5.4</span> Assessing the Accuracy of the Coefficient Estimators <span class="math inline">\(\hat{\beta}\)</span>
</h2>
<section id="bias-of-hatbeta" class="level3" data-number="5.4.1"><h3 data-number="5.4.1" class="anchored" data-anchor-id="bias-of-hatbeta">
<span class="header-section-number">5.4.1</span> Bias of <span class="math inline">\(\hat{\beta}\)</span>
</h3>
<p>Under the Assumptions 1-4, once can show that the OLS estimator <span class="math display">\[
\hat\beta = (X'X)^{-1}X'Y
\]</span> is unbiased, i.e. <span class="math display">\[
\operatorname{Bias}(\hat\beta) = E(\hat\beta) - \beta = \underset{((p+1)\times 1)}{0}.
\]</span> That is, on average <span class="math inline">\(\hat\beta\)</span> equals <span class="math inline">\(\beta.\)</span></p>
<p>This can be shown as following:</p>
<p>Observe that <span class="math display">\[
\hat\beta=(X'X)^{-1}X'Y
\]</span> consists of two multivariate random variables <span class="math inline">\(X\in\mathbb{R}^{n\times(p+1)}\)</span> and <span class="math inline">\(Y\in\mathbb{R}^n.\)</span> Thus one needs to show first the conditional unbiasedness of <span class="math inline">\(\hat\beta\)</span> given <span class="math inline">\(X\)</span> which effectively allows us to focus on randomness due to <span class="math inline">\(\epsilon,\)</span><br><span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta|X)
&amp;= E(\hat\beta|X)                                        - \beta \\[2ex]
&amp;= E((X'X)^{-1}X'\underbrace{Y}_{=X\beta+\epsilon}|X) - \beta \\[2ex]
&amp;= E((X'X)^{-1}X'(X\beta+\epsilon)|X)                 - \beta \\[2ex]
&amp;= E(\underbrace{(X'X)^{-1}X'X}_{=I_K}\beta|X) + E((X'X)^{-1}X'\epsilon|X) - \beta \\[2ex]
&amp;= \underbrace{E(\beta|X)}_{=\beta} + \underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=(X'X)^{-1}X'E(\epsilon|X)} - \beta \\[2ex]
&amp;=  (X'X)^{-1}X'\underbrace{E(\epsilon|X)}_{=0} =\underset{(K\times 1)}{0}  
\end{align*}
\]</span> Thus <span class="math inline">\(\hat\beta\)</span> is unbiased conditionally on <span class="math inline">\(X\)</span> <span class="math display">\[
\operatorname{Bias}(\hat\beta|X) = 0.
\]</span></p>
<p>From this if follows, by the iterated law of expectations, that the OLS estimator is also unconditionally unbiased, i.e.<br><span class="math display">\[
\operatorname{Bias}(\hat\beta) = E\left(\operatorname{Bias}(\hat\beta|X)\right) = E(0) = 0.
\]</span></p>
</section><section id="standard-error-of-hatbeta_j" class="level3" data-number="5.4.2"><h3 data-number="5.4.2" class="anchored" data-anchor-id="standard-error-of-hatbeta_j">
<span class="header-section-number">5.4.2</span> Standard Error of <span class="math inline">\(\hat{\beta}_j\)</span>
</h3>
<p>The standard error of <span class="math inline">\(\hat{\beta}_j,\)</span> for each <span class="math inline">\(j=0,\dots,p,\)</span> is given by <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X)=\sqrt{Var(\hat\beta_j|X)},
\]</span> where <span class="math display">\[
Var(\hat\beta_j|X) = \left[Var(\hat\beta|X)\right]_{(j,j)}
\]</span> denotes the <span class="math inline">\(j\)</span>th diagonal element of the symmetric <span class="math inline">\((p+1)\times (p+1)\)</span> variance-covariance matrix <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=\begin{pmatrix}
Var(\hat\beta_0|X)&amp;Cov(\hat\beta_0,\hat\beta_1|X)&amp;\cdots&amp;Cov(\hat\beta_0,\hat\beta_{p}|X)\\
Cov(\hat\beta_1,\hat\beta_0|X)&amp;Var(\hat\beta_1|X)&amp;  &amp;Cov(\hat\beta_1,\hat\beta_{p}|X)\\
\vdots &amp;&amp;\ddots&amp;\\
Cov(\hat\beta_p,\hat\beta_0|X)&amp;Cov(\hat\beta_p,\hat\beta_1|X)&amp;\cdots&amp;Var(\hat\beta_{p}|X)\\
\end{pmatrix}
\end{align*}
\]</span></p>
<p>Thus, to compute a useful explicit expression for <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X)=?,
\]</span> we need to compute an explicit expression for the symmetric <span class="math inline">\((p+1)\times(p+1)\)</span> variance-covariance matrix <span class="math inline">\(Var(\hat\beta|X).\)</span></p>
<p>Let us derive the general explicit expression for <span class="math inline">\(Var(\hat\beta|X).\)</span></p>
<p>Note that <span class="math display">\[
\begin{align*}
\hat{\beta}
&amp;=(X'X)^{-1}X'Y\\[2ex]
&amp;\text{Using that $Y=X\beta + \epsilon$:}\\[2ex]
&amp;=(X'X)^{-1}X'(X\beta + \epsilon)\\[2ex]
&amp;=\underbrace{(X'X)^{-1}X'X\beta}_{=\beta} + (X'X)^{-1}X'\epsilon
\end{align*}
\]</span> This leads to the so-called sampling error expression <span class="math display">\[
\hat{\beta} - \beta = (X'X)^{-1}X'\epsilon.
\]</span> With this, we can derive the general explicit expression for <span class="math inline">\(Var(\hat\beta|X).\)</span> <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=Var(\hat\beta - \beta|X)\\[2ex]
&amp;\text{Using the sampling error expression:}\\[2ex]
&amp;=Var((X'X)^{-1}X'\epsilon|X)\\[2ex]
&amp;=E\Big[\big((X'X)^{-1}X'\epsilon-\underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=0}\big)\times\\[2ex]
&amp;\phantom{=\Big(}\,\times\big((X'X)^{-1}X'\epsilon-\underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=0}\big)'|X\Big]\\[2ex]
&amp;=E\left[((X'X)^{-1}X'\epsilon)((X'X)^{-1}X'\epsilon)'|X\right]\\[2ex]
&amp;=E\left[(X'X)^{-1}X'\epsilon\epsilon' X(X'X)^{-1}|X\right]\\[2ex]
&amp;=\;\;\;(X'X)^{-1}X'\underbrace{E\left(\epsilon\epsilon'|X\right)}_{=Var(\epsilon|X)}X(X'X)^{-1}
\end{align*}
\]</span> That is, the explicit expression for <span class="math inline">\(Var(\hat\beta|X)\)</span> depends on the explicit form of the symmetric <span class="math inline">\((n\times n)\)</span> matrix <span class="math inline">\(Var(\epsilon|X)\)</span> <span class="math display">\[
\begin{align*}
&amp;Var(\epsilon|X)=\\[2ex]
&amp;=\begin{pmatrix}
Var(\epsilon_1|X)&amp;Cov(\epsilon_1,\epsilon_2|X)&amp;\cdots&amp;Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&amp;Var(\epsilon_2|X)&amp;  &amp;Cov(\epsilon_2,\epsilon_n|X)\\
\vdots &amp;&amp;\ddots&amp;\\
Cov(\epsilon_n,\epsilon_1|X)&amp;Cov(\epsilon_n,\epsilon_2|X)&amp;\cdots&amp;Var(\epsilon_n|X)\\
\end{pmatrix}
\end{align*}
\]</span></p>
<p>The explicit form of the symmetric <span class="math inline">\((n\times n)\)</span> matrix <span class="math inline">\(Var(\epsilon|X)\)</span> depends on our (hopefully correct) assumption on the error-term distribution (Assumption 4).</p>
<section id="case-of-spherical-homoskedastic-uncorrelated-errors" class="level4" data-number="5.4.2.1"><h4 data-number="5.4.2.1" class="anchored" data-anchor-id="case-of-spherical-homoskedastic-uncorrelated-errors">
<span class="header-section-number">5.4.2.1</span> Case of Spherical (Homoskedastic, Uncorrelated) Errors</h4>
<p>If <span class="math display">\[
\begin{align*}
Var(\epsilon|X)
&amp;=
\begin{pmatrix}
\sigma^2 &amp; 0        &amp; \cdots &amp; 0\\
0        &amp; \sigma^2 &amp; \cdots &amp; 0\\
\vdots   &amp; \vdots   &amp; \ddots &amp; 0\\
0        &amp; 0        &amp; \cdots &amp; \sigma^2\\
\end{pmatrix}
&amp;=\sigma^2 I_n,
\end{align*}
\]</span> then <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=(X'X)^{-1}X' \left(Var(\epsilon|X)\right) X(X'X)^{-1}\\[2ex]
&amp;=(X'X)^{-1}X' \left(\sigma^2 I_n \right) X(X'X)^{-1}\\[2ex]
&amp;=\sigma^2\;(X'X)^{-1}X' \left( I_n \right) X(X'X)^{-1}\\[2ex]
&amp;=\sigma^2\;\underbrace{(X'X)^{-1}X'X}_{I_{p+1}}\;(X'X)^{-1}\\[2ex]
&amp;=\sigma^2\;(X'X)^{-1},
\end{align*}
\]</span> where the only unknown component is <span class="math inline">\(\sigma^2=Var(\epsilon_i).\)</span></p>
<p>We can estimate the homoskedastik error term variance <span class="math inline">\(\sigma^2\)</span> using the <strong>R</strong>esidual <strong>S</strong>tandard <strong>E</strong>rror: <span class="math display">\[
\begin{align*}
\hat\sigma = \operatorname{RSE}
&amp;=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}\\[2ex]
&amp;=\sqrt{ \frac{1}{n-(p+1)} \sum_{i=1}^n e_i^2}.
\end{align*}
\]</span></p>
<p><strong>Summing up:</strong></p>
<p>In the case of spherical (homoskedastic and uncorrelated) error terms the standard error of <span class="math inline">\(\beta_j\)</span> is <span class="math display">\[
\operatorname{SE}(\beta_j|X) = \sqrt{\left[\sigma^2 \left(X'X\right)^{-1}\right]_{(j,j)}}.
\]</span> The above expression is the infeasible (since <span class="math inline">\(\sigma^2\)</span> is typically unknown) population version of the standard error. We can estimate this population version using the empirical standard error <span class="math display">\[
\widehat{\operatorname{SE}}(\beta_j|X) = \sqrt{\left[\hat{\sigma}^2 \left(X'X\right)^{-1}\right]_{(j,j)}}.
\]</span></p>
<p>This is the default version for computing the standard error in statistical software packages such as <code>R</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>      <span class="op">&lt;-</span> <span class="fl">100</span>                           <span class="co"># Sample size</span></span>
<span><span class="va">X_1</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>               <span class="co"># Predictor variable X_1</span></span>
<span><span class="va">X_2</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span>               <span class="co"># Predictor variable X_2</span></span>
<span><span class="va">error</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">*</span><span class="fl">10</span>             <span class="co"># True (usually unknown) error</span></span>
<span><span class="va">Y</span>      <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X_1</span> <span class="op">-</span><span class="fl">5</span> <span class="op">*</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">error</span>  <span class="co"># Y variable</span></span>
<span><span class="va">lm_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span><span class="op">)</span>             <span class="co"># OLS regression </span></span>
<span></span>
<span><span class="co">## Standard OLS output table:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span>                         </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Y ~ X_1 + X_2)

Residuals:
    Min      1Q  Median      3Q     Max 
-39.071  -7.138  -0.575   9.570  33.368 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.0112     4.2440   0.238    0.812    
X_1           5.1954     0.4529  11.472  &lt; 2e-16 ***
X_2          -4.7001     0.6690  -7.026 2.95e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 12.84 on 97 degrees of freedom
Multiple R-squared:  0.6565,    Adjusted R-squared:  0.6494 
F-statistic: 92.68 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section><section id="case-of-heteroskedastic-but-uncorrelated-errors" class="level4" data-number="5.4.2.2"><h4 data-number="5.4.2.2" class="anchored" data-anchor-id="case-of-heteroskedastic-but-uncorrelated-errors">
<span class="header-section-number">5.4.2.2</span> Case of Heteroskedastic, but Uncorrelated Errors</h4>
<p>If <span class="math display">\[
\begin{align*}
Var(\epsilon|X)
&amp;=
\begin{pmatrix}
\sigma_1^2 &amp; 0          &amp; \cdots &amp; 0\\
0          &amp; \sigma_2^2 &amp; \cdots &amp; 0\\
\vdots     &amp; \vdots     &amp; \ddots &amp; 0\\
0          &amp; 0          &amp; \cdots &amp; \sigma_n^2\\
\end{pmatrix}
&amp;=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2),
\end{align*}
\]</span> then <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=(X'X)^{-1}X' \left(Var(\epsilon|X)\right) X(X'X)^{-1}\\[2ex]
&amp;=(X'X)^{-1} \left(X'\left(\operatorname{diag}(\sigma_1^2,\dots,\sigma_n^2) \right) X\right) (X'X)^{-1}\\[2ex]
&amp;=(X'X)^{-1} \left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right) (X'X)^{-1}\\[2ex]
\end{align*}
\]</span> Thus, the symmetric <span class="math inline">\((p+1)\times(p+1)\)</span> variance-covariance matrix <span class="math inline">\(Var(\hat\beta|X)\)</span> keeps its “sandwich form”, where the inner part of the sandwich <span class="math display">\[
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right)
\]</span> is typically unknown, since <span class="math inline">\(\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2\)</span> are typically unknown.</p>
<p>There are different, so-called <strong>Heteroskedasticity Consistent (HC)</strong> estimators to estimate the unknown expression <span class="math display">\[
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right).
\]</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 66%">
</colgroup>
<thead><tr class="header">
<th>HC-Type</th>
<th>Formular</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>HC0</td>
<td><span class="math inline">\(\sum_{i=1}^n\hat\varepsilon_i^2X_iX_i'\)</span></td>
</tr>
<tr class="even">
<td>HC1</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{n}{n-K}\hat\varepsilon_i^2X_iX_i'\)</span></td>
</tr>
<tr class="odd">
<td>HC2</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{1-h_{i}}X_iX_i'\)</span></td>
</tr>
<tr class="even">
<td>HC3</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{2}}X_iX_i'\)</span></td>
</tr>
<tr class="odd">
<td>HC4</td>
<td><span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{\delta_{i}}}X_iX_i'\)</span></td>
</tr>
</tbody>
</table>
<p>HC3 is the most often used HC-estimator.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The statistic <span class="math inline">\(h_i:=[P_X]_{ii}\)</span> is called the <strong>leverage statistic</strong> of <span class="math inline">\(X_i,\)</span> where</p>
<ul>
<li>
<span class="math inline">\(1/n\leq h_i\leq 1\)</span> and</li>
<li>
<span class="math inline">\(\bar{h}=n^{-1}\sum_{i=1}^nh_i=K/n\)</span>.</li>
</ul>
<p>Observations <span class="math inline">\(X_i\)</span> with leverage statistics <span class="math inline">\(h_i\)</span> that greatly exceed the average leverage value <span class="math inline">\(K/n\)</span> are referred to as “high leverage” observations. High leverage observations <span class="math inline">\(X_i\)</span> are observations that are far away from all other observations <span class="math inline">\(X_j\)</span>, <span class="math inline">\(i\neq j=1,\dots,n.\)</span></p>
<p>High leverage observations <span class="math inline">\(X_i\)</span> have the potential to distort the estimation results, <span class="math inline">\(\hat\beta_n\)</span>. Indeed, a high leverage observation <span class="math inline">\(X_i\)</span> will have an distorting effect on the estimation results if the absolute value of the corresponding residual <span class="math inline">\(|\hat{\varepsilon}_i|\)</span> is unusually large—such observations are called <strong>influential outliers</strong>. Such observations increase the estimation uncertainty.</p>
<p>General idea of the HC2-HC4 estimators is to increase the estimated variance in order to account for the effects of influential outliers. The residuals <span class="math inline">\(\hat\varepsilon_i\)</span> belonging to <span class="math inline">\(X_i\)</span> values that have a large leverage <span class="math inline">\(h_i\)</span> receive a higher weight and thus increase the value of <span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i').\)</span> This strategy takes into account increased estimation uncertainties due to single influential outliers.</p>
</div>
</div>
<!-- The estimator HC0 was suggested in the econometrics literature by @White1980 and is justified by asymptotic ($n\to\infty$) arguments. The estimators HC1, HC2 and HC3 were suggested by @MacKinnon_White_1985 to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, @Long_Ervin_2000 concludes that HC3 provides the best overall performance in finite samples. @Cribari_2004 suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large $h_i$ values). 
-->
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>      <span class="op">&lt;-</span> <span class="fl">100</span>                           <span class="co"># Sample size</span></span>
<span><span class="va">X_1</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>               <span class="co"># Predictor variable X_1</span></span>
<span><span class="va">X_2</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span>               <span class="co"># Predictor variable X_2</span></span>
<span><span class="va">error</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">X_2</span><span class="op">)</span>     <span class="co"># True (usually unknown) heteroskedastic error</span></span>
<span><span class="va">Y</span>      <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X_1</span> <span class="op">-</span><span class="fl">5</span> <span class="op">*</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">error</span>  <span class="co"># Y variable</span></span>
<span></span>
<span></span>
<span><span class="co">## Package for computing robust variance estimations</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://sandwich.R-Forge.R-project.org/">"sandwich"</a></span><span class="op">)</span> <span class="co"># vcovHC(), </span></span>
<span></span>
<span><span class="co">## Package for producing an OLS output table (etc.)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"lmtest"</span><span class="op">)</span><span class="op">)</span> <span class="co"># coeftest</span></span>
<span></span>
<span><span class="co">## Estimate the linear regression model parameters</span></span>
<span><span class="va">lm_obj</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">vcovHC3_mat</span> <span class="op">&lt;-</span> <span class="fu">sandwich</span><span class="fu">::</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_obj</span>, type<span class="op">=</span><span class="st">"HC3"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">lmtest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest</a></span><span class="op">(</span><span class="va">lm_obj</span>, vcov <span class="op">=</span> <span class="va">vcovHC3_mat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
t test of coefficients:

            Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept)  3.16955    2.17658   1.4562   0.1486    
X_1          4.92390    0.22302  22.0784   &lt;2e-16 ***
X_2         -4.57382    0.34716 -13.1748   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Note: The HC3-Robust SE estimates are: </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">vcovHC3_mat</span><span class="op">)</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)         X_1         X_2 
    2.17658     0.22302     0.34716 </code></pre>
</div>
</div>
</section></section></section><section id="sec-linearRegCh" class="level1" data-number="6"><h1 data-number="6">
<span class="header-section-number">6</span> Linear Regression</h1>
<blockquote class="blockquote">
<p>Reading: Chapter 3 of our course textbook <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a></p>
</blockquote>
<!-- 
#### `R`-Codes for this Chapter {-}

* `R`-codes for Coding Challenge Nr 1: [Ch2_1_Rcodes.R](https://www.dropbox.com/scl/fi/fkbf7z5qp53ib3m57thrp/Ch2_1_Rcodes.R?rlkey=9u8yeehf3zcpym4i3h36p0sd9&dl=0)
* `R`-codes for Coding Challenge Nr 2: [Ch2_2_Rcodes.R](https://www.dropbox.com/scl/fi/5437hqcw13x9zgn0beues/Ch2_2_Rcodes.R?rlkey=td89o175f1efy1dflucmdjfgs&dl=0)  
-->
<section id="simple-linear-regression" class="level2" data-number="6.1"><h2 data-number="6.1" class="anchored" data-anchor-id="simple-linear-regression">
<span class="header-section-number">6.1</span> Simple Linear Regression</h2>
<p>The linear regression model assumes a <em>linear relationship</em> between <span class="math inline">\(Y\)</span> and the predictor(s) <span class="math inline">\(X\)</span>.</p>
<p>The simple (only one predictor) linear regression model: <span class="math display">\[
Y\approx \beta_0 + \beta_1 X
\]</span></p>
For instance,
<center>
<code>sales</code> <span class="math inline">\(\approx \beta_0 + \beta_1\)</span> <code>TV</code>
</center>
<section id="estimating-the-regression-coefficients" class="level3" data-number="6.1.1"><h3 data-number="6.1.1" class="anchored" data-anchor-id="estimating-the-regression-coefficients">
<span class="header-section-number">6.1.1</span> Estimating the Regression Coefficients</h3>
<p>Let <span class="math display">\[
(X_{1},Y_1),\dots,(X_{n},Y_n)
\]</span> denote a training data random sample. I.e.<br><span class="math display">\[
(X_{i},Y_i)\overset{iid}{\sim}(X,Y),\quad\text{for all}\quad i=1,\dots,n.
\]</span> Moreover, let <span id="eq-simpleLinMod"><span class="math display">\[
Y_i=\beta_0 + \beta_1 X_i +\epsilon_i \quad\text{for all}\quad i=1,\dots,n,
\qquad(6.1)\]</span></span> where</p>
<ul>
<li>
<span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_i\)</span> are independent from each other for all <span class="math inline">\(i=1,\dots,n\)</span><br>
</li>
<li>
<span class="math inline">\(E(\epsilon_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>
</li>
<li>
<span class="math inline">\(Var(\epsilon_i)=\sigma^2&gt;0\)</span> is constant for all <span class="math inline">\(i=1,\dots,n\)</span>
</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The assumption <span class="math inline">\(f(X) = \beta_0 + \beta_1 X\)</span> may be a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple.</p>
</div>
</div>
<p>For a given observed realization of the training data random sample <span class="math display">\[
(x_1,y_1),\dots,(x_n,y_n)
\]</span> we choose <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> such that the <strong>R</strong>esidual <strong>S</strong>um of <strong>S</strong>quares criterion is minimized: <span class="math display">\[
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat{\beta}_0,\hat{\beta_1})
&amp; = e_1^2 + \dots + e_n^2\\[2ex]
&amp;=\sum_{i=1}^n\left(y_i - \left(\hat\beta_0 + \hat\beta_1x_i\right)\right)^2\\[2ex]
&amp;=\sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2
\end{align*}
\]</span> The minimizers are <span class="math display">\[
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
\]</span> and <span class="math display">\[
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
\]</span> where <span class="math inline">\(\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i\)</span> and <span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i\)</span>.</p>
<p><img src="images/Fig_3_1.png" class="img-fluid"></p>
<p><img src="images/Fig_3_2.png" class="img-fluid"></p>
</section><section id="assessing-the-accuracy-of-the-coefficient-estimates" class="level3" data-number="6.1.2"><h3 data-number="6.1.2" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-coefficient-estimates">
<span class="header-section-number">6.1.2</span> Assessing the Accuracy of the Coefficient Estimates</h3>
<p>If the assumption of <a href="#eq-simpleLinMod" class="quarto-xref">Equation&nbsp;<span>6.1</span></a> is correct, i.e., if the data is actually generated according to the model <span class="math display">\[
Y_i=\beta_0 + \beta_1 X_i +\epsilon_i,
\]</span> then ordinary least squares estimators <span class="math display">\[
\hat\beta_0\quad\text{and}\quad\hat\beta_1
\]</span> are <strong>unbiased estimators</strong>, that is <span id="eq-SimpleRegBias"><span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta_0)&amp;=E(\hat\beta_0)-\beta_0=0\\
\operatorname{Bias}(\hat\beta_1)&amp;=E(\hat\beta_1)-\beta_1=0.
\end{align*}
\qquad(6.2)\]</span></span> I.e., on average, the estimation results equal the true (unknown) parameters.</p>
<p><strong>Note:</strong> In <a href="#eq-SimpleRegBias" class="quarto-xref">Equation&nbsp;<span>6.2</span></a>, we consider <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> as <strong>random variables</strong> from which we can compute mean values. The estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are indeed <strong>random variables</strong> since they depend on the random variables in the <strong>random sample</strong> <span class="math inline">\((X_i,Y_i)\overset{iid}{\sim}(X,Y)\)</span>:<br><span class="math display">\[
\hat\beta_1=\frac{\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^n(X_i-\bar{X})^2}
\]</span> and <span class="math display">\[
\hat\beta_0=\bar{Y} - \hat\beta_1\bar{X}.
\]</span></p>
<!-- 
For each observed realization of the random sample, we get one observed realization of the random variables 
$$
\hat\beta_0\quad\text{and}\quad \hat\beta_1.
$$ 
-->
<p>In an actual data analysis, we only have <em>one</em> realization of the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> computed from the given dataset (i.e.&nbsp;the observed realization of the random sample). Each single estimation result will have estimation errors, i.e., <span class="math display">\[
\hat\beta_0\neq \beta_0\quad\text{and}\quad\hat\beta_1\neq \beta_1.
\]</span></p>
<p>The following code generates artificial data to reproduce the plot in Figure 3.3 of our course textbook <code>ISLR</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## ###############################</span></span>
<span><span class="co">## A function to generate data </span></span>
<span><span class="co">## similar to that shown in Fig 3.3</span></span>
<span><span class="co">## ##############################</span></span>
<span></span>
<span><span class="co">## A Function to simulate data</span></span>
<span><span class="va">mySimpleRegrDataGenerator</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">n</span>      <span class="op">&lt;-</span> <span class="fl">50</span>                           <span class="co"># sample size</span></span>
<span>  <span class="va">beta_0</span> <span class="op">&lt;-</span> <span class="fl">0.1</span>                          <span class="co"># intercept parameter</span></span>
<span>  <span class="va">beta_1</span> <span class="op">&lt;-</span> <span class="fl">5</span>                            <span class="co"># slope parameter</span></span>
<span>  <span class="va">X</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, min <span class="op">=</span> <span class="op">-</span><span class="fl">2</span>, max <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>  <span class="co"># predictor</span></span>
<span>  <span class="va">error</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">8.5</span><span class="op">)</span> <span class="co"># error term</span></span>
<span>  <span class="va">Y</span>      <span class="op">&lt;-</span> <span class="va">beta_0</span> <span class="op">+</span> <span class="va">beta_1</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">error</span>  <span class="co"># outcome </span></span>
<span>  <span class="co">##</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="st">"Y"</span> <span class="op">=</span> <span class="va">Y</span>, <span class="st">"X"</span> <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">## Generate a first realization of the data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">data_sim</span> <span class="op">&lt;-</span> <span class="fu">mySimpleRegrDataGenerator</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data_sim</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Y          X
1 -18.4853427 -0.8496899
2  12.9872926  1.1532205
3  -0.4167901 -0.3640923
4  -1.9138159  1.5320696
5  19.5667725  1.7618691
6  -5.3639241 -1.8177740</code></pre>
</div>
</div>
<p>Using repeated samples form the data generating process defined in <code>mySimpleRegrDataGenerator()</code>, we can generate multiple estimation results <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> of the unknown simple linear regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and plot the corresponding empirical regression lines:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Estimation</span></span>
<span><span class="va">lm_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X</span>, data <span class="op">=</span> <span class="va">data_sim</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Plotting the results</span></span>
<span></span>
<span><span class="co">## True (usually unknown) parameter values</span></span>
<span><span class="va">beta_0</span> <span class="op">&lt;-</span> <span class="fl">0.1</span>  <span class="co"># intercept parameter</span></span>
<span><span class="va">beta_1</span> <span class="op">&lt;-</span> <span class="fl">5</span>    <span class="co"># slope parameter</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="co"># Two plots side by side</span></span>
<span></span>
<span><span class="co">## First Plot (fit for the first realization of the data)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">data_sim</span><span class="op">$</span><span class="va">X</span>, y <span class="op">=</span> <span class="va">data_sim</span><span class="op">$</span><span class="va">Y</span>, xlab <span class="op">=</span> <span class="st">"X"</span>, ylab <span class="op">=</span> <span class="st">"Y"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>a <span class="op">=</span> <span class="va">beta_0</span>, b <span class="op">=</span> <span class="va">beta_1</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lm_obj</span>, col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Second Plot (fits for multiple data realizations)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">data_sim</span><span class="op">$</span><span class="va">X</span>, y <span class="op">=</span> <span class="va">data_sim</span><span class="op">$</span><span class="va">Y</span>, xlab <span class="op">=</span> <span class="st">"X"</span>, ylab <span class="op">=</span> <span class="st">"Y"</span>, type <span class="op">=</span> <span class="st">"n"</span><span class="op">)</span> <span class="co"># type = "n": empty plot</span></span>
<span><span class="co">##</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">r</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">data_sim_new</span> <span class="op">&lt;-</span> <span class="fu">mySimpleRegrDataGenerator</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="va">lm_obj_new</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X</span>, data<span class="op">=</span><span class="va">data_sim_new</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lm_obj_new</span>, col <span class="op">=</span> <span class="st">"lightskyblue"</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">## Adding the first fit</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>a <span class="op">=</span> <span class="va">beta_0</span>, b <span class="op">=</span> <span class="va">beta_1</span>, col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lm_obj</span>, col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<!-- 
::: {.callout-caution}

## Coding Challenge (Nr 1):

Use the above `mySimpleRegrDataGenerator()` function to generate many, for instance $B=10000$, realizations of the artificial data generating process, and based on these $B$ many realizations of $\hat\beta_0$ and $\hat\beta_1:$
$$
\hat\beta_{0,1},\hat\beta_{0,2},\dots,\hat\beta_{0,B}
$$
$$
\hat\beta_{1,1},\hat\beta_{1,2},\dots,\hat\beta_{1,B}
$$
The averages of these realizations can be used to approximate the true means of the estimators, i.e.
$$
\begin{align*}
E(\hat\beta_0)&\approx \frac{1}{B}\sum_{b=1}^B\hat\beta_{0,b}\\[2ex]
E(\hat\beta_1)&\approx \frac{1}{B}\sum_{b=1}^B\hat\beta_{1,b}
\end{align*}
$$
and thus also the biases
$$
\begin{align*}
\operatorname{Bias}(\hat\beta_0)&\approx \frac{1}{B}\sum_{b=}^B\hat\beta_{0,b} -\beta_0\\
\operatorname{Bias}(\hat\beta_1)&\approx \frac{1}{B}\sum_{b=}^B\hat\beta_{0,b}-\beta_1.
\end{align*}
$$
The approximations become arbitrarily precise as $B\to\infty$ due to the Law of Large Numbers. 

**Question:** Are the estimators $\hat\beta_0$ and $\hat\beta_1$ unbiased for the artificial data generating process defined by our `mySimpleRegrDataGenerator()` function? 
:::

-->
<p>The magnitude of the estimation errors <span class="math display">\[
\hat\beta_0-\beta_0\neq 0 \quad\text{and}\quad\hat\beta_1-\beta_1\neq 0.
\]</span> is expressed in unites of <strong>Standard Errors</strong>: <span class="math display">\[
\operatorname{SE}(\hat\beta_0)=\sqrt{Var(\hat\beta_1)}=\sqrt{\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right]}
\]</span> and <span class="math display">\[
\operatorname{SE}(\hat\beta_1)=\sqrt{Var(\hat\beta_1)}=\sqrt{\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}},
\]</span> where <span class="math display">\[\begin{align*}
\sigma^2 &amp; =Var(\epsilon_i)\\
\Leftrightarrow\quad \sigma  &amp;  =SD(\epsilon_i) = \sqrt{Var(\epsilon_i)}
\end{align*}\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
<p>Typically, the variance and thus also the <strong>S</strong>tandard <strong>D</strong>eviation of the error term, <span class="math inline">\(\sigma = SD(\epsilon),\)</span> are unknown, but we can estimate <span class="math inline">\(\sigma\)</span> by the <strong>R</strong>esidual <strong>S</strong>tandard <strong>E</strong>rror: <span class="math display">\[\begin{align*}
\hat{\sigma}=\operatorname{RSE}
&amp;=\sqrt{\frac{1}{n-2}\operatorname{RSS}}\\[2ex]
&amp;=\sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y}_i)^2}\\[2ex]
&amp;=\sqrt{\frac{1}{n-2}\sum_{i=1}^n\left(y_i-\left(\hat{\beta}_0+\hat{\beta}_1x_i\right)\right)^2}
\end{align*}\]</span> where <span class="math inline">\(\operatorname{RSS}=\sum_{i=1}^n(y_i-\hat{y}_i)^2\)</span> are the residual sum of squares.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We subtract <span class="math inline">\(2\)</span> from the sample size <span class="math inline">\(n\)</span> since <span class="math inline">\(n-2\)</span> are the remaining degrees of freedom in the data after estimating two parameters <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>.</p>
</div>
</div>
<section id="confidence-intervals" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h4>
<p>Knowing the standard errors <span class="math display">\[
\operatorname{SE}(\hat\beta_0)\quad\text{and}\quad\operatorname{SE}(\hat\beta_1)
\]</span> allows us to construct approximate 95% <strong>Confidence Intervals</strong>: <span class="math display">\[
\begin{align*}
\operatorname{CI}_{\beta_0}
&amp;=\left[\hat{\beta}_0-2\operatorname{SE}(\hat\beta_0),\;
        \hat{\beta}_0+2\operatorname{SE}(\hat\beta_0)\right] = \hat\beta_0\pm 2\operatorname{SE}(\hat\beta_0)\\[2ex]
\operatorname{CI}_{\beta_1}
&amp;=\left[\hat{\beta}_1-2\operatorname{SE}(\hat\beta_1),\;
        \hat{\beta}_1+2\operatorname{SE}(\hat\beta_1)\right] = \hat\beta_1\pm 2\operatorname{SE}(\hat\beta_1)
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is approximately a 95% change (in resamplings from the data generating process) that the <strong>random confidence interval</strong> <span class="math inline">\(\operatorname{CI}_{\beta_1}\)</span> contains the true (fix) parameter value <span class="math inline">\(\beta_1\)</span>.</p>
<p>To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:</p>
<ul>
<li><a href="https://rpsychologist.com/d3/ci/">Interactive visualization for interpreting confidence intervals</a></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Only the above frequentist point of view can be nicely interpreted. Interpreting a given, observed confidence interval is hard and often done wrong.</p>
<p><img src="images/CI_meme.jpg" class="img-fluid"></p>
<p>A given, observed confidence interval (computed from the observed realization of the training data) either contains the true parameter value or not and usually we do not know it.</p>
</div>
</div>
</section><section id="confidence-intervals-for-statistical-hypothesis-testing" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="confidence-intervals-for-statistical-hypothesis-testing">Confidence Intervals for Statistical Hypothesis Testing</h4>
<p>We can use a <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> confidence interval to do statistical hypothesis testing at the significance level <span class="math inline">\(0&lt;\alpha&lt;1.\)</span> Typical significance levels:</p>
<ul>
<li><span class="math inline">\(\alpha=0.05\)</span></li>
<li><span class="math inline">\(\alpha=0.01\)</span></li>
</ul>
<p>Let us consider the following null-hypothesis <span class="math inline">\((H_0)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_1\)</span> equals the <strong>null-hypothetical value</strong> <span class="math inline">\(\beta^{(H_0)}_{1}\)</span> versus the two-sided alternative hypothesis <span class="math inline">\((H_1)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_1\)</span> does not equal the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1}:\)</span> <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta^{(H_0)}_{1}\\
H_1:&amp;\;\beta_1\neq \beta^{(H_0)}_{1}
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Classic No-Effect Null-Hypothesis
</div>
</div>
<div class="callout-body-container callout-body">
<p>For the special case, where the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1}=0\)</span> we test the classic no-effect null-hypothesis: <span class="math display">\[
\begin{align*}
H_0:&amp;\;\text{There is no relationship between $Y$ and $X$; i.e. $\beta_1=0$}\\
H_1:&amp;\;\text{There is a relationship between $Y$ and $X$; i.e. $\beta_1\neq 0$}
\end{align*}
\]</span></p>
</div>
</div>
<p><strong>Testing-Procedure:</strong></p>
<ul>
<li><p>If the observed (obs) realization of the confidence interval, <span class="math inline">\(\operatorname{CI}_{\beta_1,obs},\)</span> <strong>contains</strong> the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1},\)</span> i.e. <span class="math display">\[
\begin{align*}
\beta^{(H_0)}_{1}&amp;\in\operatorname{CI}_{\beta_1,obs}\\
\Leftrightarrow\beta^{(H_0)}_{1}&amp;\in\left[\hat{\beta}_{1,obs}-2\operatorname{SE}_{obs}(\hat\beta_1),\;\hat{\beta}_{1,obs}+2\operatorname{SE}_{obs}(\hat\beta_1)\right],
\end{align*}
\]</span> then we <strong>cannot reject</strong> the null hypothesis that <span class="math inline">\(\beta_1=\beta^{(H_0)}_{1}.\)</span></p></li>
<li><p>If, however, the observed (obs) realization of the confidence interval, <span class="math inline">\(\operatorname{CI}_{\beta_1,obs},\)</span> does <strong>not contain</strong> the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1},\)</span> i.e. <span class="math display">\[
\begin{align*}
\beta^{(H_0)}_{1}&amp;\not\in\operatorname{CI}_{\beta_1,obs}\\
\Leftrightarrow\beta^{(H_0)}_{1}&amp;\not\in\left[\hat{\beta}_{1,obs}-2\operatorname{SE}_{obs}(\hat\beta_1),\;\hat{\beta}_{1,obs}+2\operatorname{SE}_{obs}(\hat\beta_1)\right],
\end{align*}
\]</span> then we <strong>can reject</strong> the null hypothesis and adopt the alternative that <span class="math inline">\(\beta_1\neq\beta^{(H_0)}_{1}.\)</span></p></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Probability of a Type I Error is Smaller than <span class="math inline">\(\alpha\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the null-hypothesis is true, i.e.&nbsp;if the true (unknown) <span class="math inline">\(\beta_1\)</span> equals the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1},\)</span> <span class="math display">\[
\beta_1 = \beta^{(H_0)}_{1}
\]</span> then the <strong>random confidence interval</strong> <span class="math inline">\(\operatorname{CI}_{\beta_1}\)</span> covers the deterministic null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1}\)</span> with probability at least <span class="math inline">\(1-\alpha\)</span> <span class="math display">\[
P(\beta^{(H_0)}_{1} \in \operatorname{CI}_{\beta_1}| H_0\text{ is true})\geq 1-\alpha.
\]</span> I.e., in <span class="math inline">\(100\)</span> resamples we expect to see at least <span class="math inline">\((1-\alpha)\cdot 100\)</span> coverage events.</p>
<p><strong>Probability of a Type I Error:</strong> Thus, the probability of falsely rejecting the null hypothesis even though the null hypothesis is true (i.e.&nbsp;doing a type I error) is smaller or equal to the chosen significance level <span class="math inline">\(\alpha,\)</span> since <span class="math display">\[
\begin{align*}
P(\beta^{(H_0)}_{1} \in \operatorname{CI}_{\beta_1}| H_0\text{ is true})&amp;\geq 1-\alpha\\
\Leftrightarrow\quad 1-P(\beta^{(H_0)}_{1} \not\in \operatorname{CI}_{\beta_1}| H_0\text{ is true})&amp;\geq 1-\alpha\\
\Leftrightarrow\quad \underbrace{P(\beta^{(H_0)}_{1} \not\in \operatorname{CI}_{\beta_1}| H_0\text{ is true})}_{\text{Probability of a Type I Error}}&amp;\leq \alpha
\end{align*}
\]</span> Note: A type I error (rejecting <span class="math inline">\(H_0\)</span> even though <span class="math inline">\(H_0\)</span> is true) is also called a false positive event. We want that false positives happen only (very) rarely and therefore choose a small significance level <span class="math inline">\(\alpha\)</span> such as <span class="math inline">\(\alpha=0.05\)</span> or <span class="math inline">\(\alpha=0.01.\)</span></p>
</div>
</div>
</section><section id="test-statistics-for-statistical-hypothesis-testing" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="test-statistics-for-statistical-hypothesis-testing">Test Statistics for Statistical Hypothesis Testing</h4>
<p>Standard errors can also be used to construct test statistics for statistical hypothesis testing. In the following, we look at the <span class="math inline">\(t\)</span>-test statistic.</p>
<p>Choose a significance level <span class="math inline">\(0&lt;\alpha&lt;1\)</span> such as, for instance,</p>
<ul>
<li><span class="math inline">\(\alpha=0.05\)</span></li>
<li><span class="math inline">\(\alpha=0.01\)</span></li>
</ul>
<p>Let us (again) consider the null-hypothesis <span class="math inline">\((H_0)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_1\)</span> equals the <strong>null-hypothetical value</strong> <span class="math inline">\(\beta^{(H_0)}_{1}\)</span> versus the two-sided alternative hypothesis <span class="math inline">\((H_1)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_1\)</span> does not equal the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1}:\)</span> <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta^{(H_0)}_{1}\\
H_1:&amp;\;\beta_1\neq \beta^{(H_0)}_{1}
\end{align*}
\]</span></p>
<p>Under <span class="math inline">\(H_0,\)</span> i.e.&nbsp;if the (unknown) true parameter <span class="math inline">\(\beta_1\)</span> equals the null-hypothetical value, i.e.&nbsp;if <span class="math inline">\(\beta_1=\beta^{(H_0)}_{1},\)</span> the <strong>random</strong> <span class="math inline">\(t\)</span>-test statistic has a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\((n-2)\)</span> degrees of freedom, <span class="math display">\[
t=\frac{\hat\beta_1 - \beta^{(H_0)}_{1}}{\operatorname{SE}(\hat\beta_1)}\overset{H_0}{\sim}t_{(n-2)}.
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the multiple linear regression model <span class="math inline">\(Y_i=\beta_0+\sum_{j=1}^p\beta_jX_{ij}+\epsilon_i\)</span> (<a href="#sec-MultLinReg" class="quarto-xref"><span>Section 6.2</span></a>), the <span class="math inline">\(t\)</span>-test statistic for the <span class="math inline">\(j\)</span>th predictor has the following null-distribution: <span class="math display">\[
t=\frac{\hat\beta_j - \beta^{(H_0)}_{j}}{\operatorname{SE}(\hat\beta_j)}\overset{H_0}{\sim}t_{(n-p-1)}.
\]</span></p>
</div>
</div>
<p><strong><span class="math inline">\(p\)</span>-value:</strong> The <span class="math inline">\(p\)</span>-value is the probability of seeing a realization of the <strong>random</strong> <span class="math inline">\(t\)</span>-test statistic, <span class="math inline">\(t,\)</span> which is more extreme than the observed value of the test-statistic, <span class="math inline">\(t_{obs},\)</span> <span class="math display">\[\begin{align*}
p_{obs}
&amp;=P_{H_0}\left(|t|\geq|t_{obs}|\right)\\[2ex]
&amp;=2\cdot\min\{P_{H_0}\left(t\geq t_{obs} \right),\; P_{H_0}\left(t\leq t_{obs} \right)\}.
\end{align*}\]</span> where, <span class="math inline">\(P_{H_0}\)</span> means that the probability is computed “under <span class="math inline">\(H_0\)</span>;” i.e.&nbsp;for the scenario that <span class="math inline">\(H_0\)</span> is true <span class="math inline">\((\beta_1=\beta^{(H_0)}_{1}).\)</span></p>
<p><strong>Testing-Procedure:</strong> <!-- To do the statistical hypothesis test, we need to select a significance level $\alpha$ (e.g.,  $\alpha=0.05$ or $\alpha=0.01$).  --></p>
<ul>
<li><p>If the observed realization of the <span class="math inline">\(p\)</span>-value is larger than or equal to the significance level <span class="math display">\[
p_{obs}\geq \alpha,
\]</span> then we cannot reject the null hypothesis.</p></li>
<li><p>If, however, the observed realization of the <span class="math inline">\(p\)</span>-value is strictly smaller than the significance level <span class="math display">\[
p_{obs}&lt;\alpha,
\]</span> then we can reject the null hypothesis and adopt the alternative hypothesis.</p></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It can be shown that the above statistical hypothesis test based on confidence intervals is equivalent to the statistical hypothesis test based on the <span class="math inline">\(t\)</span>-test statistic.</p>
<p>In both cases, the probability of falsely rejecting the null hypothesis even though the null hypothesis is true, is smaller than or equal to the chosen significance level <span class="math inline">\(\alpha.\)</span></p>
</div>
</div>
<p><img src="images/Tab_3_1.png" class="img-fluid"></p>
</section></section><section id="assessing-the-accuracy-of-the-model" class="level3" data-number="6.1.3"><h3 data-number="6.1.3" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-model">
<span class="header-section-number">6.1.3</span> Assessing the Accuracy of the Model</h3>
<p>In tendency an accurate model has …</p>
<ul>
<li><p>a low residual standard error <span class="math inline">\(\operatorname{RSE}\)</span> <span class="math display">\[
\operatorname{RSE}=\hat\sigma=\sqrt{\frac{\operatorname{RSS}}{n-2}}
\]</span></p></li>
<li><p>a high <span class="math inline">\(R^2\)</span></p></li>
</ul>
<p><span class="math display">\[
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
\]</span> where <span class="math inline">\(0\leq R^2\leq 1\)</span> and <span class="math display">\[
\begin{align*}
\operatorname{TSS}&amp;=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2\\
\operatorname{RSS}&amp;=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2\\
\hat{y}_i&amp;=\hat\beta_0+\hat\beta_1x_i
\end{align*}
\]</span></p>
<p>TSS: “Total Sum of Squares”</p>
<p>RSS: “Residual Sum of Squares”</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Cautionary Note Nr 1:</strong> Do not forget that there is a <strong>irreducible error</strong> <span class="math inline">\(Var(\epsilon)=\sigma^2&gt;0\)</span>. Thus</p>
<ul>
<li>very low <span class="math inline">\(\operatorname{RSE}\)</span> values, <span class="math inline">\(\operatorname{RSE}\approx 0\)</span>, and</li>
<li>very high <span class="math inline">\(R^2\)</span> values, <span class="math inline">\(R^2\approx 1\)</span>,</li>
</ul>
<p>can be warning signals indicating overfitting. While overfitting typically does not happen with a simple linear regression model, it can happen with a multiple linear regression model.</p>
<p><strong>Cautionary Note Nr 2:</strong> The <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\operatorname{RSE}\)</span> are only based on <em>training</em> data. In <a href="Ch2_StatLearning.html" class="quarto-xref"><span>Chapter 2</span></a>, we have seen that a proper assessment of the model accuracy needs to take into account <em>test</em> data.</p>
</div>
</div>
<!--   -->
<section id="r2-and-correlation-coefficient-1" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="r2-and-correlation-coefficient-1">
<span class="math inline">\(R^2\)</span> and correlation coefficient</h4>
<p>In the case of the simple linear regression model, <span class="math inline">\(R^2\)</span> equals the squared sample correlation coefficient between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, <span class="math display">\[
R^2 = r_{yx}^2,
\]</span> where <span class="math display">\[
r_{yx}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}.
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the multiple linear regression model <span class="math inline">\(Y_i=\beta_0+\sum_{j=1}^p\beta_jX_{ij}+\epsilon_i\)</span> (<a href="#sec-MultLinReg" class="quarto-xref"><span>Section 6.2</span></a>), the <span class="math inline">\(R^2\)</span> equals the squared correlation between response and the fitted values: <span class="math display">\[
R^2=r^2_{y\hat{y}}
\]</span> with <span class="math display">\[
r_{y\hat{y}}=\frac{\sum_{i=1}^n(y_i-\bar{y})(\hat{y}_i-\bar{\hat{y}})}{\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}\sqrt{\sum_{i=1}^n(\hat{y}_i-\bar{\hat{y}})^2}}.
\]</span></p>
</div>
</div>
</section></section></section><section id="sec-MultLinReg" class="level2" data-number="6.2"><h2 data-number="6.2" class="anchored" data-anchor-id="sec-MultLinReg">
<span class="header-section-number">6.2</span> Multiple Linear Regression</h2>
<p>The multiple linear regression model allows for more than only one predictor:<br><span class="math display">\[
Y\approx \beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p + \epsilon
\]</span></p>
For instance,
<center>
<code>sales</code> <span class="math inline">\(\approx \beta_0 + \beta_1\)</span> <code>TV</code> <span class="math inline">\(+\beta_2\)</span> <code>radio</code> <span class="math inline">\(+\beta_3\)</span> <code>newspaper</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<section id="estimating-the-regression-coefficients-1" class="level3" data-number="6.2.1"><h3 data-number="6.2.1" class="anchored" data-anchor-id="estimating-the-regression-coefficients-1">
<span class="header-section-number">6.2.1</span> Estimating the Regression Coefficients</h3>
<p>Let <span class="math display">\[
(X_{11},\dots,X_{1p},Y_1),\dots,(X_{n1},\dots,X_{np},Y_n)
\]</span> denote a training data random sample. I.e.<br><span class="math display">\[
(X_{i1},\dots,X_{ip},Y_i)\overset{iid}{\sim}(X_{1},\dots,X_{p},Y)
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
<p>Moreover, let <span id="eq-MultLinReg1"><span class="math display">\[
Y_i=\beta_0+\beta_1X_{i1}+\dots+\beta_p X_{ip}+\epsilon_i
\qquad(6.3)\]</span></span> for all <span class="math inline">\(i=1,\dots,n,\)</span> where</p>
<ul>
<li><span class="math inline">\((p+1)&lt;n\)</span></li>
<li>
<span class="math inline">\(\epsilon_i\)</span> and the vector of predictors <span class="math inline">\((X_{i1},\dots,X_{ip})\)</span> are independent from each other for all <span class="math inline">\(i=1,\dots,n\)</span>
</li>
<li>
<span class="math inline">\(E(\epsilon_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>
</li>
<li>
<span class="math inline">\(Var(\epsilon_i)=\sigma^2&gt;0\)</span> is constant for all <span class="math inline">\(i=1,\dots,n\)</span>
</li>
</ul>
<p>Typically, it is more convenient to write the multiple linear regression model in vector and matrix notation. Let <span class="math display">\[\begin{align*}
X_i&amp;=\left(\begin{matrix}X_{i0}\\X_{i1}\\ \vdots\\X_{ip}\end{matrix}\right)\quad\text{and}\quad
\beta=\left(\begin{matrix}\beta_{0}\\\beta_1\\ \vdots\\\beta_{p}\end{matrix}\right)
\end{align*}\]</span> with <span class="math inline">\(X_{i0}=1\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
<p>This allows us to write <a href="#eq-MultLinReg1" class="quarto-xref">Equation&nbsp;<span>6.3</span></a> more compactly as <span class="math display">\[
Y_i=X_i'\beta+\epsilon_i \quad\text{for all}\quad i=1,\dots,n.
\]</span></p>
<p>Next, we can stack all components. Let <span class="math display">\[\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right)\quad\text{and}\quad
\epsilon=\left(\begin{matrix}\epsilon_1\\ \vdots\\\epsilon_n\end{matrix}\right)
\end{equation*}\]</span> denote the <span class="math inline">\((n\times 1)\)</span> vectors containing all response values <span class="math inline">\(Y_i\)</span> and all error terms <span class="math inline">\(\epsilon_i\)</span> of the random sample. Moreover, let <span class="math display">\[\begin{align*}
X
&amp;=\left(\begin{matrix}
X_{10}&amp;X_{11}&amp;\dots&amp;X_{1p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
X_{n0}&amp;X_{n1}&amp;\dots&amp;X_{np}\\
\end{matrix}\right)\\[2ex]
&amp;=\left(\begin{matrix}
\;\;1\;\;&amp;X_{11}&amp;\dots&amp;X_{1p}\\
\;\;\vdots\;\;&amp;\vdots&amp;\ddots&amp;\vdots\\
\;\;1\;\;&amp;X_{n1}&amp;\dots&amp;X_{np}\\
\end{matrix}\right)
\end{align*}\]</span> denote the <span class="math inline">\((n\times (p+1))\)</span>-dimensional matrix containing all predictor values of the random sample, where the first column is a column full of ones <span class="math inline">\((X_{10}=1,\dots,X_{n0}=0).\)</span></p>
<p>This allows us to write <a href="#eq-MultLinReg1" class="quarto-xref">Equation&nbsp;<span>6.3</span></a> even more compactly as <span class="math display">\[
Y=X\beta+\epsilon.
\]</span></p>
<p>Using matrix algebra, it can be shown that the ordinary least squares estimator of <span class="math inline">\(\beta=(\beta_0,\beta_1,\dots,\beta_p)'\)</span> is given by the following <span class="math inline">\(((p+1)\times 1)\)</span> vector: <span class="math display">\[
\left(\begin{matrix}\hat\beta_1\\ \vdots \\ \hat\beta_K\end{matrix}\right)=\hat{\beta}=(X'X)^{-1}X'Y.
\]</span></p>
<p>For a given observed realization of the training data random sample <span class="math display">\[
(x_{11},\dots,x_{1p},y_1),\dots,(x_{n1},\dots,x_{np},y_n),
\]</span> the estimator <span class="math inline">\(\hat\beta=(\beta_0,\beta_1,\dots,\beta_p)'\)</span> is selected by minimizing <span class="math display">\[\begin{align*}
\operatorname{RSS}
&amp;=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2\\[2ex]
&amp;=\sum_{i=1}^n\left(y_i-\left(\hat\beta_0 + \hat\beta_1 x_{i1} \dots + \hat\beta_p x_{ip}\right)\right)^2.
\end{align*}\]</span></p>
<p><img src="images/Fig_3_4.png" class="img-fluid"></p>
<p>Since the estimator <span class="math display">\[
\hat\beta=\left(\begin{matrix}\hat\beta_1\\ \vdots \\ \hat\beta_K\end{matrix}\right)
\]</span> depends on the random sample, it is itself a <span class="math inline">\((p+1)\)</span>-dimensional <strong>random variable</strong>. For each realization of the training data random sample, we observe a realization of the estimator.</p>
<p>In an actual data analysis, however, we only have <em>one</em> realization of the estimators <span class="math inline">\(\hat\beta_0,\hat\beta_1,\dots,\hat\beta_p\)</span> computed from the given training dataset (i.e.&nbsp;the observed realization of the training data random sample).</p>
<section id="interpretation-of-multiple-linear-regressions-and-the-omitted-variable-bias" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="interpretation-of-multiple-linear-regressions-and-the-omitted-variable-bias">Interpretation of Multiple Linear Regressions and the Omitted Variable Bias</h4>
<p>Multiple linear regression is more than mere composition of single simple linear regression models. Take a look at the following two simple linear regression results:</p>
<p><img src="images/Tab_3_3.png" class="img-fluid"></p>
<p>Observations:</p>
<ul>
<li><p>In the first simple linear regression, we see a statistical significant effect of <code>radio</code> on <code>sales</code>; i.e.&nbsp;we can reject the null hypotheses <span class="math display">\[
H_0:\beta_{radio}=0
\]</span> and adopt the alternative hypotheses <span class="math display">\[
H_1:\beta_{radio}\neq 0.
\]</span></p></li>
<li><p>In the second simple linear regression, we see a statistical significant effect of <code>newspaper</code> on <code>sales</code>; i.e.&nbsp;we can reject the null hypotheses <span class="math display">\[
H_0:\beta_{newspaper}=0
\]</span> and adopt the alternative hypotheses <span class="math display">\[
H_1:\beta_{newspaper}\neq 0.
\]</span></p></li>
</ul>
<p>By contrast, when looking at the <strong>multiple linear regression</strong> when regressing <code>sales</code> onto</p>
<ul>
<li>
<code>TV</code>,</li>
<li>
<code>radio</code> and<br>
</li>
<li>
<code>newspaper</code>,</li>
</ul>
<p>then the effect of <code>newspaper</code> becomes statistically <strong>insignificant</strong>; see Table 3.4.</p>
<p><img src="images/Tab_3_4.png" class="img-fluid"></p>
<p><strong>Reason: Omitted Variable Bias</strong></p>
<p>The reason for this change from a statistically significant effect of <code>newspaper</code> in the simple linear regression, to an insignificant effect in the multiple linear regression is the so-called <strong>Omitted Variable Bias</strong>.</p>
<p>Explanation of the omitted variables bias:</p>
<ul>
<li>
<code>radio</code> has a <strong>true positive effect</strong> on <code>sales</code>
</li>
<li>
<code>newspaper</code> has actually <strong>no effect</strong> on <code>sales</code>
</li>
<li>But, <code>newspaper</code> is correlated with <code>radio</code> <span class="math inline">\(r_{\texttt{newspaper},\texttt{radio}}=0.3541\)</span>; see Table 3.5</li>
</ul>
<p><img src="images/Tab_3_5.png" class="img-fluid"></p>
<ul>
<li>Thus, when <strong>omitting</strong> <code>radio</code> from the regression model, <code>newspaper</code> becomes a surrogate for <code>radio</code> and we see a spurious effect.</li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Interpreting statistically significant results as true effects (“a change of <span class="math inline">\(X_j\)</span> by one unit causes on average a change in <span class="math inline">\(Y\)</span> by <span class="math inline">\(\beta_{j}\)</span>”) is a delectate thing.</p>
<p>Even if the <span class="math inline">\(f(X)\)</span> is really so simple that we can write it as a simple or multiple linear regression model, we may miss to include all relevant predictor variables and thus statistically significant results may only be spurious effects due to omitted variables.</p>
</div>
</div>
<p><strong>Interpretation of the Coefficients in Table 3.4</strong></p>
<p>For fixed values of <code>TV</code> and <code>newspaper</code>, spending additionally 1000 USD for <code>radio</code>, increases on average <code>sales</code> by approximately 189 units.</p>
</section></section><section id="inference-on-beta_1dotsbeta_p" class="level3" data-number="6.2.2"><h3 data-number="6.2.2" class="anchored" data-anchor-id="inference-on-beta_1dotsbeta_p">
<span class="header-section-number">6.2.2</span> Inference on <span class="math inline">\(\beta_1,\dots,\beta_p\)</span>
</h3>
<p>The <span class="math inline">\(t\)</span>-test statistic (equivalently the confidence interval for <span class="math inline">\(\beta_j\)</span>) allows us to test a null-hypothesis about <strong>one</strong> parameter <span class="math inline">\(\beta_j.\)</span></p>
<p>To test whether there is a relationship between the response <span class="math inline">\(Y\)</span> and total <em>vector</em> predictors <span class="math inline">\((X_1,\dots,X_p)\)</span> we can use the <span class="math inline">\(F\)</span>-test statistic.</p>
<p>In this case, the <span class="math inline">\(F\)</span>-test tests the null-hypothesis <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta_2=\dots=\beta_p=0\\
\text{versus}\quad H_1:&amp;\;\text{at least one $\beta_j\neq 0$; $j=1,\dots,p$}
\end{align*}
\]</span></p>
<p><span class="math inline">\(F\)</span>-test statistic <span class="math display">\[
F=\frac{(\operatorname{TSS}-\operatorname{RSS})/p}{\operatorname{
  RSS}/(n-p-1)}\overset{H_0}{\sim} F_{p,n-p-1}
\]</span> Under <span class="math inline">\(H_0,\)</span> i.e.&nbsp;if <span class="math inline">\(H_0\)</span> is true, the <span class="math inline">\(F\)</span>-test statistic has a <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(p\)</span> numerator and <span class="math inline">\((n-p-1)\)</span> denominator degrees of freedom.</p>
<p>If <span class="math inline">\(H_0\)</span> is correct <span class="math display">\[
\begin{align*}
E((\operatorname{TSS}-\operatorname{RSS})/p)&amp;=\sigma^2\\[2ex]
E(\operatorname{RSS}/(n-p-1))&amp;=\sigma^2
\end{align*}
\]</span></p>
<p>Therefore:</p>
<ul>
<li>If <span class="math inline">\(H_0\)</span> is correct, we expect values of <span class="math inline">\(F\approx 1.\)</span>
</li>
<li>If <span class="math inline">\(H_1\)</span> is correct, we expect values of <span class="math inline">\(F\gg 1.\)</span>
</li>
</ul>
<p><span class="math inline">\(p\)</span>-value</p>
<p><span class="math display">\[\begin{align*}
p_{obs}
&amp;=P_{H_0}\left( F \geq F_{obs} \right),
\end{align*}\]</span> where <span class="math inline">\(F_{obs}\)</span> denotes the observed value of the <span class="math inline">\(F\)</span>-test statistic computed from the observed training data, and where <span class="math inline">\(F\)</span> is a random variable that has a <span class="math inline">\(F_{q,n-p-1}\)</span> distribution. <span class="math inline">\(P_{H_0}\)</span> means that the probability is computed for the scenario that <span class="math inline">\(H_0\)</span> is true.</p>
<p>To do the statistical hypothesis test, we need to select a significance level <span class="math inline">\(\alpha\)</span> (e.g.&nbsp;<span class="math inline">\(\alpha=0.01\)</span> or <span class="math inline">\(\alpha=0.05\)</span>).</p>
<ul>
<li><p>If the observed realization of the <span class="math inline">\(p\)</span>-value is larger than or equal to the significance level <span class="math display">\[
p_{obs}\geq \alpha,
\]</span> then we cannot reject the null hypothesis.</p></li>
<li><p>If, however, the observed realization of the <span class="math inline">\(p\)</span>-value is strictly smaller than the significance level <span class="math display">\[
p_{obs}&lt;\alpha,
\]</span> then we can reject the null hypothesis and adopt the alternative hypothesis.</p></li>
</ul>
<!-- Caution: Cannot be computed if $p>n$. (Chapter 6 on "high dimensional problems") --></section></section><section id="other-considerations-in-the-regression-model" class="level2" data-number="6.3"><h2 data-number="6.3" class="anchored" data-anchor-id="other-considerations-in-the-regression-model">
<span class="header-section-number">6.3</span> Other Considerations in the Regression Model</h2>
<section id="qualitative-predictors" class="level3" data-number="6.3.1"><h3 data-number="6.3.1" class="anchored" data-anchor-id="qualitative-predictors">
<span class="header-section-number">6.3.1</span> Qualitative Predictors</h3>
<p>Often some predictors are <em>qualitative</em> variables (also known as a <em>factor</em> variables). For instance, the <code>Credit</code> dataset contains the following qualitative predictors:</p>
<ul>
<li>
<code>own</code> (house ownership: yes/no)</li>
<li>
<code>student</code> (student status: yes/no)</li>
<li>
<code>status</code> (marital status: yes/no)</li>
<li>
<code>region</code> (regions: east, west or south)</li>
</ul>
<section id="predictors-with-only-two-levels" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="predictors-with-only-two-levels">Predictors with Only Two Levels</h4>
<p>If a <strong>qualitative predictor (factor)</strong> only has two levels (i.e.&nbsp;possible values), then incorporating it into a regression model is very simple: We simply create an indicator or <strong>dummy variable</strong> that takes on two possible numerical values; for instance, <span class="math display">\[
x_{i} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person owns a house}\\
  0&amp;\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
\]</span> Using this dummy variable as a predictor in the regression equation results in the following regression model: <span class="math display">\[\begin{align*}
y_{i}
&amp;=\beta_0 + \beta_1 x_i + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i &amp;\quad \text{if the $i$th person owns a house}\\
  \beta_0 + \epsilon_i           &amp;\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span>: The average credit card balance among those who do not own a house</li>
<li>
<span class="math inline">\(\beta_0+\beta_1\)</span>: The average credit card balance among those who do own a house</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The average difference in credit card balance between owners and non-owners</li>
</ul>
<p><img src="images/Tab_3_7.png" class="img-fluid"></p>
<p>Alternatively, instead of a 0/1 coding scheme, we could create a dummy variable <span class="math display">\[
x_{i} = \left\{
  \begin{array}{ll}
  1 &amp;\quad \text{if the $i$th person owns a house}\\
-1 &amp;\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
\]</span> <span class="math display">\[\begin{align*}
y_{i}
&amp;=\beta_0 + \beta_1 x_i + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i&amp;\quad \text{if the $i$th person owns a house}\\
  \beta_0 - \beta_1 + \epsilon_i&amp;\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span>: The overall average credit card balance (ignoring the house ownership effect)</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The average amount by which house owners and non-owners have credit card balances that are above and below the overall average, respectively.</li>
</ul></section><section id="qualitative-predictors-with-more-than-two-levels" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</h4>
<p>When a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the</p>
<p><code>region</code> <span class="math inline">\(\in\{\)</span><code>South</code>, <code>West</code>, <code>East</code><span class="math inline">\(\}\)</span></p>
<p>variable, we create <strong>two</strong> dummy variables. The first could be <span class="math display">\[
x_{i1} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person is from the South}\\
  0&amp;\quad \text{if the $i$th person is not from the South,}
  \end{array}\right.
\]</span> and the second could be <span class="math display">\[
x_{i2} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person is from the West}\\
  0&amp;\quad \text{if the $i$th person is not from the West.}
  \end{array}\right.
\]</span> Using both of these dummy variables results in the following regression model: order to obtain the model <span class="math display">\[\begin{align*}
y_{i}&amp;=\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1  + \epsilon_i&amp; \quad \text{if the $i$th person is from the South}\\
  \beta_0 + \beta_2  + \epsilon_i&amp; \quad \text{if the $i$th person is from the West}\\
  \beta_0            + \epsilon_i&amp; \quad \text{if the $i$th person is from the East.}\\
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span>: The average credit card balance for individuals from the East</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The difference in the average balance between people from the South versus the East</li>
<li>
<span class="math inline">\(\beta_2\)</span>: The difference in the average balance between people from the West versus the East</li>
</ul>
<p><img src="images/Tab_3_8.png" class="img-fluid"></p>
<p>There are many different ways of coding qualitative variables besides the dummy variable approach taken here. All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular <strong>contrasts</strong>. (A detailed discussion of <em>contrasts</em> is beyond the scope of this lecture.)</p>
</section></section><section id="extensions-of-the-linear-model" class="level3" data-number="6.3.2"><h3 data-number="6.3.2" class="anchored" data-anchor-id="extensions-of-the-linear-model">
<span class="header-section-number">6.3.2</span> Extensions of the Linear Model</h3>
<section id="interaction-effects" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="interaction-effects">Interaction Effects</h4>
<p>Previously, we used the following model</p>
<center>
<code>sales</code> <span class="math inline">\(= \beta_0 + \beta_1\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\)</span> <code>radio</code> <span class="math inline">\(+ \beta_3\)</span> <code>newspaper</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<p>which states, for instance, that the average increase in <code>sales</code> associated with a one-unit increase in <code>TV</code> is <span class="math inline">\(\beta_1,\)</span> regardless of the amount spent on <code>radio</code>.</p>
<p>However, this simple model may be incorrect. Suppose that there is a synergy effect, such that spending money on <code>radio</code> advertising actually increases the effectiveness of <code>TV</code> advertising.</p>
<p>Figure 3.5 suggests that such an effect may be present in the advertising data:</p>
<ul>
<li>When levels of either <code>TV</code> or <code>radio</code> are low, then the true <code>sales</code> are lower than predicted by the linear model.</li>
<li>But when advertising is split between the two media, then the model tends to <strong>underestimate</strong> sales. <img src="images/Fig_3_5.png" class="img-fluid">
</li>
</ul>
<p><strong>Solution: Interaction Effects:</strong></p>
<p>Consider the standard linear regression model with two variables, <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon.
\]</span> Here each predictor <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> has a given effect, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, on <span class="math inline">\(Y\)</span> and this effect does not depend on the value of the other predictor. <strong>(Additive Assumption)</strong></p>
<p>One way of extending this model is to include a third predictor, called an <strong>interaction term</strong>, which is constructed by computing the product of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2.\)</span> This results in the model <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 \overbrace{\color{red}X_1X_2}^{=X_3} + \epsilon.
\]</span> This is a powerful extension relaxing the additive assumption. Notice that the model can now be written as <span class="math display">\[
\begin{align*}
Y &amp;= \beta_0 + \underbrace{(\beta_1 + \beta_3 X_2)}_{=\tilde{\beta}_1(X_2)} X_1 + \beta_2 X_2 + \epsilon,
\end{align*}
\]</span> where the new slope parameter <span class="math inline">\(\tilde{\beta}_2(X_2)\)</span> is a linear function of <span class="math inline">\(X_2,\)</span> i.e. <span class="math display">\[
\tilde{\beta}_1(X_2)=\beta_1 + \beta_3 X_2.
\]</span></p>
<p>Thus, a change in the value of <span class="math inline">\(X_2\)</span> will change the association between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y.\)</span></p>
<p>A similar argument shows that a change in the value of <span class="math inline">\(X_1\)</span> changes the association between <span class="math inline">\(X_2\)</span> and <span class="math inline">\(Y.\)</span></p>
<p>Let us return to the <code>Advertising</code> example: A linear model that predicts <code>sales</code> using</p>
<ul>
<li>the predictor <code>radio</code>,</li>
<li>the predictor <code>TV</code>, and</li>
<li>the interaction <code>radio</code><span class="math inline">\(\times\)</span><code>radio</code>
</li>
</ul>
<p>takes the form</p>
<center>
<code>sales</code> <span class="math inline">\(= \beta_0 + \beta_1\times\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\times\)</span> <code>radio</code> <span class="math inline">\(+ \beta_3\times(\)</span> <code>radio</code><span class="math inline">\(\times\)</span> <code>TV</code><span class="math inline">\()+\epsilon\)</span>
</center>
<p> which can be rewritten as</p>
<center>
<code>sales</code> <span class="math inline">\(=\beta_0 + (\beta_1+ \beta_3\times\)</span> <code>radio</code> <span class="math inline">\()\times\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\times\)</span> <code>radio</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<p><br></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_3\)</span> denotes the increase in the effectiveness of TV advertising associated with a one-unit increase in radio advertising.</li>
</ul>
<p><img src="images/Tab_3_9.png" class="img-fluid"></p>
<p><strong>Interpretation of Table 3.9:</strong></p>
<ul>
<li>Both separate main effects, <code>TV</code> and <code>radio</code>, are statistically significant (<span class="math inline">\(p\)</span>-values smaller than 0.01).</li>
<li>Additionally, the <span class="math inline">\(p\)</span>-value for the interaction term, <code>TV</code><span class="math inline">\(\times\)</span><code>radio</code>, is extremely low, indicating that there is strong evidence for <span class="math inline">\(H_1: \beta_3\neq 0.\)</span> In other words, it is clear that the true relationship is not additive.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hierarchical Principle of Interaction Terms
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we include an interaction in a model, we should also include the main effects, even if the <span class="math inline">\(p\)</span>-values associated with their coefficients are not significant.</p>
</div>
</div>
<p><strong>Interactions with Qualitative Variables:</strong></p>
<p>An interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation.</p>
<p>Consider the <code>Credit</code> data set and suppose that we wish to predict <code>balance</code> using the predictors:</p>
<ul>
<li>
<code>income</code> (quantitative) and</li>
<li>
<code>student</code> (qualitative) using a dummy variable with <span class="math display">\[
x_{i2}=\left\{
\begin{array}{ll}
1&amp;\text{if person $i$ is a student}\\
0&amp;\text{if not}\\
\end{array}
\right.
\]</span>
</li>
</ul>
<p>In the absence of an interaction term, the model takes the form <img src="images/Eq_3_34.png" class="img-fluid"></p>
<p>Thus, the regression lines for students and non-students have different intercepts, <span class="math inline">\(\beta_0+\beta_2\)</span> versus <span class="math inline">\(\beta_0\)</span>, <strong>but the same slope</strong> <span class="math inline">\(\beta_1\)</span>.</p>
<p>This represents a potentially serious limitation of the model, since a change in <code>income</code> may have a very different effect on the credit card <code>balance</code> of a student versus a non-student.</p>
<p>This limitation can be addressed by adding an interaction variable, created by multiplying <code>income</code> with the dummy variable for student. Our model now becomes <img src="images/Eq_3_35.png" class="img-fluid"></p>
<p>Now we have different intercepts for students and non-students but also different slopes for these groups. <img src="images/Fig_3_7.png" class="img-fluid"></p>
</section><section id="polynomial-regression-non-linear-relationships" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="polynomial-regression-non-linear-relationships">Polynomial Regression: Non-linear Relationships</h4>
<p>Polynomial regression allows to accommodate non-linear relationships between the predictors <span class="math inline">\(X\)</span> and the outcome <span class="math inline">\(Y.\)</span> <img src="images/Fig_3_8.png" class="img-fluid"></p>
<p>For example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that a model of the form</p>
<center>
<code>mpg</code> <span class="math inline">\(=\beta_0 + \beta_1\times\)</span> <code>horsepower</code> <span class="math inline">\(+ \beta_2\times(\)</span><code>horsepower</code><span class="math inline">\()^2+\epsilon\)</span>
</center>
<p></p>
<p>This regression model involves predicting <code>mpg</code> using a non-linear function of <code>horsepower</code>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>But it is still a linear model!</strong> It’s simply a multiple linear regression model <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
\]</span> with</p>
<ul>
<li>
<span class="math inline">\(X_1=\)</span><code>horsepower</code> and</li>
<li>
<span class="math inline">\(X_2 =(\)</span><code>horsepower</code><span class="math inline">\()^2\)</span>
</li>
</ul>
<p>as the predictor variables.</p>
</div>
</div>
<p>Since this is nothing but a multiple linear regression model, we can use standard linear regression software to estimate <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> in order to fit the (quadratic) non-linear regression function.</p>
<p><img src="images/Tab_3_10.png" class="img-fluid"></p>
</section></section><section id="potential-problems" class="level3" data-number="6.3.3"><h3 data-number="6.3.3" class="anchored" data-anchor-id="potential-problems">
<span class="header-section-number">6.3.3</span> Potential Problems</h3>
<p><strong>1. Non-linearity of the response-predictor relationships.</strong></p>
<p><strong>Diagnostic residual plots</strong> are most useful to detect possible non-linear response-predictor relationships.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span> </span>
<span></span>
<span><span class="co">## Gives the variable names in the Auto dataset</span></span>
<span><span class="co"># names(Auto)</span></span>
<span></span>
<span><span class="co">## Simple linear regression</span></span>
<span><span class="va">lmobj_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Quadratic regression </span></span>
<span><span class="va">lmobj_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Diagnostic Plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_1</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_2</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Plotting <strong>residuals versus fitted values</strong>, i.e.&nbsp;plotting the data pairs <span class="math display">\[
(\underbrace{y_i - \hat{y}_i}_{=e_i}, \hat{y}_i),\quad\text{for all}\quad i=1,\dots,n
\]</span> is a useful graphical tool for identifying non-linearity which works for the</p>
<ul>
<li>
<strong>simple linear regression model</strong> with <span class="math inline">\(\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_{i1}\)</span> and the</li>
<li>
<strong>multiple linear regression model</strong> with <span class="math inline">\(\hat{y}_i=\hat{\beta}_0+\sum_{j=1}^p\hat{\beta}_jx_{ij}\)</span>
</li>
</ul>
<p>If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as <span class="math display">\[
\log(X),\; \sqrt{X},\; \text{or}\; X^2
\]</span> in the regression model.</p>
<!-- In the later chapters, we will discuss other more advanced non-linear approaches for addressing this issue. -->
<p><strong>2. Correlation of Error Terms</strong></p>
<p>An important assumption of the linear regression model is that the error terms, <span class="math display">\[
\epsilon_1, \epsilon_2, \dots , \epsilon_n,
\]</span> are independent and thus uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that <span class="math inline">\(\epsilon_i\)</span> is positive provides little or no information about the sign of <span class="math inline">\(\epsilon_{i+1}.\)</span></p>
<p>Auto-correlations among the error terms typically occur in time series data. Figure 3.10 shows time-series of residuals with</p>
<ul>
<li>no auto-correlation (<span class="math inline">\(\rho=0\)</span>)</li>
<li>intermediate auto-correlation (<span class="math inline">\(\rho=0.5\)</span>)</li>
<li>strong auto-correlation (<span class="math inline">\(\rho=0.9\)</span>)</li>
</ul>
<p><img src="images/Fig_3_10.png" class="img-fluid"></p>
<p><strong>3. Non-Constant Variance of Error Terms (Heteroskedasticity)</strong></p>
<p>Another important assumption of the linear regression model is that the error terms have a constant variance, <span class="math display">\[
Var(\epsilon_i) = \sigma^2,\quad\text{for all}\quad i=1,\dots,n.
\]</span></p>
<p>One can identify <strong>non-constant variances (“heteroskedasticity”)</strong> in the errors, using diagnostic residual plots.</p>
<p>Often one observes that the magnitude of the scattering of the residuals tends to increase with the fitted values. When faced with this problem, one possible solution is to transform the response <span class="math inline">\(Y\)</span> using a concave function such as <span class="math display">\[
\log(Y)\;\text{ or }\; \sqrt{Y}.
\]</span> Such a transformation results in a greater amount of shrinkage of the larger responses.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Quadratic regression </span></span>
<span><span class="va">lmobj_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Quadratic regression with transformed response log(Y)</span></span>
<span><span class="va">lmobj_3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">)</span><span class="op">)</span> <span class="op">~</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Diagnostic Plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_2</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_3</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>The standard formulas for</p>
<ul>
<li>standard errors,</li>
<li>confidence intervals, and</li>
<li>hypothesis tests</li>
</ul>
<p>in this chapter are based on the assumption of</p>
<ul>
<li>uncorrelated error terms <span class="math inline">\(Cor(\epsilon_i,\epsilon_j)=0\)</span> for all <span class="math inline">\(i\neq j\)</span><br>
</li>
<li>with equal variances <span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span>
</li>
</ul>
<p>If in fact there is correlation and/or heteroskedasticity among the error terms, then the estimated standard errors will be wrong leading to invalid inferences.</p>
<p>Thus, if the error terms are auto-correlated and/or heteroskedastic, we need to take this into account by using so-called auto-correlation and/or heteroskedasticity robust standard errors.</p>
<p>The <code>R</code> package <a href="https://cran.r-project.org/web/packages/sandwich/index.html">sandwich</a> contains such robust standard error estimators.</p>
</div>
</div>
<p><strong>4. Outliers</strong></p>
<p>An outlier is a point <span class="math inline">\(i\)</span> for which <span class="math inline">\(y_i\)</span> is far from the value <span class="math inline">\(\hat{y}_i\)</span> predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p>
<p>Outliers typically have a strong effect on the <span class="math inline">\(R^2\)</span> value since they add a <strong>very large residual</strong> to its computation.</p>
<p><strong>Harmless Outlier:</strong> Figure 3.12 shows a clear outlier (observation 20) which, however, has a typical predictor value <span class="math inline">\(x_i\)</span>; i.e.&nbsp;the <span class="math inline">\(x_i\)</span>-value is right in the center of all predicor values. Such outliers have little effect on the regression fit. <img src="images/Fig_3_12.png" class="img-fluid"></p>
<p><strong>Harmful Outlier:</strong> Figure 3.13 shows again a clear outlier (observation 41) which, however, has a predictor value <span class="math inline">\(x_i\)</span> that is <strong>very atypical</strong>. Such outliers are said to have <strong>large leverage</strong> giving them power to affect the regression fit considerably. <img src="images/Fig_3_13.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Critical outliers have both,</p>
<ul>
<li><strong>large residuals</strong></li>
</ul>
<p><em>and</em></p>
<ul>
<li>
<strong>large leverage</strong>.</li>
</ul>
</div>
</div>
<p><strong>5. High Leverage Points</strong></p>
<p>In order to quantify an observation’s leverage, we compute the <strong>leverage statistic</strong> <span class="math inline">\(h_i\)</span> for each observation <span class="math inline">\(i=1,\dots,n.\)</span> A large value of this statistic indicates an observation with high leverage.</p>
<p>In the case of the <strong>simple linear regression model</strong> <span class="math display">\[
h_i = \frac{1}{n} + \frac{(x_i-\bar{x})^2}{\sum_{i'=1}^n(x_{i'}-\bar{x})^2}.
\]</span></p>
<p>In the case of the <strong>multiple linear regression model</strong>, <span class="math inline">\(h_i\)</span> is the <span class="math inline">\(i\)</span>th diagonal value of the <span class="math inline">\((n\times n)\)</span>-dimensional “hat-matrix” <span class="math display">\[
H=X(X'X)^{-1}X'.
\]</span></p>
<ul>
<li>The leverage statistic <span class="math inline">\(h_i\)</span> is always between <span class="math inline">\(1/n\)</span> and <span class="math inline">\(1\)</span>
</li>
<li>The average leverage for all the observations is equal to <span class="math display">\[
\bar{h}=\frac{1}{n}\sum_{i=1}^n h_i=(p + 1)/n.
\]</span>
</li>
<li>If a given observation has a leverage statistic <span class="math inline">\(h_i\)</span> that greatly exceeds the average leverage value, <span class="math inline">\((p+1)/n,\)</span> then we may suspect that the corresponding point has high leverage.</li>
</ul>
<p><strong>6. Collinearity</strong></p>
<p>Collinearity refers to the situation in which two or more predictor variables are closely related to one another.</p>
<p>In the following example, the variables <code>Age</code> and <code>Limit</code> are essentially unrelated, but the variables <code>Rating</code> and <code>Limit</code> are closely related to one another.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Credit</span><span class="op">)</span> <span class="co"># names(Credit)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Age</span>,    x <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Limit</span>, main <span class="op">=</span> <span class="st">"No Collinearity"</span>, ylab <span class="op">=</span> <span class="st">"Age"</span>, xlab <span class="op">=</span> <span class="st">"Limit"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Rating</span>, x <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Limit</span>, main <span class="op">=</span> <span class="st">"Strong Collinearity"</span>, ylab <span class="op">=</span> <span class="st">"Rating"</span>, xlab <span class="op">=</span> <span class="st">"Limit"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The left panel of Figure 3.15 shows that, in the case of unrelated predictors (<code>Age</code> and <code>Limit</code>), the least squares problem has a minimum <span class="math inline">\((\hat\beta_{Age},\hat\beta_{Limit})\)</span> that is well identified since the minimum is well defined.</p>
<p>The right panel of Figure 3.15 shows that, in the case of collinear predictors (<code>Rating</code> and <code>Limit</code>), the least squares problem has a minimum <span class="math inline">\((\hat\beta_{Rating},\hat\beta_{Limit})\)</span> that is not well identified: One can substitute values of <span class="math inline">\(\hat\beta_{Limit}'\)</span> for <span class="math inline">\(\hat\beta_{Rating}'\)</span> ending up in new pairs <span class="math inline">\((\hat\beta_{Rating}',\hat\beta_{Limit}')\)</span> with basically the same RSS-value than the original value than it is achieved by the minimizer <span class="math inline">\((\hat\beta_{Rating},\hat\beta_{Limit})\)</span>.</p>
<p><img src="images/Fig_3_15.png" class="img-fluid"></p>
<p>Table 3.11 demonstrates that this identification problem between the collinear predictors (<code>Rating</code> and <code>Limit</code>) causes a variance inflation in the variance (square of standard error) of the estimators <span class="math inline">\(\hat\beta_{Rating}\)</span> and <span class="math inline">\(\hat\beta_{Limit}.\)</span></p>
<ul>
<li>In Model 1: <span class="math inline">\(\hat\beta_{Limit} = 0.005^2=0.000025\)</span>
</li>
<li>In Model 2: <span class="math inline">\(\hat\beta_{Rating} = 0.064^2=0.004096\)</span>
</li>
</ul>
<p><img src="images/Tab_3_11.png" class="img-fluid"></p>
<p>We call this situation <strong>multicollinearity</strong>.</p>
<p>To detect multicollinearity issues, one can use the variance inflation factor (VIF) <span class="math display">\[
\operatorname{VIF}(\hat{\beta}_j)=\frac{1}{1-R^2_{X_j|X_-j}},
\]</span> where <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors.</p>
<ul>
<li>If <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is close to one, then multicollinearity is present, and <span class="math inline">\(\operatorname{VIF}(\hat{\beta}_j)\)</span> will be large.</li>
</ul>
<p>In the <code>Credit</code> data, one gets for the predictors <code>age</code>, <code>rating</code>, and <code>limit</code> the following VIF values:</p>
<ul>
<li>1.01 (<code>age</code>)</li>
<li>160.67 (<code>rating</code>)</li>
<li>160.59 (<code>limit</code>)</li>
</ul>
<p>Thus, as we suspected, there is considerable collinearity in the data!</p>
<p>Possible solutions:</p>
<ol type="1">
<li><p>Drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. <br><strong>Caution:</strong> In econometrics, dropping control variables is generally not a good idea since control variables are there to rule out possible issues with omitted variables biases.</p></li>
<li><p>Combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.</p></li>
<li><p>Use a different estimation procedure like ridge regression.</p></li>
<li><p>Live with it. At least you know where the large stand errors are coming from.</p></li>
</ol></section></section><section id="comparison-linear-regression-vs.-k-nn-regression" class="level2" data-number="6.4"><h2 data-number="6.4" class="anchored" data-anchor-id="comparison-linear-regression-vs.-k-nn-regression">
<span class="header-section-number">6.4</span> Comparison: Linear Regression vs.&nbsp;K-NN Regression</h2>
<p>Linear regression is an example of a parametric approach because it assumes a linear model form for <span class="math inline">\(f(X).\)</span></p>
<p><strong>Advantages of parametric approaches:</strong></p>
<ul>
<li>Typically easy to fit</li>
<li>Simple interpretation</li>
<li>Simple inference</li>
</ul>
<p><strong>Disadvantages of parametric approaches:</strong></p>
<ul>
<li>The parametric model assumption can be far from true; i.e. <span class="math display">\[
f(X) \neq \beta_0+\beta_1X_1+\dots+\beta_pX_p
\]</span>
</li>
</ul>
<p>Alternative: <strong>Non-parametric methods</strong> such as <em>K-nearest neighbors regression</em> since non-parametric approaches do not explicitly assume a parametric form for <span class="math inline">\(f(X).\)</span></p>
<section id="k-nearest-neighbors-k-nn-regression" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="k-nearest-neighbors-k-nn-regression">K-Nearest Neighbors (K-NN) Regression</h4>
<p>Given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0,\)</span> KNN regression regression computes <span class="math inline">\(\hat{f}(x_0)\)</span> in two steps:</p>
<ol type="1">
<li>identify the <span class="math inline">\(K\)</span> training observations that are closest to <span class="math inline">\(x_0\)</span>, represented by the index set <span class="math inline">\(\mathcal{N}_0\subset\{1,2,\dots,n_{Train}\}.\)</span>
</li>
<li>estimate <span class="math inline">\(f(x_0)\)</span> using the average of all the training responses <span class="math inline">\(y_i\)</span> with <span class="math inline">\(i\in\mathcal{N}_0,\)</span> i.e.&nbsp; <span class="math display">\[
\hat{f}(x_0)=\frac{1}{K}\sum_{i\in\mathcal{N}_0}y_i.
\]</span>
</li>
</ol>
<p>The left panel of Figure 3.16 shows the estimation result for <span class="math inline">\(K=1\)</span> and the right panel for <span class="math inline">\(K=9.\)</span></p>
<p><img src="images/Fig_3_16.png" class="img-fluid"></p>
<p>In general, the optimal value for <span class="math inline">\(K\)</span> will depend on the <em>bias-variance tradeoff</em>, which we introduced in <a href="Ch2_StatLearning.html" class="quarto-xref"><span>Chapter 2</span></a>:</p>
<ul>
<li>A small value for <span class="math inline">\(K\)</span> provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent, e.g., on just one observation if <span class="math inline">\(K=1\)</span>.</li>
<li>A large value of <span class="math inline">\(K\)</span> provides a less flexible fit. The prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in <span class="math inline">\(f(X).\)</span>
</li>
</ul>
<p>An optimal value of <span class="math inline">\(K\)</span> can be chosen using, e.g., cross-validation; see <a href="Ch6_ResamplingMethods.html" class="quarto-xref"><span>Chapter 6</span></a>.</p>
<p>Generally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of <span class="math inline">\(f\)</span> and vice versa.</p>
<p>Figure 3.17 provides an example with data generated from a one-dimensional linear regression model:</p>
<ul>
<li>black solid lines: true <span class="math inline">\(f(x)\)</span>
</li>
<li>blue curves: KNN fits <span class="math inline">\(\hat{f}(x)\)</span> using <span class="math inline">\(K = 1\)</span> (left plot) and <span class="math inline">\(K = 9\)</span> (right plot).</li>
</ul>
<p>Observations:</p>
<ul>
<li>The KNN fit <span class="math inline">\(\hat{f}(x)\)</span> using <span class="math inline">\(K = 1\)</span> is far too wiggly</li>
<li>The KNN fit <span class="math inline">\(\hat{f}(x)\)</span> using <span class="math inline">\(K = 9\)</span> is much closer to the true <span class="math inline">\(f(X).\)</span>
</li>
</ul>
<p>However, since the true regression function is here linear, it is hard for a non-parametric approach to compete with simple linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. <img src="images/Fig_3_17.png" class="img-fluid"></p>
<p>The blue dashed line in the left-hand panel of Figure 3.18 represents the simple linear regression fit to the same data. It is almost perfect. The right-hand panel of Figure 3.18 reveals that linear regression outperforms KNN for this data. <img src="images/Fig_3_18.png" class="img-fluid"></p>
<p>Figure 3.19 displays a non-linear situations in which KNN performs much better than simple linear regression. <img src="images/Fig_3_19.png" class="img-fluid"></p>
</section><section id="curse-of-dimensionality" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="curse-of-dimensionality">Curse of Dimensionality</h4>
<p>Unfortunately, in higher dimensions, KNN often performs worse than simple/multiple linear regression, since non-parametric approaches suffer from the <strong>curse of dimensionality</strong>.</p>
<p>Figure 3.20 considers the same strongly non-linear situation as in the second row of Figure 3.19, except that we have added additional noise (i.e.&nbsp;redundant) predictors that are not associated with the response.</p>
<ul>
<li>When <span class="math inline">\(p = 1\)</span> or <span class="math inline">\(p = 2,\)</span> KNN outperforms linear regression.</li>
<li>But for <span class="math inline">\(p = 3\)</span> the results are mixed, and for <span class="math inline">\(p\geq 4\)</span> linear regression is superior to KNN. <img src="images/Fig_3_20.png" class="img-fluid">
</li>
</ul>
<p>Observations:</p>
<ul>
<li>When <span class="math inline">\(p=1\)</span>, a sample size of <span class="math inline">\(n=50\)</span> can provide enough information to estimate <span class="math inline">\(f(X)\)</span> accurately using non-parametric methods since the <span class="math inline">\(K\)</span> nearest neighbors can actually be close to a given test observation <span class="math inline">\(x_0.\)</span>
</li>
<li>However, when spreading the <span class="math inline">\(n=50\)</span> data points over a large number of, for instance, <span class="math inline">\(p=20\)</span> dimensions, the <span class="math inline">\(K\)</span> nearest neighbors tend to become far away from <span class="math inline">\(x_0.\)</span>
</li>
</ul></section></section><section id="self-study-r-lab-linear-regression" class="level2" data-number="6.5"><h2 data-number="6.5" class="anchored" data-anchor-id="self-study-r-lab-linear-regression">
<span class="header-section-number">6.5</span> Self-Study <code>R</code>-Lab: Linear Regression</h2>
<section id="libraries" class="level3" data-number="6.5.1"><h3 data-number="6.5.1" class="anchored" data-anchor-id="libraries">
<span class="header-section-number">6.5.1</span> Libraries</h3>
<p>The <code><a href="https://rdrr.io/r/base/library.html">library()</a></code> function is used to load <em>libraries</em>, or groups of functions and data sets that are not included in the base <code>R</code> distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries.</p>
<p>Here we load the <code>MASS</code> package, which is a very large collection of data sets and functions. We also load the <code>ISLR2</code> package, which includes the data sets associated with this book.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as <code>MASS</code>, come with <code>R</code> and do not need to be separately installed on your computer. However, other packages, such as <code>ISLR2</code>, must be downloaded the first time they are used. <!-- 
This can be done directly from within `R`. For example, on a Windows system,  select the `Install package` option under the `Packages` tab.  After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and `R` will automatically download the package. Alternatively,  --></p>
<p>This can be done, for instance, at the <code>R</code> command line via <code>install.packages("ISLR2")</code> function. This installation only needs to be done the first time you use a package. However, the <code><a href="https://rdrr.io/r/base/library.html">library()</a></code> function must be called within each <code>R</code> session.</p>
</section><section id="simple-linear-regression-1" class="level3" data-number="6.5.2"><h3 data-number="6.5.2" class="anchored" data-anchor-id="simple-linear-regression-1">
<span class="header-section-number">6.5.2</span> Simple Linear Regression</h3>
<p>The <code>ISLR2</code> library contains the <code>Boston</code> data set, which records <code>medv</code> (median house value) for <span class="math inline">\(506\)</span> census tracts in Boston. We will seek to predict <code>medv</code> using <span class="math inline">\(12\)</span> predictors such as <code>rmvar</code> (average number of rooms per house), <code>age</code> (average age of houses), and <code>lstat</code> (percent of households with low socioeconomic status).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat
1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21
  medv
1 24.0
2 21.6
3 34.7
4 33.4
5 36.2
6 28.7</code></pre>
</div>
</div>
<p>To find out more about the data set, we can type <code><a href="https://rdrr.io/pkg/ISLR2/man/Boston.html">?Boston</a></code>.</p>
<p>We will start by using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to fit a simple linear regression model, with <code>medv</code> as the response and <code>lstat</code> as the predictor. The basic syntax is <code>lm(y ~ x, data)</code>, where <code>y</code> is the response, <code>x</code> is the predictor, and <code>data</code> is the data set in which these two variables are kept.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(predvars, data, env): object 'medv' not found</code></pre>
</div>
</div>
<p>The command causes an error because <code>R</code> does not know where to find the variables <code>medv</code> and <code>lstat</code>.</p>
<p>The next line tells <code>R</code> that the variables are in <code>Boston</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Alternatively, we can attach the <code>Boston</code> object:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/attach.html">attach</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">)</span></span>
<span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we type <code>lm.fit</code>, some basic information about the model is output. For more detailed information, we use <code>summary(lm.fit)</code>. This gives us <span class="math inline">\(p\)</span>-values and standard errors for the coefficients, as well as the <span class="math inline">\(R^2\)</span> statistic and <span class="math inline">\(F\)</span>-statistic for the model.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat)

Coefficients:
(Intercept)        lstat  
      34.55        -0.95  </code></pre>
</div>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.168  -3.990  -1.318   2.034  24.500 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***
lstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.216 on 504 degrees of freedom
Multiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 
F-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can use the <code><a href="https://rdrr.io/r/base/names.html">names()</a></code> or the <code><a href="https://rdrr.io/r/utils/str.html">str()</a></code> function in order to find out what other pieces of information are stored in <code>lm.fit</code>.</p>
<p>We can extract these quantities by name—e.g.&nbsp;<code>lm.fit$coefficients</code>.</p>
<p>For some objects, there are also specific extractor functions like <code><a href="https://rdrr.io/r/stats/coef.html">coef()</a></code> to access them.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"        </code></pre>
</div>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)       lstat 
 34.5538409  -0.9500494 </code></pre>
</div>
</div>
<p>In order to obtain a confidence interval for the coefficient estimates, we can use the <code><a href="https://rdrr.io/r/stats/confint.html">confint()</a></code> command.</p>
<p>Type <code>confint(lm.fit)</code> at the command line to obtain the confidence intervals for the linear regression coefficients.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                2.5 %     97.5 %
(Intercept) 33.448457 35.6592247
lstat       -1.026148 -0.8739505</code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function can be used to produce confidence intervals and prediction intervals for the prediction of <code>medv</code> for a given value of <code>lstat</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lm.fit</span>, <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>lstat <span class="op">=</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">10</span>, <span class="fl">15</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>        interval <span class="op">=</span> <span class="st">"confidence"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit      lwr      upr
1 29.80359 29.00741 30.59978
2 25.05335 24.47413 25.63256
3 20.30310 19.73159 20.87461</code></pre>
</div>
</div>
<p>For instance, the observed value of the 95% confidence interval associated with a <code>lstat</code> value of <span class="math inline">\(10,\)</span> i.e.&nbsp;associated with <span class="math display">\[
\beta_0 + \beta_1 \cdot 10
\]</span> is <span class="math display">\[
\operatorname{CI}_{\beta_0 + \beta_1 \cdot 10,obs}=[24.47, 25.63]
\]</span> with observed prediction value <span class="math display">\[
25.05335 = \hat\beta_0 + \hat\beta_1 \cdot 10.
\]</span></p>
<!-- predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), 
        interval = "prediction")
The 95\% prediction interval associated with a `lstat` value of 10 is $(12.828, 37.28)$. 

As expected, the confidence and prediction intervals are centered around the same point (a predicted value of $\hat{y}_i=25.05$ for `medv` when $x_{i2}=$`lstat` equals 10), but the latter are substantially wider.

::: {.callout-caution}
The `predict()` function with the option `interval = "prediction"` assumes **Gaussian** error terms; i.e. 
$$
\epsilon_i\sim\mathcal{N}(0,\sigma^2),\quad\text{for all}\quad i=1,\dots,n.
$$
If this distributional assumption is not fulfilled, you should not use the prediction interval.
::: -->
<p>We will now plot <code>medv</code> and <code>lstat</code> along with the least squares regression line using the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> and <code><a href="https://rdrr.io/r/graphics/abline.html">abline()</a></code> functions.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>There is some evidence for non-linearity in the relationship between <code>lstat</code> and <code>medv</code>. We will explore this issue later in this lab.</p>
<p>The <code><a href="https://rdrr.io/r/graphics/abline.html">abline()</a></code> function can be used to draw any line, not just the least squares regression line. To draw a line with intercept <code>a</code> and slope <code>b</code>, we type <code>abline(a, b)</code>. Below we experiment with some additional settings for plotting lines and points. The <code>lwd = 3</code> command causes the width of the regression line to be increased by a factor of 3; this works for the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> and <code><a href="https://rdrr.io/r/graphics/lines.html">lines()</a></code> functions also. We can also use the <code>pch</code> option to create different plotting symbols.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lm.fit</span>, lwd <span class="op">=</span> <span class="fl">3</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk10-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span>, pch <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk10-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span>, pch <span class="op">=</span> <span class="st">"+"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk10-4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, pch <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk10-5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Next we examine some diagnostic plots. Four diagnostic plots are automatically produced by applying the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> function directly to the output from <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>. In general, this command will produce one plot at a time, and hitting <em>Enter</em> will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the <code><a href="https://rdrr.io/r/graphics/par.html">par()</a></code> and <code>mfrow()</code> functions, which tell <code>R</code> to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, <code>par(mfrow = c(2, 2))</code> divides the plotting region into a <span class="math inline">\(2 \times 2\)</span> grid of panels.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk11-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Alternatively, we can compute the residuals from a linear regression fit using the <code><a href="https://rdrr.io/r/stats/residuals.html">residuals()</a></code> function. The function <code><a href="https://rdrr.io/r/stats/influence.measures.html">rstudent()</a></code> will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk12-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">rstudent</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk12-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>On the basis of the residual plots, there is some evidence of non-linearity.</p>
<p>Leverage statistics can be computed using the <code><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues()</a></code> function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk13-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.max</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>375 
375 </code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/base/which.min.html">which.max()</a></code> function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span>, decreasing <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       375        415        374 
0.02686517 0.02495670 0.02097101 </code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/base/sort.html">sort()</a></code> function can be used to sort and print values of a vector like <code>hatvalues(lm.fit)</code>.</p>
</section><section id="multiple-linear-regression" class="level3" data-number="6.5.3"><h3 data-number="6.5.3" class="anchored" data-anchor-id="multiple-linear-regression">
<span class="header-section-number">6.5.3</span> Multiple Linear Regression</h3>
<p>In order to fit a multiple linear regression model using least squares, we again use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function. The syntax <code>lm(y ~ x1 + x2 + x3)</code> is used to fit a model with three predictors, <code>x1</code>, <code>x2</code>, and <code>x3</code>. The <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function now outputs the regression coefficients for all the predictors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span> <span class="op">+</span> <span class="va">age</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + age, data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.981  -3.978  -1.283   1.968  23.158 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***
lstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***
age          0.03454    0.01223   2.826  0.00491 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.173 on 503 degrees of freedom
Multiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 
F-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code>Boston</code> data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ ., data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.595  -2.730  -0.518   1.777  26.199 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
zn           4.642e-02  1.373e-02   3.382 0.000778 ***
indus        2.056e-02  6.150e-02   0.334 0.738288    
chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
rm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***
age          6.922e-04  1.321e-02   0.052 0.958229    
dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
black        9.312e-03  2.686e-03   3.467 0.000573 ***
lstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.745 on 492 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 
F-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can access the individual components of a summary object by name (type <code><a href="https://rdrr.io/r/stats/summary.lm.html">?summary.lm</a></code> to see what is available). Hence <code>summary(lm.fit)$r.sq</code> gives us the <span class="math inline">\(R^2\)</span>, and <code>summary(lm.fit)$sigma</code> gives us the RSE. The <code><a href="https://rdrr.io/pkg/car/man/vif.html">vif()</a></code> function, part of the <code>car</code> package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The <code>car</code> package is not part of the base <code>R</code> installation so it must be downloaded the first time you use it via the <code><a href="https://rdrr.io/r/utils/install.packages.html">install.packages()</a></code> function in <code>R</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span><span class="op">)</span> <span class="co"># contains the vif() function</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span> <span class="co"># computes the VIF statistics and sorts them</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    chas    black     crim  ptratio       rm       zn    lstat      age 
1.073995 1.348521 1.792192 1.799084 1.933744 2.298758 2.941491 3.100826 
     dis    indus      nox      rad      tax 
3.955945 3.991596 4.393720 7.484496 9.008554 </code></pre>
</div>
</div>
<p>What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, <code>age</code> has a high <span class="math inline">\(p\)</span>-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except <code>age</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="va">age</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ . - age, data = Boston)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.6054  -2.7313  -0.5188   1.7601  26.2243 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***
crim         -0.108006   0.032832  -3.290 0.001075 ** 
zn            0.046334   0.013613   3.404 0.000719 ***
indus         0.020562   0.061433   0.335 0.737989    
chas          2.689026   0.859598   3.128 0.001863 ** 
nox         -17.713540   3.679308  -4.814 1.97e-06 ***
rm            3.814394   0.408480   9.338  &lt; 2e-16 ***
dis          -1.478612   0.190611  -7.757 5.03e-14 ***
rad           0.305786   0.066089   4.627 4.75e-06 ***
tax          -0.012329   0.003755  -3.283 0.001099 ** 
ptratio      -0.952211   0.130294  -7.308 1.10e-12 ***
black         0.009321   0.002678   3.481 0.000544 ***
lstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.74 on 493 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 
F-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Alternatively, the <code><a href="https://rdrr.io/r/stats/update.html">update()</a></code> function can be used.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span><span class="op">(</span><span class="va">lm.fit</span>, <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="va">age</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="interaction-terms" class="level3" data-number="6.5.4"><h3 data-number="6.5.4" class="anchored" data-anchor-id="interaction-terms">
<span class="header-section-number">6.5.4</span> Interaction Terms</h3>
<p>It is easy to include interaction terms in a linear model using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function. The syntax <code>lstat:black</code> tells <code>R</code> to include an interaction term between <code>lstat</code> and <code>black</code>. The syntax <code>lstat * age</code> simultaneously includes <code>lstat</code>, <code>age</code>, and the interaction term <code>lstat</code><span class="math inline">\(\times\)</span><code>age</code> as predictors; it is a shorthand for <code>lstat + age + lstat:age</code>. %We can also pass in transformed versions of the predictors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span> <span class="op">*</span> <span class="va">age</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat * age, data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.806  -4.045  -1.333   2.085  27.552 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***
lstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***
age         -0.0007209  0.0198792  -0.036   0.9711    
lstat:age    0.0041560  0.0018518   2.244   0.0252 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.149 on 502 degrees of freedom
Multiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 
F-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section><section id="non-linear-transformations-of-the-predictors" class="level3" data-number="6.5.5"><h3 data-number="6.5.5" class="anchored" data-anchor-id="non-linear-transformations-of-the-predictors">
<span class="header-section-number">6.5.5</span> Non-linear Transformations of the Predictors</h3>
<p>The <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function can also accommodate non-linear transformations of the predictors. For instance, given a predictor <span class="math inline">\(X\)</span>, we can create a predictor <span class="math inline">\(X^2\)</span> using <code>I(X^2)</code>. The function <code><a href="https://rdrr.io/r/base/AsIs.html">I()</a></code> is needed since the <code>^</code> has a special meaning in a formula object; wrapping as we do allows the standard usage in <code>R</code>, which is to raise <code>X</code> to the power <code>2</code>. We now perform a regression of <code>medv</code> onto <code>lstat</code> and <code>lstat^2</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">lstat</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + I(lstat^2))

Residuals:
     Min       1Q   Median       3Q      Max 
-15.2834  -3.8313  -0.5295   2.3095  25.4148 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***
lstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***
I(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.524 on 503 degrees of freedom
Multiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 
F-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The near-zero <span class="math inline">\(p\)</span>-value associated with the quadratic term suggests that it leads to an improved model. We use the <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> function to further quantify the extent to which the quadratic fit is superior to the linear fit.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">lm.fit</span>, <span class="va">lm.fit2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: medv ~ lstat
Model 2: medv ~ lstat + I(lstat^2)
  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
1    504 19472                                 
2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Here Model 1 represents the linear submodel containing only one predictor, <code>lstat</code>, while Model 2 corresponds to the larger quadratic model that has two predictors, <code>lstat</code> and <code>lstat^2</code>. The <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior.</p>
<p>Here the <span class="math inline">\(F\)</span>-statistic is <span class="math inline">\(135\)</span> and the associated <span class="math inline">\(p\)</span>-value is virtually zero. This provides very clear evidence that the model containing the predictors <code>lstat</code> and <code>lstat^2</code> is far superior to the model that only contains the predictor <code>lstat</code>.</p>
<p>This is not surprising, since earlier we saw evidence for non-linearity in the relationship between <code>medv</code> and <code>lstat</code>.</p>
<p>If we type</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lm.fit2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_OLD_WS_2324_files/figure-html/chunk22-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>then we see that when the <code>lstat^2</code> term is included in the model, there is little discernible pattern in the residuals.</p>
<p>In order to create a <strong>cubic fit</strong>, we can include a predictor of the form <code>I(X^3)</code>. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function to create the polynomial within <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>. For example, the following command produces a fifth-order polynomial fit:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit5</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ poly(lstat, 5))

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5433  -3.1039  -0.7052   2.0844  27.1153 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***
poly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***
poly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***
poly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***
poly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***
poly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.215 on 500 degrees of freedom
Multiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 
F-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>This suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant <span class="math inline">\(p\)</span>-values in a regression fit.</p>
<p>By default, the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function, the argument <code>raw = TRUE</code> must be used.</p>
<p>Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">rm</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ log(rm), data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-19.487  -2.875  -0.104   2.837  39.816 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***
log(rm)       54.055      2.739   19.73   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.915 on 504 degrees of freedom
Multiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 
F-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section><section id="qualitative-predictors-1" class="level3" data-number="6.5.6"><h3 data-number="6.5.6" class="anchored" data-anchor-id="qualitative-predictors-1">
<span class="header-section-number">6.5.6</span> Qualitative Predictors</h3>
<p>We will now examine the <code>Carseats</code> data, which is part of the <code>ISLR2</code> library. We will attempt to predict <code>Sales</code> (child car seat sales) in <span class="math inline">\(400\)</span> locations based on a number of predictors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Carseats</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education
1  9.50       138     73          11        276   120       Bad  42        17
2 11.22       111     48          16        260    83      Good  65        10
3 10.06       113     35          10        269    80    Medium  59        12
4  7.40       117    100           4        466    97    Medium  55        14
5  4.15       141     64           3        340   128       Bad  38        13
6 10.81       124    113          13        501    72       Bad  78        16
  Urban  US
1   Yes Yes
2   Yes Yes
3   Yes Yes
4   Yes Yes
5   Yes  No
6    No Yes</code></pre>
</div>
</div>
<p>The <code>Carseats</code> data includes qualitative predictors such as <code>shelveloc</code>, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor <code>shelveloc</code> takes on three possible values: <em>Bad</em>, <em>Medium</em>, and <em>Good</em>. Given a qualitative variable such as <code>shelveloc</code>, <code>R</code> generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Sales</span> <span class="op">~</span> <span class="va">.</span> <span class="op">+</span> <span class="va">Income</span><span class="op">:</span><span class="va">Advertising</span> <span class="op">+</span> <span class="va">Price</span><span class="op">:</span><span class="va">Age</span>, </span>
<span>    data <span class="op">=</span> <span class="va">Carseats</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9208 -0.7503  0.0177  0.6754  3.3413 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***
CompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***
Income              0.0108940  0.0026044   4.183 3.57e-05 ***
Advertising         0.0702462  0.0226091   3.107 0.002030 ** 
Population          0.0001592  0.0003679   0.433 0.665330    
Price              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***
ShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***
ShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***
Age                -0.0579466  0.0159506  -3.633 0.000318 ***
Education          -0.0208525  0.0196131  -1.063 0.288361    
UrbanYes            0.1401597  0.1124019   1.247 0.213171    
USYes              -0.1575571  0.1489234  -1.058 0.290729    
Income:Advertising  0.0007510  0.0002784   2.698 0.007290 ** 
Price:Age           0.0001068  0.0001333   0.801 0.423812    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.011 on 386 degrees of freedom
Multiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 
F-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/contrasts.html">contrasts()</a></code> function returns the coding that <code>R</code> uses for the dummy variables.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/attach.html">attach</a></span><span class="op">(</span><span class="va">Carseats</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/contrasts.html">contrasts</a></span><span class="op">(</span><span class="va">ShelveLoc</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Good Medium
Bad       0      0
Good      1      0
Medium    0      1</code></pre>
</div>
</div>
<p>Use <code><a href="https://rdrr.io/r/stats/contrasts.html">?contrasts</a></code> to learn about other contrasts, and how to set them.</p>
<p><code>R</code> has created a <code>ShelveLocGood</code> dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a <code>ShelveLocMedium</code> dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for <code>ShelveLocGood</code> in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And <code>ShelveLocMedium</code> has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.</p>
</section><section id="writing-functions" class="level3" data-number="6.5.7"><h3 data-number="6.5.7" class="anchored" data-anchor-id="writing-functions">
<span class="header-section-number">6.5.7</span> Writing Functions</h3>
<p>As we have seen, <code>R</code> comes with many useful functions, and still more functions are available by way of <code>R</code> libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the <code>ISLR2</code> and <code>MASS</code> libraries, called <code>LoadLibraries()</code>. Before we have created the function, <code>R</code> returns an error if we try to call it.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LoadLibraries</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(expr, envir, enclos): object 'LoadLibraries' not found</code></pre>
</div>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">LoadLibraries</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in LoadLibraries(): could not find function "LoadLibraries"</code></pre>
</div>
</div>
<p>We now create the function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LoadLibraries</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span> <span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span></span>
<span> <span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="st">"The libraries have been loaded."</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now if we type in <code>LoadLibraries</code>, <code>R</code> will tell us what is in the function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LoadLibraries</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>function() {
 library(ISLR2)
 library(MASS)
 print("The libraries have been loaded.")
}</code></pre>
</div>
</div>
<p>If we call the function, the libraries are loaded in and the print statement is output.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">LoadLibraries</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "The libraries have been loaded."</code></pre>
</div>
</div>
</section></section><section id="exercises" class="level2" data-number="6.6"><h2 data-number="6.6" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">6.6</span> Exercises</h2>
<p>Prepare the following exercises of Chapter 3 in our course textbook <code>ISLR</code>:</p>
<ul>
<li>Exercise 1</li>
<li>Exercise 2</li>
<li>Exercise 3</li>
<li>Exercise 8</li>
<li>Exercise 9</li>
</ul>
<!-- {{< include Ch4_LinearRegression_Solutions.qmd >}} -->

</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./Ch4_LinearRegression.html" class="pagination-link" aria-label="Linear Regression">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch5_Classification.html" class="pagination-link" aria-label="Classification">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>