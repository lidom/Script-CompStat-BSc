<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Computer-Aided Statistical Analysis (B.Sc.) - 2&nbsp; Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch3_Classification.html" rel="next">
<link href="./Ch1_StatLearning.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer-Aided Statistical Analysis (B.Sc.)</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Organization of the Course</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_StatLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_LinearRegression.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_Classification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_ResamplingMethods.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link active" data-scroll-target="#simple-linear-regression"><span class="toc-section-number">2.1</span>  Simple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#estimating-the-regression-coefficients" id="toc-estimating-the-regression-coefficients" class="nav-link" data-scroll-target="#estimating-the-regression-coefficients"><span class="toc-section-number">2.1.1</span>  Estimating the Regression Coefficients</a></li>
  <li><a href="#assessing-the-accuracy-of-the-coefficient-estimates" id="toc-assessing-the-accuracy-of-the-coefficient-estimates" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-coefficient-estimates"><span class="toc-section-number">2.1.2</span>  Assessing the Accuracy of the Coefficient Estimates</a></li>
  <li><a href="#assessing-the-accuracy-of-the-model" id="toc-assessing-the-accuracy-of-the-model" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-model"><span class="toc-section-number">2.1.3</span>  Assessing the Accuracy of the Model</a></li>
  </ul></li>
  <li><a href="#sec-MultLinReg" id="toc-sec-MultLinReg" class="nav-link" data-scroll-target="#sec-MultLinReg"><span class="toc-section-number">2.2</span>  Multiple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#estimating-the-regression-coefficients-1" id="toc-estimating-the-regression-coefficients-1" class="nav-link" data-scroll-target="#estimating-the-regression-coefficients-1"><span class="toc-section-number">2.2.1</span>  Estimating the Regression Coefficients</a></li>
  <li><a href="#inference-on-beta_1dotsbeta_p" id="toc-inference-on-beta_1dotsbeta_p" class="nav-link" data-scroll-target="#inference-on-beta_1dotsbeta_p"><span class="toc-section-number">2.2.2</span>  Inference on <span class="math inline">\(\beta_1,\dots,\beta_p\)</span></a></li>
  </ul></li>
  <li><a href="#other-considerations-in-the-regression-model" id="toc-other-considerations-in-the-regression-model" class="nav-link" data-scroll-target="#other-considerations-in-the-regression-model"><span class="toc-section-number">2.3</span>  Other Considerations in the Regression Model</a>
  <ul class="collapse">
  <li><a href="#qualitative-predictors" id="toc-qualitative-predictors" class="nav-link" data-scroll-target="#qualitative-predictors"><span class="toc-section-number">2.3.1</span>  Qualitative Predictors</a></li>
  <li><a href="#extensions-of-the-linear-model" id="toc-extensions-of-the-linear-model" class="nav-link" data-scroll-target="#extensions-of-the-linear-model"><span class="toc-section-number">2.3.2</span>  Extensions of the Linear Model</a></li>
  <li><a href="#potential-problems" id="toc-potential-problems" class="nav-link" data-scroll-target="#potential-problems"><span class="toc-section-number">2.3.3</span>  Potential Problems</a></li>
  </ul></li>
  <li><a href="#comparison-of-linear-regression-with-k-nearest-neighbors" id="toc-comparison-of-linear-regression-with-k-nearest-neighbors" class="nav-link" data-scroll-target="#comparison-of-linear-regression-with-k-nearest-neighbors"><span class="toc-section-number">2.4</span>  Comparison of Linear Regression with K-Nearest Neighbors</a></li>
  <li><a href="#r-lab-linear-regression" id="toc-r-lab-linear-regression" class="nav-link" data-scroll-target="#r-lab-linear-regression"><span class="toc-section-number">2.5</span>  <code>R</code>-Lab: Linear Regression</a>
  <ul class="collapse">
  <li><a href="#libraries" id="toc-libraries" class="nav-link" data-scroll-target="#libraries"><span class="toc-section-number">2.5.1</span>  Libraries</a></li>
  <li><a href="#simple-linear-regression-1" id="toc-simple-linear-regression-1" class="nav-link" data-scroll-target="#simple-linear-regression-1"><span class="toc-section-number">2.5.2</span>  Simple Linear Regression</a></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="toc-section-number">2.5.3</span>  Multiple Linear Regression</a></li>
  <li><a href="#interaction-terms" id="toc-interaction-terms" class="nav-link" data-scroll-target="#interaction-terms"><span class="toc-section-number">2.5.4</span>  Interaction Terms</a></li>
  <li><a href="#non-linear-transformations-of-the-predictors" id="toc-non-linear-transformations-of-the-predictors" class="nav-link" data-scroll-target="#non-linear-transformations-of-the-predictors"><span class="toc-section-number">2.5.5</span>  Non-linear Transformations of the Predictors</a></li>
  <li><a href="#qualitative-predictors-1" id="toc-qualitative-predictors-1" class="nav-link" data-scroll-target="#qualitative-predictors-1"><span class="toc-section-number">2.5.6</span>  Qualitative Predictors</a></li>
  <li><a href="#writing-functions" id="toc-writing-functions" class="nav-link" data-scroll-target="#writing-functions"><span class="toc-section-number">2.5.7</span>  Writing Functions</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">2.6</span>  Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-linearRegCh" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<blockquote class="blockquote">
<p>Reading: Chapter 3 of our course textbook <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a></p>
</blockquote>
<section id="r-codes-for-this-chapter" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="r-codes-for-this-chapter"><code>R</code>-Codes for this Chapter</h4>
<ul>
<li><code>R</code>-codes for Coding Challenge Nr 1: <a href="https://www.dropbox.com/scl/fi/fkbf7z5qp53ib3m57thrp/Ch2_1_Rcodes.R?rlkey=9u8yeehf3zcpym4i3h36p0sd9&amp;dl=0">Ch2_1_Rcodes.R</a></li>
<li><code>R</code>-codes for Coding Challenge Nr 2: <a href="https://www.dropbox.com/scl/fi/5437hqcw13x9zgn0beues/Ch2_2_Rcodes.R?rlkey=td89o175f1efy1dflucmdjfgs&amp;dl=0">Ch2_2_Rcodes.R</a></li>
</ul>
</section>
<section id="simple-linear-regression" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="simple-linear-regression"><span class="header-section-number">2.1</span> Simple Linear Regression</h2>
<p>The linear regression model assumes a <em>linear relationship</em> between <span class="math inline">\(Y\)</span> and the predictor(s) <span class="math inline">\(X\)</span>.</p>
<p>The simple (only one predictor) linear regression model: <span class="math display">\[
Y\approx \beta_0 + \beta_1 X
\]</span></p>
For instance,
<center>
<code>sales</code> <span class="math inline">\(\approx \beta_0 + \beta_1\)</span> <code>TV</code>
</center>
<section id="estimating-the-regression-coefficients" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="estimating-the-regression-coefficients"><span class="header-section-number">2.1.1</span> Estimating the Regression Coefficients</h3>
<p>Let <span class="math display">\[
(X_{1},Y_1),\dots,(X_{n},Y_n)
\]</span> denote a training data random sample. I.e.<br>
<span class="math display">\[
(X_{i},Y_i)\overset{iid}{\sim}(X,Y),\quad\text{for all}\quad i=1,\dots,n.
\]</span> Moreover, let <span id="eq-simpleLinMod"><span class="math display">\[
Y_i=\beta_0 + \beta_1 X_i +\epsilon_i \quad\text{for all}\quad i=1,\dots,n,
\tag{2.1}\]</span></span> where</p>
<ul>
<li><span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_i\)</span> are independent from each other for all <span class="math inline">\(i=1,\dots,n\)</span><br>
</li>
<li><span class="math inline">\(E(\epsilon_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span></li>
<li><span class="math inline">\(Var(\epsilon_i)=\sigma^2&gt;0\)</span> is constant for all <span class="math inline">\(i=1,\dots,n\)</span></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The assumption <span class="math inline">\(f(X) = \beta_0 + \beta_1 X\)</span> may be a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple.</p>
</div>
</div>
<p>For a given observed realization of the training data random sample <span class="math display">\[
(x_1,y_1),\dots,(x_n,y_n)
\]</span> we choose <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> such that the <strong>R</strong>esidual <strong>S</strong>um of <strong>S</strong>quares criterion is minimized: <span class="math display">\[
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat{\beta}_0,\hat{\beta_1})
&amp; = e_1^2 + \dots + e_n^2\\[2ex]
&amp;=\sum_{i=1}^n\left(y_i - \left(\hat\beta_0 + \hat\beta_1x_i\right)\right)^2\\[2ex]
&amp;=\sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2
\end{align*}
\]</span> The minimizers are <span class="math display">\[
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
\]</span> and <span class="math display">\[
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
\]</span> where <span class="math inline">\(\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i\)</span> and <span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i\)</span>.</p>
<p><img src="images/Fig_3_1.png" class="img-fluid"></p>
<p><img src="images/Fig_3_2.png" class="img-fluid"></p>
</section>
<section id="assessing-the-accuracy-of-the-coefficient-estimates" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-coefficient-estimates"><span class="header-section-number">2.1.2</span> Assessing the Accuracy of the Coefficient Estimates</h3>
<p>If the assumption of <a href="#eq-simpleLinMod">Equation&nbsp;<span>2.1</span></a> is correct, i.e., if the data is actually generated according to the model <span class="math display">\[
Y_i=\beta_0 + \beta_1 X_i +\epsilon_i,
\]</span> then ordinary least squares estimators <span class="math display">\[
\hat\beta_0\quad\text{and}\quad\hat\beta_1
\]</span> are <strong>unbiased estimators</strong>, that is <span id="eq-SimpleRegBias"><span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta_0)&amp;=E(\hat\beta_0)-\beta_0=0\\
\operatorname{Bias}(\hat\beta_1)&amp;=E(\hat\beta_1)-\beta_1=0.
\end{align*}
\tag{2.2}\]</span></span> I.e., on average, the estimation results equal the true (unknown) parameters.</p>
<p><strong>Note:</strong> In <a href="#eq-SimpleRegBias">Equation&nbsp;<span>2.2</span></a>, we consider <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> as <strong>random variables</strong> from which we can compute mean values. The estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are indeed <strong>random variables</strong> since they depend on the random variables in the <strong>random sample</strong> <span class="math inline">\((X_i,Y_i)\overset{iid}{\sim}(X,Y)\)</span>:<br>
<span class="math display">\[
\hat\beta_1=\frac{\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^n(X_i-\bar{X})^2}
\]</span> and <span class="math display">\[
\hat\beta_0=\bar{Y} - \hat\beta_1\bar{X}.
\]</span></p>
<!-- For each observed realization of the random sample, we get one observed realization of the random variables 
$$
\hat\beta_0\quad\text{and}\quad \hat\beta_1.
$$ -->
<p>In an actual data analysis, we only have <em>one</em> realization of the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> computed from the given dataset (i.e.&nbsp;the observed realization of the random sample). Each single estimation result will have estimation errors, i.e., <span class="math display">\[
\hat\beta_0\neq \beta_0\quad\text{and}\quad\hat\beta_1\neq \beta_1.
\]</span></p>
<p>The following code generates artificial data to reproduce the plot in Figure 3.3 of our course textbook <code>ISLR</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ###############################</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="do">## A function to generate data </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="do">## similar to that shown in Fig 3.3</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="do">## ##############################</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="do">## A Function to simulate data</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>mySimpleRegrDataGenerator <span class="ot">&lt;-</span> <span class="cf">function</span>(){</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  n      <span class="ot">&lt;-</span> <span class="dv">50</span>                           <span class="co"># sample size</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  beta_0 <span class="ot">&lt;-</span> <span class="fl">0.1</span>                          <span class="co"># intercept parameter</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  beta_1 <span class="ot">&lt;-</span> <span class="dv">5</span>                            <span class="co"># slope parameter</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  X      <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="at">min =</span> <span class="sc">-</span><span class="dv">2</span>, <span class="at">max =</span> <span class="dv">2</span>)  <span class="co"># predictor</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  error  <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fl">8.5</span>) <span class="co"># error term</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  Y      <span class="ot">&lt;-</span> beta_0 <span class="sc">+</span> beta_1 <span class="sc">*</span> X <span class="sc">+</span> error  <span class="co"># outcome </span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  <span class="do">##</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">data.frame</span>(<span class="st">"Y"</span> <span class="ot">=</span> Y, <span class="st">"X"</span> <span class="ot">=</span> X))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate a first realization of the data</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>data_sim <span class="ot">&lt;-</span> <span class="fu">mySimpleRegrDataGenerator</span>()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data_sim)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Y          X
1 -18.4853427 -0.8496899
2  12.9872926  1.1532205
3  -0.4167901 -0.3640923
4  -1.9138159  1.5320696
5  19.5667725  1.7618691
6  -5.3639241 -1.8177740</code></pre>
</div>
</div>
<p>Using repeated samples form the data generating process defined in <code>mySimpleRegrDataGenerator()</code>, we can generate multiple estimation results <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> of the unknown simple linear regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and plot the corresponding empirical regression lines:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Estimation</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>lm_obj <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X, <span class="at">data =</span> data_sim)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Plotting the results</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="do">## True (usually unknown) parameter values</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> <span class="fl">0.1</span>  <span class="co"># intercept parameter</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> <span class="dv">5</span>    <span class="co"># slope parameter</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)) <span class="co"># Two plots side by side</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="do">## First Plot (fit for the first realization of the data)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> data_sim<span class="sc">$</span>X, <span class="at">y =</span> data_sim<span class="sc">$</span>Y, <span class="at">xlab =</span> <span class="st">"X"</span>, <span class="at">ylab =</span> <span class="st">"Y"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> beta_0, <span class="at">b =</span> beta_1, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm_obj, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Second Plot (fits for multiple data realizations)</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> data_sim<span class="sc">$</span>X, <span class="at">y =</span> data_sim<span class="sc">$</span>Y, <span class="at">xlab =</span> <span class="st">"X"</span>, <span class="at">ylab =</span> <span class="st">"Y"</span>, <span class="at">type =</span> <span class="st">"n"</span>) <span class="co"># type = "n": empty plot</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>){</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>  data_sim_new <span class="ot">&lt;-</span> <span class="fu">mySimpleRegrDataGenerator</span>()</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>  lm_obj_new   <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X, <span class="at">data=</span>data_sim_new)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(lm_obj_new, <span class="at">col =</span> <span class="st">"lightskyblue"</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Adding the first fit</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> beta_0, <span class="at">b =</span> beta_1, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm_obj, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<!-- *  Coding-Questions: Can you do this animated? https://gganimate.com/articles/gganimate.html -->
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Coding Challenge (Nr 1):
</div>
</div>
<div class="callout-body-container callout-body">
<p>Use the above <code>mySimpleRegrDataGenerator()</code> function to generate many, for instance <span class="math inline">\(B=10000\)</span>, realizations of the artificial data generating process, and based on these <span class="math inline">\(B\)</span> many realizations of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1:\)</span> <span class="math display">\[
\hat\beta_{0,1},\hat\beta_{0,2},\dots,\hat\beta_{0,B}
\]</span> <span class="math display">\[
\hat\beta_{1,1},\hat\beta_{1,2},\dots,\hat\beta_{1,B}
\]</span> The averages of these realizations can be used to approximate the true means of the estimators, i.e. <span class="math display">\[
\begin{align*}
E(\hat\beta_0)&amp;\approx \frac{1}{B}\sum_{b=1}^B\hat\beta_{0,b}\\[2ex]
E(\hat\beta_1)&amp;\approx \frac{1}{B}\sum_{b=1}^B\hat\beta_{1,b}
\end{align*}
\]</span> and thus also the biases <span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta_0)&amp;\approx \frac{1}{B}\sum_{b=}^B\hat\beta_{0,b} -\beta_0\\
\operatorname{Bias}(\hat\beta_1)&amp;\approx \frac{1}{B}\sum_{b=}^B\hat\beta_{0,b}-\beta_1.
\end{align*}
\]</span> The approximations become arbitrarily precise as <span class="math inline">\(B\to\infty\)</span> due to the Law of Large Numbers.</p>
<p><strong>Question:</strong> Are the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> unbiased for the artificial data generating process defined by our <code>mySimpleRegrDataGenerator()</code> function?</p>
</div>
</div>
<p>The magnitude of the estimation errors <span class="math display">\[
\hat\beta_0-\beta_0\neq 0 \quad\text{and}\quad\hat\beta_1-\beta_1\neq 0.
\]</span> is expressed in unites of <strong>Standard Errors</strong>: <span class="math display">\[
\operatorname{SE}(\hat\beta_0)=\sqrt{Var(\hat\beta_1)}=\sqrt{\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right]}
\]</span> and <span class="math display">\[
\operatorname{SE}(\hat\beta_1)=\sqrt{Var(\hat\beta_1)}=\sqrt{\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}},
\]</span> where <span class="math display">\[\begin{align*}
\sigma^2 &amp; =Var(\epsilon_i)\\
\Leftrightarrow\quad \sigma  &amp;  =SD(\epsilon_i) = \sqrt{Var(\epsilon_i)}
\end{align*}\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
<p>Typically, the variance and thus also the <strong>S</strong>tandard <strong>D</strong>eviation of the error term, <span class="math inline">\(\sigma = SD(\epsilon),\)</span> are unknown, but we can estimate <span class="math inline">\(\sigma\)</span> by the <strong>R</strong>esidual <strong>S</strong>tandard <strong>E</strong>rror: <span class="math display">\[\begin{align*}
\hat{\sigma}=\operatorname{RSE}
&amp;=\sqrt{\frac{1}{n-2}\operatorname{RSS}}\\[2ex]
&amp;=\sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y}_i)^2}\\[2ex]
&amp;=\sqrt{\frac{1}{n-2}\sum_{i=1}^n\left(y_i-\left(\hat{\beta}_0+\hat{\beta}_1x_i\right)\right)^2}
\end{align*}\]</span> where <span class="math inline">\(\operatorname{RSS}=\sum_{i=1}^n(y_i-\hat{y}_i)^2\)</span> are the residual sum of squares.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We subtract <span class="math inline">\(2\)</span> from the sample size <span class="math inline">\(n\)</span> since <span class="math inline">\(n-2\)</span> are the remaining degrees of freedom in the data after estimating two parameters <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>.</p>
</div>
</div>
<section id="confidence-intervals" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h4>
<p>Knowing the standard errors <span class="math display">\[
\operatorname{SE}(\hat\beta_0)\quad\text{and}\quad\operatorname{SE}(\hat\beta_1)
\]</span> allows us to construct approximate 95% <strong>Confidence Intervals</strong>: <span class="math display">\[
\begin{align*}
\operatorname{CI}_{\beta_0}
&amp;=\left[\hat{\beta}_0-2\operatorname{SE}(\hat\beta_0),\;
        \hat{\beta}_0+2\operatorname{SE}(\hat\beta_0)\right] = \hat\beta_0\pm 2\operatorname{SE}(\hat\beta_0)\\[2ex]
\operatorname{CI}_{\beta_1}
&amp;=\left[\hat{\beta}_1-2\operatorname{SE}(\hat\beta_1),\;
        \hat{\beta}_1+2\operatorname{SE}(\hat\beta_1)\right] = \hat\beta_1\pm 2\operatorname{SE}(\hat\beta_1)
\end{align*}
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Interpretation
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is approximately a 95% change (in resamplings from the data generating process) that the <strong>random confidence interval</strong> <span class="math inline">\(\operatorname{CI}_{\beta_1}\)</span> contains the true (fix) parameter value <span class="math inline">\(\beta_1\)</span>.</p>
<p>To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:</p>
<ul>
<li><a href="https://rpsychologist.com/d3/ci/">Interactive visualization for interpreting confidence intervals</a></li>
</ul>
</div>
</div>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Danger
</div>
</div>
<div class="callout-body-container callout-body">
<p>Only the above frequentist point of view can be nicely interpreted. Interpreting a given, observed confidence interval is hard and often done wrong.</p>
<p><img src="images/CI_meme.jpg" class="img-fluid"></p>
<p>A given, observed confidence interval (computed from the observed realization of the training data) either contains the true parameter value or not and usually we do not know it.</p>
</div>
</div>
</section>
<section id="confidence-intervals-for-statistical-hypothesis-testing" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="confidence-intervals-for-statistical-hypothesis-testing">Confidence Intervals for Statistical Hypothesis Testing</h4>
<p>We can use a <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> confidence interval to do statistical hypothesis testing at the significance level <span class="math inline">\(0&lt;\alpha&lt;1.\)</span> Typical significance levels:</p>
<ul>
<li><span class="math inline">\(\alpha=0.05\)</span></li>
<li><span class="math inline">\(\alpha=0.01\)</span></li>
</ul>
<p>Let us consider the following null-hypothesis <span class="math inline">\((H_0)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_1\)</span> equals the <strong>null-hypothetical value</strong> <span class="math inline">\(\beta^{(H_0)}_{1}\)</span> versus the two-sided alternative hypothesis <span class="math inline">\((H_1)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_1\)</span> does not equal the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1}:\)</span> <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta^{(H_0)}_{1}\\
H_1:&amp;\;\beta_1\neq \beta^{(H_0)}_{1}
\end{align*}
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Classic No-Effect Null-Hypothesis
</div>
</div>
<div class="callout-body-container callout-body">
<p>For the special case, where the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1}=0\)</span> we test the classic no-effect null-hypothesis: <span class="math display">\[
\begin{align*}
H_0:&amp;\;\text{There is no relationship between $Y$ and $X$; i.e. $\beta_1=0$}\\
H_1:&amp;\;\text{There is a relationship between $Y$ and $X$; i.e. $\beta_1\neq 0$}
\end{align*}
\]</span></p>
</div>
</div>
<p><strong>Testing-Procedure:</strong></p>
<ul>
<li><p>If the observed (obs) realization of the confidence interval, <span class="math inline">\(\operatorname{CI}_{\beta_1,obs},\)</span> <strong>contains</strong> the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1},\)</span> i.e. <span class="math display">\[
\begin{align*}
\beta^{(H_0)}_{1}&amp;\in\operatorname{CI}_{\beta_1,obs}\\
\Leftrightarrow\beta^{(H_0)}_{1}&amp;\in\left[\hat{\beta}_{1,obs}-2\operatorname{SE}_{obs}(\hat\beta_1),\;\hat{\beta}_{1,obs}+2\operatorname{SE}_{obs}(\hat\beta_1)\right],
\end{align*}
\]</span> then we <strong>cannot reject</strong> the null hypothesis that <span class="math inline">\(\beta_1=\beta^{(H_0)}_{1}.\)</span></p></li>
<li><p>If, however, the observed (obs) realization of the confidence interval, <span class="math inline">\(\operatorname{CI}_{\beta_1,obs},\)</span> does <strong>not contain</strong> the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1},\)</span> i.e. <span class="math display">\[
\begin{align*}
\beta^{(H_0)}_{1}&amp;\not\in\operatorname{CI}_{\beta_1,obs}\\
\Leftrightarrow\beta^{(H_0)}_{1}&amp;\not\in\left[\hat{\beta}_{1,obs}-2\operatorname{SE}_{obs}(\hat\beta_1),\;\hat{\beta}_{1,obs}+2\operatorname{SE}_{obs}(\hat\beta_1)\right],
\end{align*}
\]</span> then we <strong>can reject</strong> the null hypothesis and adopt the alternative that <span class="math inline">\(\beta_1\neq\beta^{(H_0)}_{1}.\)</span></p></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Probability of a Type I Error is Smaller than <span class="math inline">\(\alpha\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the null-hypothesis is true, i.e.&nbsp;if the true (unknown) <span class="math inline">\(\beta_1\)</span> equals the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1},\)</span> <span class="math display">\[
\beta_1 = \beta^{(H_0)}_{1}
\]</span> then the <strong>random confidence interval</strong> <span class="math inline">\(\operatorname{CI}_{\beta_1}\)</span> covers the deterministic null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1}\)</span> with probability at least <span class="math inline">\(1-\alpha\)</span> <span class="math display">\[
P(\beta^{(H_0)}_{1} \in \operatorname{CI}_{\beta_1}| H_0\text{ is true})\geq 1-\alpha.
\]</span> I.e., in <span class="math inline">\(100\)</span> resamples we expect to see at least <span class="math inline">\((1-\alpha)\cdot 100\)</span> coverage events.</p>
<p><strong>Probability of a Type I Error:</strong><br> Thus, the probability of falsely rejecting the null hypothesis even though the null hypothesis is true (i.e.&nbsp;doing a type I error) is smaller or equal to the chosen significance level <span class="math inline">\(\alpha,\)</span> since <span class="math display">\[
\begin{align*}
P(\beta^{(H_0)}_{1} \in \operatorname{CI}_{\beta_1}| H_0\text{ is true})&amp;\geq 1-\alpha\\
\Leftrightarrow\quad 1-P(\beta^{(H_0)}_{1} \not\in \operatorname{CI}_{\beta_1}| H_0\text{ is true})&amp;\geq 1-\alpha\\
\Leftrightarrow\quad \underbrace{P(\beta^{(H_0)}_{1} \not\in \operatorname{CI}_{\beta_1}| H_0\text{ is true})}_{\text{Probability of a Type I Error}}&amp;\leq \alpha
\end{align*}
\]</span> Note: A type I error (rejecting <span class="math inline">\(H_0\)</span> even though <span class="math inline">\(H_0\)</span> is true) is also called a false positive event. We want that false positives happen only (very) rarely and therefore choose a small significance level <span class="math inline">\(\alpha\)</span> such as <span class="math inline">\(\alpha=0.05\)</span> or <span class="math inline">\(\alpha=0.01.\)</span></p>
</div>
</div>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Coding Challenge (Nr 2):
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Use the above <code>mySimpleRegrDataGenerator()</code> function to generate one data set. Use this data set to compute the observed realization of the confidence interval, <span class="math inline">\(\operatorname{CI}_{\beta_1,obs}\)</span> for the (in principle unknown) parameter <span class="math inline">\(\beta_1.\)</span> Use this observed realization of the confidence interval to test <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta^{(H_0)}_{1}\\
\text{versus}\quad
H_1:&amp;\;\beta_1\neq \beta^{(H_0)}_{1}
\end{align*}
\]</span> with <span class="math inline">\(\beta^{(H_0)}_{1}=5;\)</span> i.e.&nbsp;with a correct null-hypothesis, since <span class="math inline">\(\beta_1=5\)</span> in <code>mySimpleRegrDataGenerator()</code>.
<ul>
<li>Can you reject <span class="math inline">\(H_0\)</span>? Is the test-decision in line with your expectation?</li>
</ul></li>
<li>Now, use the above <code>mySimpleRegrDataGenerator()</code> function to generate <span class="math inline">\(B=10,000\)</span> many data sets. Compute for each of these data sets the observed realizations of the confidence interval, <span class="math display">\[
\operatorname{CI}_{\beta_1,obs,b}\quad\text{for}\quad b=1,\dots,B.
\]</span><br>
Use each of the observed confidence interval realizations to test <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta^{(H_0)}_{1}\\
\text{versus}\quad
H_1:&amp;\;\beta_1\neq \beta^{(H_0)}_{1}
\end{align*}
\]</span> with <span class="math inline">\(\beta^{(H_0)}_{1}=5;\)</span> i.e.&nbsp;with a correct null-hypothesis, since <span class="math inline">\(\beta_1=5\)</span> in <code>mySimpleRegrDataGenerator()</code>.
<ul>
<li>What is the relative frequency of <span class="math inline">\(H_0\)</span>-rejections? Is this relative frequency in line with your expectation?</li>
</ul></li>
<li>Repeat 1. and 2., but with <span class="math display">\[
\beta^{(H_0)}_{1}=0;
\]</span> i.e.&nbsp;with a false null-hypothesis (since <span class="math inline">\(\beta_1=5\)</span> in <code>mySimpleRegrDataGenerator()</code>).</li>
</ol>
</div>
</div>
</section>
<section id="test-statistics-for-statistical-hypothesis-testing" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="test-statistics-for-statistical-hypothesis-testing">Test Statistics for Statistical Hypothesis Testing</h4>
<p>Standard errors can also be used to construct test statistics for statistical hypothesis testing. In the following, we look at the <span class="math inline">\(t\)</span>-test statistic.</p>
<p>Choose a significance level <span class="math inline">\(0&lt;\alpha&lt;1\)</span> such as, for instance,</p>
<ul>
<li><span class="math inline">\(\alpha=0.05\)</span></li>
<li><span class="math inline">\(\alpha=0.01\)</span></li>
</ul>
<p>Let us (again) consider the null-hypothesis <span class="math inline">\((H_0)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_1\)</span> equals the <strong>null-hypothetical value</strong> <span class="math inline">\(\beta^{(H_0)}_{1}\)</span> versus the two-sided alternative hypothesis <span class="math inline">\((H_1)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_1\)</span> does not equal the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{1}:\)</span> <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta^{(H_0)}_{1}\\
H_1:&amp;\;\beta_1\neq \beta^{(H_0)}_{1}
\end{align*}
\]</span></p>
<p>Under <span class="math inline">\(H_0,\)</span> i.e.&nbsp;if the (unknown) true parameter <span class="math inline">\(\beta_1\)</span> equals the null-hypothetical value, i.e.&nbsp;if <span class="math inline">\(\beta_1=\beta^{(H_0)}_{1},\)</span> the <strong>random</strong> <span class="math inline">\(t\)</span>-test statistic has a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\((n-2)\)</span> degrees of freedom, <span class="math display">\[
t=\frac{\hat\beta_1 - \beta^{(H_0)}_{1}}{\operatorname{SE}(\hat\beta_1)}\overset{H_0}{\sim}t_{(n-2)}.
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the multiple linear regression model <span class="math inline">\(Y_i=\beta_0+\sum_{j=1}^p\beta_jX_{ij}+\epsilon_i\)</span> (<a href="#sec-MultLinReg"><span>Section&nbsp;2.2</span></a>), the <span class="math inline">\(t\)</span>-test statistic for the <span class="math inline">\(j\)</span>th predictor has the following null-distribution: <span class="math display">\[
t=\frac{\hat\beta_j - \beta^{(H_0)}_{j}}{\operatorname{SE}(\hat\beta_j)}\overset{H_0}{\sim}t_{(n-p-1)}.
\]</span></p>
</div>
</div>
<p><strong><span class="math inline">\(p\)</span>-value:</strong><br> The <span class="math inline">\(p\)</span>-value is the probability of seeing a realization of the <strong>random</strong> <span class="math inline">\(t\)</span>-test statistic, <span class="math inline">\(t,\)</span> which is more extreme than the observed value of the test-statistic, <span class="math inline">\(t_{obs},\)</span> <span class="math display">\[\begin{align*}
p_{obs}
&amp;=P_{H_0}\left(|t|\geq|t_{obs}|\right)\\[2ex]
&amp;=2\cdot\min\{P_{H_0}\left(t\geq t_{obs} \right),\; P_{H_0}\left(t\leq t_{obs} \right)\}.
\end{align*}\]</span> where, <span class="math inline">\(P_{H_0}\)</span> means that the probability is computed under <span class="math inline">\(H_0\)</span>; i.e.&nbsp;for the scenario that <span class="math inline">\(H_0\)</span> is true <span class="math inline">\((\beta_1=\beta^{(H_0)}_{1}).\)</span></p>
<p><strong>Testing-Procedure:</strong> <!-- To do the statistical hypothesis test, we need to select a significance level $\alpha$ (e.g.,  $\alpha=0.05$ or $\alpha=0.01$).  --></p>
<ul>
<li><p>If the observed realization of the <span class="math inline">\(p\)</span>-value is larger than or equal to the significance level <span class="math display">\[
p_{obs}\geq \alpha,
\]</span> then we cannot reject the null hypothesis.</p></li>
<li><p>If, however, the observed realization of the <span class="math inline">\(p\)</span>-value is strictly smaller than the significance level <span class="math display">\[
p_{obs}&lt;\alpha,
\]</span> then we can reject the null hypothesis and adopt the alternative hypothesis.</p></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It can be shown that the above statistical hypothesis test based on confidence intervals is equivalent to the statistical hypothesis test based on the <span class="math inline">\(t\)</span>-test statistic.</p>
<p>In both cases, the probability of falsely rejecting the null hypothesis even though the null hypothesis is true, is smaller than or equal to the chosen significance level <span class="math inline">\(\alpha.\)</span></p>
</div>
</div>
<p><img src="images/Tab_3_1.png" class="img-fluid"></p>
</section>
</section>
<section id="assessing-the-accuracy-of-the-model" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-model"><span class="header-section-number">2.1.3</span> Assessing the Accuracy of the Model</h3>
<p>In tendency an accurate model has </p>
<ul>
<li><p>a low residual standard error <span class="math inline">\(\operatorname{RSE}\)</span> <span class="math display">\[
\operatorname{RSE}=\hat\sigma=\sqrt{\frac{\operatorname{RSS}}{n-2}}
\]</span></p></li>
<li><p>a high <span class="math inline">\(R^2\)</span></p></li>
</ul>
<p><span class="math display">\[
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
\]</span> where <span class="math inline">\(0\leq R^2\leq 1\)</span> and <span class="math display">\[
\begin{align*}
\operatorname{TSS}&amp;=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2\\
\operatorname{RSS}&amp;=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2\\
\hat{y}_i&amp;=\hat\beta_0+\hat\beta_1x_i
\end{align*}
\]</span></p>
<p>TSS: Total Sum of Squares</p>
<p>RSS: Residual Sum of Squares</p>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Danger
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Cautionary Note Nr 1:</strong> Do not forget that there is a <strong>irreducible error</strong> <span class="math inline">\(Var(\epsilon)=\sigma^2&gt;0\)</span>. Thus</p>
<ul>
<li>very low <span class="math inline">\(\operatorname{RSE}\)</span> values, <span class="math inline">\(\operatorname{RSE}\approx 0\)</span>, and</li>
<li>very high <span class="math inline">\(R^2\)</span> values, <span class="math inline">\(R^2\approx 1\)</span>,</li>
</ul>
<p>can be warning signals indicating overfitting. While overfitting typically does not happen with a simple linear regression model, it can happen with a multiple linear regression model.</p>
<p><strong>Cautionary Note Nr 2:</strong> The <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\operatorname{RSE}\)</span> are only based on <em>training</em> data. In <span class="quarto-unresolved-ref">?sec-SL</span>, we have seen that a proper assessment of the model accuracy needs to take into account <em>test</em> data.</p>
</div>
</div>
<!--   -->
<section id="r2-and-correlation-coefficient" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="r2-and-correlation-coefficient"><span class="math inline">\(R^2\)</span> and correlation coefficient</h4>
<p>In the case of the simple linear regression model, <span class="math inline">\(R^2\)</span> equals the squared sample correlation coefficient between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, <span class="math display">\[
R^2 = r_{yx}^2,
\]</span> where <span class="math display">\[
r_{yx}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}.
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the multiple linear regression model <span class="math inline">\(Y_i=\beta_0+\sum_{j=1}^p\beta_jX_{ij}+\epsilon_i\)</span> (<a href="#sec-MultLinReg"><span>Section&nbsp;2.2</span></a>), the <span class="math inline">\(R^2\)</span> equals the squared correlation between response and the fitted values: <span class="math display">\[
R^2=r^2_{y\hat{y}}
\]</span> with <span class="math display">\[
r_{y\hat{y}}=\frac{\sum_{i=1}^n(y_i-\bar{y})(\hat{y}_i-\bar{\hat{y}})}{\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}\sqrt{\sum_{i=1}^n(\hat{y}_i-\bar{\hat{y}})^2}}.
\]</span></p>
</div>
</div>
</section>
</section>
</section>
<section id="sec-MultLinReg" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-MultLinReg"><span class="header-section-number">2.2</span> Multiple Linear Regression</h2>
<p>The multiple linear regression model allows for more than only one predictor:<br>
<span class="math display">\[
Y\approx \beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p + \epsilon
\]</span></p>
For instance,
<center>
<code>sales</code> <span class="math inline">\(\approx \beta_0 + \beta_1\)</span> <code>TV</code> <span class="math inline">\(+\beta_2\)</span> <code>radio</code> <span class="math inline">\(+\beta_3\)</span> <code>newspaper</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<section id="estimating-the-regression-coefficients-1" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="estimating-the-regression-coefficients-1"><span class="header-section-number">2.2.1</span> Estimating the Regression Coefficients</h3>
<p>Let <span class="math display">\[
(X_{11},\dots,X_{1p},Y_1),\dots,(X_{n1},\dots,X_{np},Y_n)
\]</span> denote a training data random sample. I.e.<br>
<span class="math display">\[
(X_{i1},\dots,X_{ip},Y_i)\overset{iid}{\sim}(X_{1},\dots,X_{p},Y)
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
<p>Moreover, let <span id="eq-MultLinReg1"><span class="math display">\[
Y_i=\beta_0+\beta_1X_{i1}+\dots+\beta_p X_{ip}+\epsilon_i
\tag{2.3}\]</span></span> for all <span class="math inline">\(i=1,\dots,n,\)</span> where</p>
<ul>
<li><span class="math inline">\((p+1)&lt;n\)</span></li>
<li><span class="math inline">\(\epsilon_i\)</span> and the vector of predictors <span class="math inline">\((X_{i1},\dots,X_{ip})\)</span> are independent from each other for all <span class="math inline">\(i=1,\dots,n\)</span></li>
<li><span class="math inline">\(E(\epsilon_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span></li>
<li><span class="math inline">\(Var(\epsilon_i)=\sigma^2&gt;0\)</span> is constant for all <span class="math inline">\(i=1,\dots,n\)</span></li>
</ul>
<p>Typically, it is more convenient to write the multiple linear regression model in vector and matrix notation. Let <span class="math display">\[\begin{align*}
X_i&amp;=\left(\begin{matrix}X_{i0}\\X_{i1}\\ \vdots\\X_{ip}\end{matrix}\right)\quad\text{and}\quad
\beta=\left(\begin{matrix}\beta_{0}\\\beta_1\\ \vdots\\\beta_{p}\end{matrix}\right)
\end{align*}\]</span> with <span class="math inline">\(X_{i0}=1\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
<p>This allows us to write <a href="#eq-MultLinReg1">Equation&nbsp;<span>2.3</span></a> more compactly as <span class="math display">\[
Y_i=X_i'\beta+\epsilon_i \quad\text{for all}\quad i=1,\dots,n.
\]</span></p>
<p>Next, we can stack all components. Let <span class="math display">\[\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right)\quad\text{and}\quad
\epsilon=\left(\begin{matrix}\epsilon_1\\ \vdots\\\epsilon_n\end{matrix}\right)
\end{equation*}\]</span> denote the <span class="math inline">\((n\times 1)\)</span> vectors containing all response values <span class="math inline">\(Y_i\)</span> and all error terms <span class="math inline">\(\epsilon_i\)</span> of the random sample. Moreover, let <span class="math display">\[\begin{align*}
X
&amp;=\left(\begin{matrix}
X_{10}&amp;X_{11}&amp;\dots&amp;X_{1p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
X_{n0}&amp;X_{n1}&amp;\dots&amp;X_{np}\\
\end{matrix}\right)\\[2ex]
&amp;=\left(\begin{matrix}
\;\;1\;\;&amp;X_{11}&amp;\dots&amp;X_{1p}\\
\;\;\vdots\;\;&amp;\vdots&amp;\ddots&amp;\vdots\\
\;\;1\;\;&amp;X_{n1}&amp;\dots&amp;X_{np}\\
\end{matrix}\right)
\end{align*}\]</span> denote the <span class="math inline">\((n\times (p+1))\)</span>-dimensional matrix containing all predictor values of the random sample, where the first column is a column full of ones <span class="math inline">\((X_{10}=1,\dots,X_{n0}=0).\)</span></p>
<p>This allows us to write <a href="#eq-MultLinReg1">Equation&nbsp;<span>2.3</span></a> even more compactly as <span class="math display">\[
Y=X\beta+\epsilon.
\]</span></p>
<p>Using matrix algebra, it can be shown that the ordinary least squares estimator of <span class="math inline">\(\beta=(\beta_0,\beta_1,\dots,\beta_p)'\)</span> is given by the following <span class="math inline">\(((p+1)\times 1)\)</span> vector: <span class="math display">\[
\left(\begin{matrix}\hat\beta_1\\ \vdots \\ \hat\beta_K\end{matrix}\right)=\hat{\beta}=(X'X)^{-1}X'Y.
\]</span></p>
<p>For a given observed realization of the training data random sample <span class="math display">\[
(x_{11},\dots,x_{1p},y_1),\dots,(x_{n1},\dots,x_{np},y_n),
\]</span> the estimator <span class="math inline">\(\hat\beta=(\beta_0,\beta_1,\dots,\beta_p)'\)</span> is selected by minimizing <span class="math display">\[\begin{align*}
\operatorname{RSS}
&amp;=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2\\[2ex]
&amp;=\sum_{i=1}^n\left(y_i-\left(\hat\beta_0 + \hat\beta_1 x_{i1} \dots + \hat\beta_p x_{ip}\right)\right)^2.
\end{align*}\]</span></p>
<p><img src="images/Fig_3_4.png" class="img-fluid"></p>
<p>Since the estimator <span class="math display">\[
\hat\beta=\left(\begin{matrix}\hat\beta_1\\ \vdots \\ \hat\beta_K\end{matrix}\right)
\]</span> depends on the random sample, it is itself a <span class="math inline">\((p+1)\)</span>-dimensional <strong>random variable</strong>. For each realization of the training data random sample, we observe a realization of the estimator.</p>
<p>In an actual data analysis, however, we only have <em>one</em> realization of the estimators <span class="math inline">\(\hat\beta_0,\hat\beta_1,\dots,\hat\beta_p\)</span> computed from the given training dataset (i.e.&nbsp;the observed realization of the training data random sample).</p>
<section id="interpretation-of-multiple-linear-regressions-and-the-omitted-variable-bias" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="interpretation-of-multiple-linear-regressions-and-the-omitted-variable-bias">Interpretation of Multiple Linear Regressions and the Omitted Variable Bias</h4>
<p>Multiple linear regression is more than mere composition of single simple linear regression models. Take a look at the following two simple linear regression results:</p>
<p><img src="images/Tab_3_3.png" class="img-fluid"></p>
<p>Observations:</p>
<ul>
<li><p>In the first simple linear regression, we see a statistical significant effect of <code>radio</code> on <code>sales</code>; i.e.&nbsp;we can reject the null hypotheses <span class="math display">\[
H_0:\beta_{radio}=0
\]</span> and adopt the alternative hypotheses <span class="math display">\[
H_1:\beta_{radio}\neq 0.
\]</span></p></li>
<li><p>In the second simple linear regression, we see a statistical significant effect of <code>newspaper</code> on <code>sales</code>; i.e.&nbsp;we can reject the null hypotheses <span class="math display">\[
H_0:\beta_{newspaper}=0
\]</span> and adopt the alternative hypotheses <span class="math display">\[
H_1:\beta_{newspaper}\neq 0.
\]</span></p></li>
</ul>
<p>By contrast, when looking at the <strong>multiple linear regression</strong> when regressing <code>sales</code> onto</p>
<ul>
<li><code>TV</code>,</li>
<li><code>radio</code> and<br>
</li>
<li><code>newspaper</code>,</li>
</ul>
<p>then the effect of <code>newspaper</code> becomes statistically <strong>insignificant</strong>; see Table 3.4.</p>
<p><img src="images/Tab_3_4.png" class="img-fluid"></p>
<p><strong>Reason: Omitted Variable Bias</strong></p>
<p>The reason for this change from a statistically significant effect of <code>newspaper</code> in the simple linear regression, to an insignificant effect in the multiple linear regression is the so-called <strong>Omitted Variable Bias</strong>.</p>
<p>Explanation of the omitted variables bias:</p>
<ul>
<li><code>radio</code> has a <strong>true positive effect</strong> on <code>sales</code></li>
<li><code>newspaper</code> has actually <strong>no effect</strong> on <code>sales</code></li>
<li>But, <code>newspaper</code> is correlated with <code>radio</code> <span class="math inline">\(r_{\texttt{newspaper},\texttt{radio}}=0.3541\)</span>; see Table 3.5</li>
</ul>
<p><img src="images/Tab_3_5.png" class="img-fluid"></p>
<ul>
<li>Thus, when <strong>omitting</strong> <code>radio</code> from the regression model, <code>newspaper</code> becomes a surrogate for <code>radio</code> and we see a spurious effect.</li>
</ul>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Danger
</div>
</div>
<div class="callout-body-container callout-body">
<p>Interpreting statistically significant results as true effects (a change of <span class="math inline">\(X_j\)</span> by one unit causes on average a change in <span class="math inline">\(Y\)</span> by <span class="math inline">\(\beta_{j}\)</span>) is a delectate thing.</p>
<p>Even if the <span class="math inline">\(f(X)\)</span> is really so simple that we can write it as a simple or multiple linear regression model, we may miss to include all relevant predictor variables and thus statistically significant results may only be spurious effects due to omitted variables.</p>
</div>
</div>
<p><strong>Interpretation of the Coefficients in Table 3.4</strong></p>
<p>For fixed values of <code>TV</code> and <code>newspaper</code>, spending additionally 1000 USD for <code>radio</code>, increases on average <code>sales</code> by approximately 189 units.</p>
</section>
</section>
<section id="inference-on-beta_1dotsbeta_p" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="inference-on-beta_1dotsbeta_p"><span class="header-section-number">2.2.2</span> Inference on <span class="math inline">\(\beta_1,\dots,\beta_p\)</span></h3>
<p>The <span class="math inline">\(t\)</span>-test statistic (equivalently the confidence interval for <span class="math inline">\(\beta_j\)</span>) allows us to test a null-hypothesis about <strong>one</strong> parameter <span class="math inline">\(\beta_j.\)</span></p>
<p>To test whether there is a relationship between the response <span class="math inline">\(Y\)</span> and total <em>vector</em> predictors <span class="math inline">\((X_1,\dots,X_p)\)</span> we can use the <span class="math inline">\(F\)</span>-test statistic.</p>
<p>In this case, the <span class="math inline">\(F\)</span>-test tests the null-hypothesis <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta_2=\dots=\beta_p=0\\
\text{versus}\quad H_1:&amp;\;\text{at least one $\beta_j\neq 0$; $j=1,\dots,p$}
\end{align*}
\]</span></p>
<p><span class="math inline">\(F\)</span>-test statistic <span class="math display">\[
F=\frac{(\operatorname{TSS}-\operatorname{RSS})/p}{\operatorname{
  RSS}/(n-p-1)}\overset{H_0}{\sim} F_{p,n-p-1}
\]</span> Under <span class="math inline">\(H_0,\)</span> i.e.&nbsp;if <span class="math inline">\(H_0\)</span> is true, the <span class="math inline">\(F\)</span>-test statistic has a <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(p\)</span> numerator and <span class="math inline">\((n-p-1)\)</span> denominator degrees of freedom.</p>
<p>If <span class="math inline">\(H_0\)</span> is correct <span class="math display">\[
\begin{align*}
E((\operatorname{TSS}-\operatorname{RSS})/p)&amp;=\sigma^2\\[2ex]
E(\operatorname{RSS}/(n-p-1))&amp;=\sigma^2
\end{align*}
\]</span></p>
<p>Therefore:</p>
<ul>
<li>If <span class="math inline">\(H_0\)</span> is correct, we expect values of <span class="math inline">\(F\approx 1.\)</span></li>
<li>If <span class="math inline">\(H_1\)</span> is correct, we expect values of <span class="math inline">\(F\gg 1.\)</span></li>
</ul>
<p><span class="math inline">\(p\)</span>-value</p>
<p><span class="math display">\[\begin{align*}
p_{obs}
&amp;=P_{H_0}\left( F \geq F_{obs} \right),
\end{align*}\]</span> where <span class="math inline">\(F_{obs}\)</span> denotes the observed value of the <span class="math inline">\(F\)</span>-test statistic computed from the observed training data, and where <span class="math inline">\(F\)</span> is a random variable that has a <span class="math inline">\(F_{q,n-p-1}\)</span> distribution. <span class="math inline">\(P_{H_0}\)</span> means that the probability is computed for the scenario that <span class="math inline">\(H_0\)</span> is true.</p>
<p>To do the statistical hypothesis test, we need to select a significance level <span class="math inline">\(\alpha\)</span> (e.g.&nbsp;<span class="math inline">\(\alpha=0.01\)</span> or <span class="math inline">\(\alpha=0.05\)</span>).</p>
<ul>
<li><p>If the observed realization of the <span class="math inline">\(p\)</span>-value is larger than or equal to the significance level <span class="math display">\[
p_{obs}\geq \alpha,
\]</span> then we cannot reject the null hypothesis.</p></li>
<li><p>If, however, the observed realization of the <span class="math inline">\(p\)</span>-value is strictly smaller than the significance level <span class="math display">\[
p_{obs}&lt;\alpha,
\]</span> then we can reject the null hypothesis and adopt the alternative hypothesis.</p></li>
</ul>
<!-- Caution: Cannot be computed if $p>n$. (Chapter 6 on "high dimensional problems") -->
</section>
</section>
<section id="other-considerations-in-the-regression-model" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="other-considerations-in-the-regression-model"><span class="header-section-number">2.3</span> Other Considerations in the Regression Model</h2>
<section id="qualitative-predictors" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="qualitative-predictors"><span class="header-section-number">2.3.1</span> Qualitative Predictors</h3>
<p>Often some predictors are <em>qualitative</em> variables (also known as a <em>factor</em> variables). For instance, the <code>Credit</code> dataset contains the following qualitative predictors:</p>
<ul>
<li><code>own</code> (house ownership: yes/no)</li>
<li><code>student</code> (student status: yes/no)</li>
<li><code>status</code> (marital status: yes/no)</li>
<li><code>region</code> (regions: east, west or south)</li>
</ul>
<section id="predictors-with-only-two-levels" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="predictors-with-only-two-levels">Predictors with Only Two Levels</h4>
<p>If a <strong>qualitative predictor (factor)</strong> only has two levels (i.e.&nbsp;possible values), then incorporating it into a regression model is very simple: We simply create an indicator or <strong>dummy variable</strong> that takes on two possible numerical values; for instance, <span class="math display">\[
x_{i} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person owns a house}\\
  0&amp;\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
\]</span> Using this dummy variable as a predictor in the regression equation results in the following regression model: <span class="math display">\[\begin{align*}
y_{i}
&amp;=\beta_0 + \beta_1 x_i + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i &amp;\quad \text{if the $i$th person owns a house}\\
  \beta_0 + \epsilon_i           &amp;\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span>: The average credit card balance among those who do not own a house</li>
<li><span class="math inline">\(\beta_0+\beta_1\)</span>: The average credit card balance among those who do own a house</li>
<li><span class="math inline">\(\beta_1\)</span>: The average difference in credit card balance between owners and non-owners</li>
</ul>
<p><img src="images/Tab_3_7.png" class="img-fluid"></p>
<p>Alternatively, instead of a 0/1 coding scheme, we could create a dummy variable <span class="math display">\[
x_{i} = \left\{
  \begin{array}{ll}
  1 &amp;\quad \text{if the $i$th person owns a house}\\
-1 &amp;\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
\]</span> <span class="math display">\[\begin{align*}
y_{i}
&amp;=\beta_0 + \beta_1 x_i + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i&amp;\quad \text{if the $i$th person owns a house}\\
  \beta_0 - \beta_1 + \epsilon_i&amp;\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span>: The overall average credit card balance (ignoring the house ownership effect)</li>
<li><span class="math inline">\(\beta_1\)</span>: The average amount by which house owners and non-owners have credit card balances that are above and below the overall average, respectively.</li>
</ul>
</section>
<section id="qualitative-predictors-with-more-than-two-levels" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</h4>
<p>When a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the</p>
<p><code>region</code> <span class="math inline">\(\in\{\)</span><code>South</code>, <code>West</code>, <code>East</code><span class="math inline">\(\}\)</span></p>
<p>variable, we create <strong>two</strong> dummy variables. The first could be <span class="math display">\[
x_{i1} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person is from the South}\\
  0&amp;\quad \text{if the $i$th person is not from the South,}
  \end{array}\right.
\]</span> and the second could be <span class="math display">\[
x_{i2} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person is from the West}\\
  0&amp;\quad \text{if the $i$th person is not from the West.}
  \end{array}\right.
\]</span> Using both of these dummy variables results in the following regression model: order to obtain the model <span class="math display">\[\begin{align*}
y_{i}&amp;=\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1  + \epsilon_i&amp; \quad \text{if the $i$th person is from the South}\\
  \beta_0 + \beta_2  + \epsilon_i&amp; \quad \text{if the $i$th person is from the West}\\
  \beta_0            + \epsilon_i&amp; \quad \text{if the $i$th person is from the East.}\\
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span>: The average credit card balance for individuals from the East</li>
<li><span class="math inline">\(\beta_1\)</span>: The difference in the average balance between people from the South versus the East</li>
<li><span class="math inline">\(\beta_2\)</span>: The difference in the average balance between people from the West versus the East</li>
</ul>
<p><img src="images/Tab_3_8.png" class="img-fluid"></p>
<p>There are many different ways of coding qualitative variables besides the dummy variable approach taken here. All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular <strong>contrasts</strong>. (A detailed discussion of <em>contrasts</em> is beyond the scope of this lecture.)</p>
</section>
</section>
<section id="extensions-of-the-linear-model" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="extensions-of-the-linear-model"><span class="header-section-number">2.3.2</span> Extensions of the Linear Model</h3>
<section id="interaction-effects" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="interaction-effects">Interaction Effects</h4>
<p>Previously, we used the following model</p>
<center>
<code>sales</code> <span class="math inline">\(= \beta_0 + \beta_1\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\)</span> <code>radio</code> <span class="math inline">\(+ \beta_3\)</span> <code>newspaper</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<p>which states, for instance, that the average increase in <code>sales</code> associated with a one-unit increase in <code>TV</code> is <span class="math inline">\(\beta_1,\)</span> regardless of the amount spent on <code>radio</code>.</p>
<p>However, this simple model may be incorrect. Suppose that there is a synergy effect, such that spending money on <code>radio</code> advertising actually increases the effectiveness of <code>TV</code> advertising.</p>
<p>Figure 3.5 suggests that such an effect may be present in the advertising data:</p>
<ul>
<li>When levels of either <code>TV</code> or <code>radio</code> are low, then the true <code>sales</code> are lower than predicted by the linear model.</li>
<li>But when advertising is split between the two media, then the model tends to <strong>underestimate</strong> sales. <img src="images/Fig_3_5.png" class="img-fluid"></li>
</ul>
<p><strong>Solution: Interaction Effects:</strong></p>
<p>Consider the standard linear regression model with two variables, <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon.
\]</span> Here each predictor <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> has a given effect, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, on <span class="math inline">\(Y\)</span> and this effect does not depend on the value of the other predictor. <strong>(Additive Assumption)</strong></p>
<p>One way of extending this model is to include a third predictor, called an <strong>interaction term</strong>, which is constructed by computing the product of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2.\)</span> This results in the model <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 \overbrace{\color{red}X_1X_2}^{=X_3} + \epsilon.
\]</span> This is a powerful extension relaxing the additive assumption. Notice that the model can now be written as <span class="math display">\[
\begin{align*}
Y &amp;= \beta_0 + \underbrace{(\beta_1 + \beta_3 X_2)}_{=\tilde{\beta}_1(X_2)} X_1 + \beta_2 X_2 + \epsilon,
\end{align*}
\]</span> where the new slope parameter <span class="math inline">\(\tilde{\beta}_2(X_2)\)</span> is a linear function of <span class="math inline">\(X_2,\)</span> i.e. <span class="math display">\[
\tilde{\beta}_1(X_2)=\beta_1 + \beta_3 X_2.
\]</span></p>
<p>Thus, a change in the value of <span class="math inline">\(X_2\)</span> will change the association between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y.\)</span></p>
<p>A similar argument shows that a change in the value of <span class="math inline">\(X_1\)</span> changes the association between <span class="math inline">\(X_2\)</span> and <span class="math inline">\(Y.\)</span></p>
<p>Let us return to the <code>Advertising</code> example:<br> A linear model that predicts <code>sales</code> using</p>
<ul>
<li>the predictor <code>radio</code>,</li>
<li>the predictor <code>TV</code>, and</li>
<li>the interaction <code>radio</code><span class="math inline">\(\times\)</span><code>radio</code></li>
</ul>
<p>takes the form</p>
<center>
<code>sales</code> <span class="math inline">\(= \beta_0 + \beta_1\times\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\times\)</span> <code>radio</code> <span class="math inline">\(+ \beta_3\times(\)</span> <code>radio</code><span class="math inline">\(\times\)</span> <code>TV</code><span class="math inline">\()+\epsilon\)</span><br>
</center>
<p><br> which can be rewritten as</p>
<center>
<code>sales</code> <span class="math inline">\(=\beta_0 + (\beta_1+ \beta_3\times\)</span> <code>radio</code> <span class="math inline">\()\times\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\times\)</span> <code>radio</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<p><br></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li><span class="math inline">\(\beta_3\)</span> denotes the increase in the effectiveness of TV advertising associated with a one-unit increase in radio advertising.</li>
</ul>
<p><img src="images/Tab_3_9.png" class="img-fluid"></p>
<p><strong>Interpretation of Table 3.9:</strong></p>
<ul>
<li>Both separate main effects, <code>TV</code> and <code>radio</code>, are statistically significant (<span class="math inline">\(p\)</span>-values smaller than 0.01).</li>
<li>Additionally, the <span class="math inline">\(p\)</span>-value for the interaction term, <code>TV</code><span class="math inline">\(\times\)</span><code>radio</code>, is extremely low, indicating that there is strong evidence for <span class="math inline">\(H_1: \beta_3\neq 0.\)</span> In other words, it is clear that the true relationship is not additive.</li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Hierarchical Principle of Interaction Terms
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we include an interaction in a model, we should also include the main effects, even if the <span class="math inline">\(p\)</span>-values associated with their coefficients are not significant.</p>
</div>
</div>
<p><strong>Interactions with Qualitative Variables:</strong></p>
<p>An interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation.</p>
<p>Consider the <code>Credit</code> data set and suppose that we wish to predict <code>balance</code> using the predictors:</p>
<ul>
<li><code>income</code> (quantitative) and</li>
<li><code>student</code> (qualitative) using a dummy variable with <span class="math display">\[
x_{i2}=\left\{
\begin{array}{ll}
1&amp;\text{if person $i$ is a student}\\
0&amp;\text{if not}\\
\end{array}
\right.
\]</span></li>
</ul>
<p>In the absence of an interaction term, the model takes the form <img src="images/Eq_3_34.png" class="img-fluid"></p>
<p>Thus, the regression lines for students and non-students have different intercepts, <span class="math inline">\(\beta_0+\beta_2\)</span> versus <span class="math inline">\(\beta_0\)</span>, <strong>but the same slope</strong> <span class="math inline">\(\beta_1\)</span>.</p>
<p>This represents a potentially serious limitation of the model, since a change in <code>income</code> may have a very different effect on the credit card <code>balance</code> of a student versus a non-student.</p>
<p>This limitation can be addressed by adding an interaction variable, created by multiplying <code>income</code> with the dummy variable for student. Our model now becomes <img src="images/Eq_3_35.png" class="img-fluid"></p>
<p>Now we have different intercepts for students and non-students but also different slopes for these groups. <img src="images/Fig_3_7.png" class="img-fluid"></p>
</section>
<section id="polynomial-regression-non-linear-relationships" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="polynomial-regression-non-linear-relationships">Polynomial Regression: Non-linear Relationships</h4>
<p>Polynomial regression allows to accommodate non-linear relationships between the predictors <span class="math inline">\(X\)</span> and the outcome <span class="math inline">\(Y.\)</span> <img src="images/Fig_3_8.png" class="img-fluid"></p>
<p>For example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that a model of the form</p>
<center>
<code>mpg</code> <span class="math inline">\(=\beta_0 + \beta_1\times\)</span> <code>horsepower</code> <span class="math inline">\(+ \beta_2\times(\)</span><code>horsepower</code><span class="math inline">\()^2+\epsilon\)</span>
</center>
<p><br></p>
<p>This regression model involves predicting <code>mpg</code> using a non-linear function of <code>horsepower</code>.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>But it is still a linear model!</strong> Its simply a multiple linear regression model <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
\]</span> with</p>
<ul>
<li><span class="math inline">\(X_1=\)</span><code>horsepower</code> and</li>
<li><span class="math inline">\(X_2 =(\)</span><code>horsepower</code><span class="math inline">\()^2\)</span></li>
</ul>
<p>as the predictor variables.</p>
</div>
</div>
<p>Since this is nothing but a multiple linear regression model, we can use standard linear regression software to estimate <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> in order to fit the (quadratic) non-linear regression function.</p>
<p><img src="images/Tab_3_10.png" class="img-fluid"></p>
</section>
</section>
<section id="potential-problems" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="potential-problems"><span class="header-section-number">2.3.3</span> Potential Problems</h3>
<p><strong>1. Non-linearity of the response-predictor relationships.</strong></p>
<p><strong>Diagnostic residual plots</strong> are most useful to detect possible non-linear response-predictor relationships.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ISLR2"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Auto) </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Gives the variable names in the Auto dataset</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># names(Auto)</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Simple linear regression</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>lmobj_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Quadratic regression </span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>lmobj_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower <span class="sc">+</span> <span class="fu">I</span>(horsepower<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> Auto)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Diagnostic Plot</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmobj_1, <span class="at">which =</span> <span class="dv">1</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmobj_2, <span class="at">which =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Plotting <strong>residuals versus fitted values</strong>, i.e.&nbsp;plotting the data pairs <span class="math display">\[
(\underbrace{y_i - \hat{y}_i}_{=e_i}, \hat{y}_i),\quad\text{for all}\quad i=1,\dots,n
\]</span> is a useful graphical tool for identifying non-linearity which works for the</p>
<ul>
<li><strong>simple linear regression model</strong> with <span class="math inline">\(\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_{i1}\)</span> and the</li>
<li><strong>multiple linear regression model</strong> with <span class="math inline">\(\hat{y}_i=\hat{\beta}_0+\sum_{j=1}^p\hat{\beta}_jx_{ij}\)</span></li>
</ul>
<p>If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as <span class="math display">\[
\log(X),\; \sqrt{X},\; \text{or}\; X^2
\]</span> in the regression model.</p>
<!-- In the later chapters, we will discuss other more advanced non-linear approaches for addressing this issue. -->
<p><strong>2. Correlation of Error Terms</strong></p>
<p>An important assumption of the linear regression model is that the error terms, <span class="math display">\[
\epsilon_1, \epsilon_2, \dots , \epsilon_n,
\]</span> are independent and thus uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that <span class="math inline">\(\epsilon_i\)</span> is positive provides little or no information about the sign of <span class="math inline">\(\epsilon_{i+1}.\)</span></p>
<p>Auto-correlations among the error terms typically occur in time series data. Figure 3.10 shows time-series of residuals with</p>
<ul>
<li>no auto-correlation (<span class="math inline">\(\rho=0\)</span>)</li>
<li>intermediate auto-correlation (<span class="math inline">\(\rho=0.5\)</span>)</li>
<li>strong auto-correlation (<span class="math inline">\(\rho=0.9\)</span>)</li>
</ul>
<p><img src="images/Fig_3_10.png" class="img-fluid"></p>
<p><strong>3. Non-Constant Variance of Error Terms (Heteroskedasticity)</strong></p>
<p>Another important assumption of the linear regression model is that the error terms have a constant variance, <span class="math display">\[
Var(\epsilon_i) = \sigma^2,\quad\text{for all}\quad i=1,\dots,n.
\]</span></p>
<p>One can identify <strong>non-constant variances (heteroskedasticity)</strong> in the errors, using diagnostic residual plots.</p>
<p>Often one observes that the magnitude of the scattering of the residuals tends to increase with the fitted values. When faced with this problem, one possible solution is to transform the response <span class="math inline">\(Y\)</span> using a concave function such as <span class="math display">\[
\log(Y)\;\text{ or }\; \sqrt{Y}.
\]</span> Such a transformation results in a greater amount of shrinkage of the larger responses.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Quadratic regression </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>lmobj_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower <span class="sc">+</span> <span class="fu">I</span>(horsepower<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> Auto)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Quadratic regression with transformed response log(Y)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>lmobj_3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">I</span>(<span class="fu">log</span>(mpg)) <span class="sc">~</span> horsepower <span class="sc">+</span> <span class="fu">I</span>(horsepower<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> Auto)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Diagnostic Plot</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmobj_2, <span class="at">which =</span> <span class="dv">1</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmobj_3, <span class="at">which =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Danger
</div>
</div>
<div class="callout-body-container callout-body">
<p>The standard formulas for</p>
<ul>
<li>standard errors,</li>
<li>confidence intervals, and</li>
<li>hypothesis tests</li>
</ul>
<p>in this chapter are based on the assumption of</p>
<ul>
<li>uncorrelated error terms <span class="math inline">\(Cor(\epsilon_i,\epsilon_j)=0\)</span> for all <span class="math inline">\(i\neq j\)</span><br>
</li>
<li>with equal variances <span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></li>
</ul>
<p>If in fact there is correlation and/or heteroskedasticity among the error terms, then the estimated standard errors will be wrong leading to invalid inferences.</p>
<p>Thus, if the error terms are auto-correlated and/or heteroskedastic, we need to take this into account by using so-called auto-correlation and/or heteroskedasticity robust standard errors.</p>
<p>The <code>R</code> package <a href="https://cran.r-project.org/web/packages/sandwich/index.html">sandwich</a> contains such robust standard error estimators.</p>
</div>
</div>
<p><strong>4. Outliers</strong></p>
<p>An outlier is a point <span class="math inline">\(i\)</span> for which <span class="math inline">\(y_i\)</span> is far from the value <span class="math inline">\(\hat{y}_i\)</span> predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p>
<p>Outliers typically have a strong effect on the <span class="math inline">\(R^2\)</span> value since they add a <strong>very large residual</strong> to its computation.</p>
<p><strong>Harmless Outlier:</strong> Figure 3.12 shows a clear outlier (observation 20) which, however, has a typical predictor value <span class="math inline">\(x_i\)</span>; i.e.&nbsp;the <span class="math inline">\(x_i\)</span>-value is right in the center of all predicor values. Such outliers have little effect on the regression fit. <img src="images/Fig_3_12.png" class="img-fluid"></p>
<p><strong>Harmful Outlier:</strong> Figure 3.13 shows again a clear outlier (observation 41) which, however, has a predictor value <span class="math inline">\(x_i\)</span> that is <strong>very atypical</strong>. Such outliers are said to have <strong>large leverage</strong> giving them power to affect the regression fit considerably. <img src="images/Fig_3_13.png" class="img-fluid"></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Critical outliers have both,</p>
<ul>
<li><strong>large residuals</strong></li>
</ul>
<p><em>and</em></p>
<ul>
<li><strong>large leverage</strong>.</li>
</ul>
</div>
</div>
<p><strong>5. High Leverage Points</strong></p>
<p>In order to quantify an observations leverage, we compute the <strong>leverage statistic</strong> <span class="math inline">\(h_i\)</span> for each observation <span class="math inline">\(i=1,\dots,n.\)</span> A large value of this statistic indicates an observation with high leverage.</p>
<p>In the case of the <strong>simple linear regression model</strong> <span class="math display">\[
h_i = \frac{1}{n} + \frac{(x_i-\bar{x})^2}{\sum_{i'=1}^n(x_{i'}-\bar{x})^2}.
\]</span></p>
<p>In the case of the <strong>multiple linear regression model</strong>, <span class="math inline">\(h_i\)</span> is the <span class="math inline">\(i\)</span>th diagonal value of the <span class="math inline">\((n\times n)\)</span>-dimensional hat-matrix <span class="math display">\[
H=X(X'X)^{-1}X'.
\]</span></p>
<ul>
<li>The leverage statistic <span class="math inline">\(h_i\)</span> is always between <span class="math inline">\(1/n\)</span> and <span class="math inline">\(1\)</span></li>
<li>The average leverage for all the observations is equal to <span class="math display">\[
\bar{h}=\frac{1}{n}\sum_{i=1}^n h_i=(p + 1)/n.
\]</span></li>
<li>If a given observation has a leverage statistic <span class="math inline">\(h_i\)</span> that greatly exceeds the average leverage value, <span class="math inline">\((p+1)/n,\)</span> then we may suspect that the corresponding point has high leverage.</li>
</ul>
<p><strong>6. Collinearity</strong></p>
<p>Collinearity refers to the situation in which two or more predictor variables are closely related to one another.</p>
<p>In the following example, the variables <code>Age</code> and <code>Limit</code> are essentially unrelated, but the variables <code>Rating</code> and <code>Limit</code> are closely related to one another.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ISLR2"</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Credit) <span class="co"># names(Credit)</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">y =</span> Credit<span class="sc">$</span>Age,    <span class="at">x =</span> Credit<span class="sc">$</span>Limit, <span class="at">main =</span> <span class="st">"No Collinearity"</span>, <span class="at">ylab =</span> <span class="st">"Age"</span>, <span class="at">xlab =</span> <span class="st">"Limit"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">y =</span> Credit<span class="sc">$</span>Rating, <span class="at">x =</span> Credit<span class="sc">$</span>Limit, <span class="at">main =</span> <span class="st">"Strong Collinearity"</span>, <span class="at">ylab =</span> <span class="st">"Rating"</span>, <span class="at">xlab =</span> <span class="st">"Limit"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The left panel of Figure 3.15 shows that, in the case of unrelated predictors (<code>Age</code> and <code>Limit</code>), the least squares problem has a minimum <span class="math inline">\((\hat\beta_{Age},\hat\beta_{Limit})\)</span> that is well identified since the minimum is well defined.</p>
<p>The right panel of Figure 3.15 shows that, in the case of collinear predictors (<code>Rating</code> and <code>Limit</code>), the least squares problem has a minimum <span class="math inline">\((\hat\beta_{Rating},\hat\beta_{Limit})\)</span> that is not well identified: One can substitute values of <span class="math inline">\(\hat\beta_{Limit}'\)</span> for <span class="math inline">\(\hat\beta_{Rating}'\)</span> ending up in new pairs <span class="math inline">\((\hat\beta_{Rating}',\hat\beta_{Limit}')\)</span> with basically the same RSS-value than the original value than it is achieved by the minimizer <span class="math inline">\((\hat\beta_{Rating},\hat\beta_{Limit})\)</span>.</p>
<p><img src="images/Fig_3_15.png" class="img-fluid"></p>
<p>Table 3.11 demonstrates that this identification problem between the collinear predictors (<code>Rating</code> and <code>Limit</code>) causes a variance inflation in the variance (square of standard error) of the estimators <span class="math inline">\(\hat\beta_{Rating}\)</span> and <span class="math inline">\(\hat\beta_{Limit}.\)</span></p>
<ul>
<li>In Model 1: <span class="math inline">\(\hat\beta_{Limit} = 0.005^2=0.000025\)</span></li>
<li>In Model 2: <span class="math inline">\(\hat\beta_{Rating} = 0.064^2=0.004096\)</span></li>
</ul>
<p><img src="images/Tab_3_11.png" class="img-fluid"></p>
<p>We call this situation <strong>multicollinearity</strong>.</p>
<p>To detect multicollinearity issues, one can use the variance inflation factor (VIF) <span class="math display">\[
\operatorname{VIF}(\hat{\beta}_j)=\frac{1}{1-R^2_{X_j|X_-j}},
\]</span> where <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors.</p>
<ul>
<li>If <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is close to one, then multicollinearity is present, and <span class="math inline">\(\operatorname{VIF}(\hat{\beta}_j)\)</span> will be large.</li>
</ul>
<p>In the <code>Credit</code> data, one gets for the predictors <code>age</code>, <code>rating</code>, and <code>limit</code> the following VIF values:</p>
<ul>
<li>1.01 (<code>age</code>)</li>
<li>160.67 (<code>rating</code>)</li>
<li>160.59 (<code>limit</code>)</li>
</ul>
<p>Thus, as we suspected, there is considerable collinearity in the data!</p>
<p>Possible solutions:</p>
<ol type="1">
<li><p>Drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. <br> <strong>Caution:</strong> In econometrics, dropping control variables is generally not a good idea since control variables are there to rule out possible issues with omitted variables biases.</p></li>
<li><p>Combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.</p></li>
<li><p>Use a different estimation procedure like ridge regression.</p></li>
<li><p>Live with it. At least you know where the large stand errors are coming from.</p></li>
</ol>
</section>
</section>
<section id="comparison-of-linear-regression-with-k-nearest-neighbors" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="comparison-of-linear-regression-with-k-nearest-neighbors"><span class="header-section-number">2.4</span> Comparison of Linear Regression with K-Nearest Neighbors</h2>
<p>Linear regression is an example of a parametric approach because it assumes a linear model form for <span class="math inline">\(f(X).\)</span></p>
<p><strong>Advantages of parametric approaches:</strong></p>
<ul>
<li>Typically easy to fit</li>
<li>Simple interpretation</li>
<li>Simple inference</li>
</ul>
<p><strong>Disadvantages of parametric approaches:</strong></p>
<ul>
<li>The parametric model assumption can be far from true; i.e. <span class="math display">\[
f(X) \neq \beta_0+\beta_1X_1+\dots+\beta_pX_p
\]</span></li>
</ul>
<p>Alternative: <strong>Non-parametric methods</strong> such as <em>K-nearest neighbors regression</em> since non-parametric approaches do not explicitly assume a parametric form for <span class="math inline">\(f(X).\)</span></p>
<section id="k-nearest-neighbors-regression-knn-regression" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="k-nearest-neighbors-regression-knn-regression">K-nearest neighbors regression (KNN regression)</h4>
<p>Given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0,\)</span> KNN regression regression computes <span class="math inline">\(\hat{f}(x_0)\)</span> in two steps:</p>
<ol type="1">
<li>identify the <span class="math inline">\(K\)</span> training observations that are closest to <span class="math inline">\(x_0\)</span>, represented by the index set <span class="math inline">\(\mathcal{N}_0\subset\{1,2,\dots,n_{Train}\}.\)</span></li>
<li>estimate <span class="math inline">\(f(x_0)\)</span> using the average of all the training responses <span class="math inline">\(y_i\)</span> with <span class="math inline">\(i\in\mathcal{N}_0,\)</span> i.e.&nbsp; <span class="math display">\[
\hat{f}(x_0)=\frac{1}{K}\sum_{i\in\mathcal{N}_0}y_i.
\]</span></li>
</ol>
<p>The left panel of Figure 3.16 shows the estimation result for <span class="math inline">\(K=1\)</span> and the right panel for <span class="math inline">\(K=9.\)</span></p>
<p><img src="images/Fig_3_16.png" class="img-fluid"></p>
<p>In general, the optimal value for <span class="math inline">\(K\)</span> will depend on the <em>bias-variance tradeoff</em>, which we introduced in <span class="quarto-unresolved-ref">?sec-SL</span>:</p>
<ul>
<li>A small value for <span class="math inline">\(K\)</span> provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent, e.g., on just one observation if <span class="math inline">\(K=1\)</span>.</li>
<li>A large value of <span class="math inline">\(K\)</span> provides a less flexible fit. The prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in <span class="math inline">\(f(X).\)</span></li>
</ul>
<p>An optimal value of <span class="math inline">\(K\)</span> can be chosen using, e.g., cross-validation; see <span class="quarto-unresolved-ref">?sec-resamplingmethods</span>.</p>
<p>Generally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of <span class="math inline">\(f\)</span> and vice versa.</p>
<p>Figure 3.17 provides an example with data generated from a one-dimensional linear regression model:</p>
<ul>
<li>black solid lines: true <span class="math inline">\(f(x)\)</span></li>
<li>blue curves: KNN fits <span class="math inline">\(\hat{f}(x)\)</span> using <span class="math inline">\(K = 1\)</span> (left plot) and <span class="math inline">\(K = 9\)</span> (right plot).</li>
</ul>
<p>Observations:</p>
<ul>
<li>The KNN fit <span class="math inline">\(\hat{f}(x)\)</span> using <span class="math inline">\(K = 1\)</span> is far too wiggly</li>
<li>The KNN fit <span class="math inline">\(\hat{f}(x)\)</span> using <span class="math inline">\(K = 9\)</span> is much closer to the true <span class="math inline">\(f(X).\)</span></li>
</ul>
<p>However, since the true regression function is here linear, it is hard for a non-parametric approach to compete with simple linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. <img src="images/Fig_3_17.png" class="img-fluid"></p>
<p>The blue dashed line in the left-hand panel of Figure 3.18 represents the simple linear regression fit to the same data. It is almost perfect. The right-hand panel of Figure 3.18 reveals that linear regression outperforms KNN for this data. <img src="images/Fig_3_18.png" class="img-fluid"></p>
<p>Figure 3.19 displays a non-linear situations in which KNN performs much better than simple linear regression. <img src="images/Fig_3_19.png" class="img-fluid"></p>
</section>
<section id="curse-of-dimensionality" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="curse-of-dimensionality">Curse of Dimensionality</h4>
<p>Unfortunately, in higher dimensions, KNN often performs worse than simple/multiple linear regression, since non-parametric approaches suffer from the <strong>curse of dimensionality</strong>.</p>
<p>Figure 3.20 considers the same strongly non-linear situation as in the second row of Figure 3.19, except that we have added additional noise (i.e.&nbsp;redundant) predictors that are not associated with the response.</p>
<ul>
<li>When <span class="math inline">\(p = 1\)</span> or <span class="math inline">\(p = 2,\)</span> KNN outperforms linear regression.</li>
<li>But for <span class="math inline">\(p = 3\)</span> the results are mixed, and for <span class="math inline">\(p\geq 4\)</span> linear regression is superior to KNN. <img src="images/Fig_3_20.png" class="img-fluid"></li>
</ul>
<p>Observations:</p>
<ul>
<li>When <span class="math inline">\(p=1\)</span>, a sample size of <span class="math inline">\(n=50\)</span> can provide enough information to estimate <span class="math inline">\(f(X)\)</span> accurately using non-parametric methods since the <span class="math inline">\(K\)</span> nearest neighbors can actually be close to a given test observation <span class="math inline">\(x_0.\)</span></li>
<li>However, when spreading the <span class="math inline">\(n=50\)</span> data points over a large number of, for instance, <span class="math inline">\(p=20\)</span> dimensions, the <span class="math inline">\(K\)</span> nearest neighbors tend to become far away from <span class="math inline">\(x_0.\)</span></li>
</ul>
</section>
</section>
<section id="r-lab-linear-regression" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="r-lab-linear-regression"><span class="header-section-number">2.5</span> <code>R</code>-Lab: Linear Regression</h2>
<section id="libraries" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="libraries"><span class="header-section-number">2.5.1</span> Libraries</h3>
<p>The <code>library()</code> function is used to load <em>libraries</em>, or groups of functions and data sets that are not included in the base <code>R</code> distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries.</p>
<p>Here we load the <code>MASS</code> package, which is a very large collection of data sets and functions. We also load the <code>ISLR2</code> package, which includes the data sets associated with this book.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(MASS))</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(ISLR2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as <code>MASS</code>, come with <code>R</code> and do not need to be separately installed on your computer. However, other packages, such as <code>ISLR2</code>, must be downloaded the first time they are used. <!-- 
This can be done directly from within `R`. For example, on a Windows system,  select the `Install package` option under the `Packages` tab.  After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and `R` will automatically download the package. Alternatively,  --></p>
<p>This can be done, for instance, at the <code>R</code> command line via <code>install.packages("ISLR2")</code> function. This installation only needs to be done the first time you use a package. However, the <code>library()</code> function must be called within each <code>R</code> session.</p>
</section>
<section id="simple-linear-regression-1" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="simple-linear-regression-1"><span class="header-section-number">2.5.2</span> Simple Linear Regression</h3>
<p>The <code>ISLR2</code> library contains the <code>Boston</code> data set, which records <code>medv</code> (median house value) for <span class="math inline">\(506\)</span> census tracts in Boston. We will seek to predict <code>medv</code> using <span class="math inline">\(12\)</span> predictors such as <code>rmvar</code> (average number of rooms per house), <code>age</code> (average age of houses), and <code>lstat</code> (percent of households with low socioeconomic status).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Boston)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat
1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21
  medv
1 24.0
2 21.6
3 34.7
4 33.4
5 36.2
6 28.7</code></pre>
</div>
</div>
<p>To find out more about the data set, we can type <code>?Boston</code>.</p>
<p>We will start by using the <code>lm()</code> function to fit a simple linear regression model, with <code>medv</code> as the response and <code>lstat</code> as the predictor. The basic syntax is <code>lm(y ~ x, data)</code>, where <code>y</code> is the response, <code>x</code> is the predictor, and <code>data</code> is the data set in which these two variables are kept.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(predvars, data, env): object 'medv' not found</code></pre>
</div>
</div>
<p>The command causes an error because <code>R</code> does not know where to find the variables <code>medv</code> and <code>lstat</code>.</p>
<p>The next line tells <code>R</code> that the variables are in <code>Boston</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat, <span class="at">data =</span> Boston)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Alternatively, we can attach the <code>Boston</code> object:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Boston)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we type <code>lm.fit</code>, some basic information about the model is output. For more detailed information, we use <code>summary(lm.fit)</code>. This gives us <span class="math inline">\(p\)</span>-values and standard errors for the coefficients, as well as the <span class="math inline">\(R^2\)</span> statistic and <span class="math inline">\(F\)</span>-statistic for the model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>lm.fit</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat)

Coefficients:
(Intercept)        lstat  
      34.55        -0.95  </code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.168  -3.990  -1.318   2.034  24.500 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***
lstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.216 on 504 degrees of freedom
Multiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 
F-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can use the <code>names()</code> or the <code>str()</code> function in order to find out what other pieces of information are stored in <code>lm.fit</code>.</p>
<p>We can extract these quantities by namee.g.&nbsp;<code>lm.fit$coefficients</code>.</p>
<p>For some objects, there are also specific extractor functions like <code>coef()</code> to access them.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"        </code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)       lstat 
 34.5538409  -0.9500494 </code></pre>
</div>
</div>
<p>In order to obtain a confidence interval for the coefficient estimates, we can use the <code>confint()</code> command.</p>
<p>Type <code>confint(lm.fit)</code> at the command line to obtain the confidence intervals for the linear regression coefficients.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                2.5 %     97.5 %
(Intercept) 33.448457 35.6592247
lstat       -1.026148 -0.8739505</code></pre>
</div>
</div>
<p>The <code>predict()</code> function can be used to produce confidence intervals and prediction intervals for the prediction of <code>medv</code> for a given value of <code>lstat</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm.fit, <span class="fu">data.frame</span>(<span class="at">lstat =</span> (<span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>))), </span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">interval =</span> <span class="st">"confidence"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit      lwr      upr
1 29.80359 29.00741 30.59978
2 25.05335 24.47413 25.63256
3 20.30310 19.73159 20.87461</code></pre>
</div>
</div>
<p>For instance, the observed value of the 95% confidence interval associated with a <code>lstat</code> value of <span class="math inline">\(10,\)</span> i.e.&nbsp;associated with <span class="math display">\[
\beta_0 + \beta_1 \cdot 10
\]</span> is <span class="math display">\[
\operatorname{CI}_{\beta_0 + \beta_1 \cdot 10,obs}=[24.47, 25.63]
\]</span> with observed prediction value <span class="math display">\[
25.05335 = \hat\beta_0 + \hat\beta_1 \cdot 10.
\]</span></p>
<!-- predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), 
        interval = "prediction")
The 95\% prediction interval associated with a `lstat` value of 10 is $(12.828, 37.28)$. 

As expected, the confidence and prediction intervals are centered around the same point (a predicted value of $\hat{y}_i=25.05$ for `medv` when $x_{i2}=$`lstat` equals 10), but the latter are substantially wider.

::: {.callout-caution}
The `predict()` function with the option `interval = "prediction"` assumes **Gaussian** error terms; i.e. 
$$
\epsilon_i\sim\mathcal{N}(0,\sigma^2),\quad\text{for all}\quad i=1,\dots,n.
$$
If this distributional assumption is not fulfilled, you should not use the prediction interval.
::: -->
<p>We will now plot <code>medv</code> and <code>lstat</code> along with the least squares regression line using the <code>plot()</code> and <code>abline()</code> functions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat, medv)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>There is some evidence for non-linearity in the relationship between <code>lstat</code> and <code>medv</code>. We will explore this issue later in this lab.</p>
<p>The <code>abline()</code> function can be used to draw any line, not just the least squares regression line. To draw a line with intercept <code>a</code> and slope <code>b</code>, we type <code>abline(a, b)</code>. Below we experiment with some additional settings for plotting lines and points. The <code>lwd = 3</code> command causes the width of the regression line to be increased by a factor of 3; this works for the <code>plot()</code> and <code>lines()</code> functions also. We can also use the <code>pch</code> option to create different plotting symbols.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat, medv)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm.fit, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk10-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat, medv, <span class="at">col =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk10-2.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat, medv, <span class="at">pch =</span> <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk10-3.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstat, medv, <span class="at">pch =</span> <span class="st">"+"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk10-4.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, <span class="at">pch =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk10-5.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Next we examine some diagnostic plots. Four diagnostic plots are automatically produced by applying the <code>plot()</code> function directly to the output from <code>lm()</code>. In general, this command will produce one plot at a time, and hitting <em>Enter</em> will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the <code>par()</code> and <code>mfrow()</code> functions, which tell <code>R</code> to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, <code>par(mfrow = c(2, 2))</code> divides the plotting region into a <span class="math inline">\(2 \times 2\)</span> grid of panels.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk11-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Alternatively, we can compute the residuals from a linear regression fit using the <code>residuals()</code> function. The function <code>rstudent()</code> will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(lm.fit), <span class="fu">residuals</span>(lm.fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk12-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(lm.fit), <span class="fu">rstudent</span>(lm.fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk12-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>On the basis of the residual plots, there is some evidence of non-linearity.</p>
<p>Leverage statistics can be computed using the <code>hatvalues()</code> function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">hatvalues</span>(lm.fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk13-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.max</span>(<span class="fu">hatvalues</span>(lm.fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>375 
375 </code></pre>
</div>
</div>
<p>The <code>which.max()</code> function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">hatvalues</span>(lm.fit), <span class="at">decreasing =</span> <span class="cn">TRUE</span>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       375        415        374 
0.02686517 0.02495670 0.02097101 </code></pre>
</div>
</div>
<p>The <code>sort()</code> function can be used to sort and print values of a vector like <code>hatvalues(lm.fit)</code>.</p>
</section>
<section id="multiple-linear-regression" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="multiple-linear-regression"><span class="header-section-number">2.5.3</span> Multiple Linear Regression</h3>
<p>In order to fit a multiple linear regression model using least squares, we again use the <code>lm()</code> function. The syntax <code>lm(y ~ x1 + x2 + x3)</code> is used to fit a model with three predictors, <code>x1</code>, <code>x2</code>, and <code>x3</code>. The <code>summary()</code> function now outputs the regression coefficients for all the predictors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat <span class="sc">+</span> age, <span class="at">data =</span> Boston)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + age, data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.981  -3.978  -1.283   1.968  23.158 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***
lstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***
age          0.03454    0.01223   2.826  0.00491 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.173 on 503 degrees of freedom
Multiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 
F-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code>Boston</code> data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ ., data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.595  -2.730  -0.518   1.777  26.199 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
zn           4.642e-02  1.373e-02   3.382 0.000778 ***
indus        2.056e-02  6.150e-02   0.334 0.738288    
chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
rm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***
age          6.922e-04  1.321e-02   0.052 0.958229    
dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
black        9.312e-03  2.686e-03   3.467 0.000573 ***
lstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.745 on 492 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 
F-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can access the individual components of a summary object by name (type <code>?summary.lm</code> to see what is available). Hence <code>summary(lm.fit)$r.sq</code> gives us the <span class="math inline">\(R^2\)</span>, and <code>summary(lm.fit)$sigma</code> gives us the RSE. The <code>vif()</code> function, part of the <code>car</code> package, can be used to compute variance inflation factors. Most VIFs are low to moderate for this data. The <code>car</code> package is not part of the base <code>R</code> installation so it must be downloaded the first time you use it via the <code>install.packages()</code> function in <code>R</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(car)) <span class="co"># contains the vif() function</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">vif</span>(lm.fit)) <span class="co"># computes the VIF statistics and sorts them</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    chas    black     crim  ptratio       rm       zn    lstat      age 
1.073995 1.348521 1.792192 1.799084 1.933744 2.298758 2.941491 3.100826 
     dis    indus      nox      rad      tax 
3.955945 3.991596 4.393720 7.484496 9.008554 </code></pre>
</div>
</div>
<p>What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, <code>age</code> has a high <span class="math inline">\(p\)</span>-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except <code>age</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>lm.fit1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> . <span class="sc">-</span> age, <span class="at">data =</span> Boston)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ . - age, data = Boston)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.6054  -2.7313  -0.5188   1.7601  26.2243 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***
crim         -0.108006   0.032832  -3.290 0.001075 ** 
zn            0.046334   0.013613   3.404 0.000719 ***
indus         0.020562   0.061433   0.335 0.737989    
chas          2.689026   0.859598   3.128 0.001863 ** 
nox         -17.713540   3.679308  -4.814 1.97e-06 ***
rm            3.814394   0.408480   9.338  &lt; 2e-16 ***
dis          -1.478612   0.190611  -7.757 5.03e-14 ***
rad           0.305786   0.066089   4.627 4.75e-06 ***
tax          -0.012329   0.003755  -3.283 0.001099 ** 
ptratio      -0.952211   0.130294  -7.308 1.10e-12 ***
black         0.009321   0.002678   3.481 0.000544 ***
lstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.74 on 493 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 
F-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Alternatively, the <code>update()</code> function can be used.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>lm.fit1 <span class="ot">&lt;-</span> <span class="fu">update</span>(lm.fit, <span class="sc">~</span> . <span class="sc">-</span> age)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="interaction-terms" class="level3" data-number="2.5.4">
<h3 data-number="2.5.4" class="anchored" data-anchor-id="interaction-terms"><span class="header-section-number">2.5.4</span> Interaction Terms</h3>
<p>It is easy to include interaction terms in a linear model using the <code>lm()</code> function. The syntax <code>lstat:black</code> tells <code>R</code> to include an interaction term between <code>lstat</code> and <code>black</code>. The syntax <code>lstat * age</code> simultaneously includes <code>lstat</code>, <code>age</code>, and the interaction term <code>lstat</code><span class="math inline">\(\times\)</span><code>age</code> as predictors; it is a shorthand for <code>lstat + age + lstat:age</code>. %We can also pass in transformed versions of the predictors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(medv <span class="sc">~</span> lstat <span class="sc">*</span> age, <span class="at">data =</span> Boston))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat * age, data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.806  -4.045  -1.333   2.085  27.552 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***
lstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***
age         -0.0007209  0.0198792  -0.036   0.9711    
lstat:age    0.0041560  0.0018518   2.244   0.0252 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.149 on 502 degrees of freedom
Multiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 
F-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
<section id="non-linear-transformations-of-the-predictors" class="level3" data-number="2.5.5">
<h3 data-number="2.5.5" class="anchored" data-anchor-id="non-linear-transformations-of-the-predictors"><span class="header-section-number">2.5.5</span> Non-linear Transformations of the Predictors</h3>
<p>The <code>lm()</code> function can also accommodate non-linear transformations of the predictors. For instance, given a predictor <span class="math inline">\(X\)</span>, we can create a predictor <span class="math inline">\(X^2\)</span> using <code>I(X^2)</code>. The function <code>I()</code> is needed since the <code>^</code> has a special meaning in a formula object; wrapping as we do allows the standard usage in <code>R</code>, which is to raise <code>X</code> to the power <code>2</code>. We now perform a regression of <code>medv</code> onto <code>lstat</code> and <code>lstat^2</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>lm.fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat <span class="sc">+</span> <span class="fu">I</span>(lstat<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + I(lstat^2))

Residuals:
     Min       1Q   Median       3Q      Max 
-15.2834  -3.8313  -0.5295   2.3095  25.4148 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***
lstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***
I(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.524 on 503 degrees of freedom
Multiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 
F-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The near-zero <span class="math inline">\(p\)</span>-value associated with the quadratic term suggests that it leads to an improved model. We use the <code>anova()</code> function to further quantify the extent to which the quadratic fit is superior to the linear fit.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> lstat)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm.fit, lm.fit2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: medv ~ lstat
Model 2: medv ~ lstat + I(lstat^2)
  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
1    504 19472                                 
2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Here Model 1 represents the linear submodel containing only one predictor, <code>lstat</code>, while Model 2 corresponds to the larger quadratic model that has two predictors, <code>lstat</code> and <code>lstat^2</code>. The <code>anova()</code> function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior.</p>
<p>Here the <span class="math inline">\(F\)</span>-statistic is <span class="math inline">\(135\)</span> and the associated <span class="math inline">\(p\)</span>-value is virtually zero. This provides very clear evidence that the model containing the predictors <code>lstat</code> and <code>lstat^2</code> is far superior to the model that only contains the predictor <code>lstat</code>.</p>
<p>This is not surprising, since earlier we saw evidence for non-linearity in the relationship between <code>medv</code> and <code>lstat</code>.</p>
<p>If we type</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm.fit2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_LinearRegression_files/figure-html/chunk22-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>then we see that when the <code>lstat^2</code> term is included in the model, there is little discernible pattern in the residuals.</p>
<p>In order to create a <strong>cubic fit</strong>, we can include a predictor of the form <code>I(X^3)</code>. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the <code>poly()</code> function to create the polynomial within <code>lm()</code>. For example, the following command produces a fifth-order polynomial fit:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>lm.fit5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> <span class="fu">poly</span>(lstat, <span class="dv">5</span>))</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit5)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ poly(lstat, 5))

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5433  -3.1039  -0.7052   2.0844  27.1153 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***
poly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***
poly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***
poly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***
poly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***
poly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.215 on 500 degrees of freedom
Multiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 
F-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>This suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant <span class="math inline">\(p\)</span>-values in a regression fit.</p>
<p>By default, the <code>poly()</code> function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the <code>poly()</code> function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the <code>poly()</code> function, the argument <code>raw = TRUE</code> must be used.</p>
<p>Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(medv <span class="sc">~</span> <span class="fu">log</span>(rm), <span class="at">data =</span> Boston))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ log(rm), data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-19.487  -2.875  -0.104   2.837  39.816 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***
log(rm)       54.055      2.739   19.73   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.915 on 504 degrees of freedom
Multiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 
F-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
<section id="qualitative-predictors-1" class="level3" data-number="2.5.6">
<h3 data-number="2.5.6" class="anchored" data-anchor-id="qualitative-predictors-1"><span class="header-section-number">2.5.6</span> Qualitative Predictors</h3>
<p>We will now examine the <code>Carseats</code> data, which is part of the <code>ISLR2</code> library. We will attempt to predict <code>Sales</code> (child car seat sales) in <span class="math inline">\(400\)</span> locations based on a number of predictors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Carseats)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education
1  9.50       138     73          11        276   120       Bad  42        17
2 11.22       111     48          16        260    83      Good  65        10
3 10.06       113     35          10        269    80    Medium  59        12
4  7.40       117    100           4        466    97    Medium  55        14
5  4.15       141     64           3        340   128       Bad  38        13
6 10.81       124    113          13        501    72       Bad  78        16
  Urban  US
1   Yes Yes
2   Yes Yes
3   Yes Yes
4   Yes Yes
5   Yes  No
6    No Yes</code></pre>
</div>
</div>
<p>The <code>Carseats</code> data includes qualitative predictors such as <code>shelveloc</code>, an indicator of the quality of the shelving locationthat is, the space within a store in which the car seat is displayedat each location. The predictor <code>shelveloc</code> takes on three possible values: <em>Bad</em>, <em>Medium</em>, and <em>Good</em>. Given a qualitative variable such as <code>shelveloc</code>, <code>R</code> generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sales <span class="sc">~</span> . <span class="sc">+</span> Income<span class="sc">:</span>Advertising <span class="sc">+</span> Price<span class="sc">:</span>Age, </span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> Carseats)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9208 -0.7503  0.0177  0.6754  3.3413 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***
CompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***
Income              0.0108940  0.0026044   4.183 3.57e-05 ***
Advertising         0.0702462  0.0226091   3.107 0.002030 ** 
Population          0.0001592  0.0003679   0.433 0.665330    
Price              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***
ShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***
ShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***
Age                -0.0579466  0.0159506  -3.633 0.000318 ***
Education          -0.0208525  0.0196131  -1.063 0.288361    
UrbanYes            0.1401597  0.1124019   1.247 0.213171    
USYes              -0.1575571  0.1489234  -1.058 0.290729    
Income:Advertising  0.0007510  0.0002784   2.698 0.007290 ** 
Price:Age           0.0001068  0.0001333   0.801 0.423812    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.011 on 386 degrees of freedom
Multiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 
F-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code>contrasts()</code> function returns the coding that <code>R</code> uses for the dummy variables.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Carseats)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(ShelveLoc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Good Medium
Bad       0      0
Good      1      0
Medium    0      1</code></pre>
</div>
</div>
<p>Use <code>?contrasts</code> to learn about other contrasts, and how to set them.</p>
<p><code>R</code> has created a <code>ShelveLocGood</code> dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a <code>ShelveLocMedium</code> dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for <code>ShelveLocGood</code> in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And <code>ShelveLocMedium</code> has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.</p>
</section>
<section id="writing-functions" class="level3" data-number="2.5.7">
<h3 data-number="2.5.7" class="anchored" data-anchor-id="writing-functions"><span class="header-section-number">2.5.7</span> Writing Functions</h3>
<p>As we have seen, <code>R</code> comes with many useful functions, and still more functions are available by way of <code>R</code> libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the <code>ISLR2</code> and <code>MASS</code> libraries, called <code>LoadLibraries()</code>. Before we have created the function, <code>R</code> returns an error if we try to call it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>LoadLibraries</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(expr, envir, enclos): object 'LoadLibraries' not found</code></pre>
</div>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">LoadLibraries</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in LoadLibraries(): could not find function "LoadLibraries"</code></pre>
</div>
</div>
<p>We now create the function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>LoadLibraries <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">library</span>(ISLR2)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">library</span>(MASS)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">print</span>(<span class="st">"The libraries have been loaded."</span>)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now if we type in <code>LoadLibraries</code>, <code>R</code> will tell us what is in the function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>LoadLibraries</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>function() {
 library(ISLR2)
 library(MASS)
 print("The libraries have been loaded.")
}</code></pre>
</div>
</div>
<p>If we call the function, the libraries are loaded in and the print statement is output.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">LoadLibraries</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "The libraries have been loaded."</code></pre>
</div>
</div>
</section>
</section>
<section id="exercises" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="exercises"><span class="header-section-number">2.6</span> Exercises</h2>
<p>Prepare the following exercises of Chapter 3 in our course textbook <code>ISLR</code>:</p>
<ul>
<li>Exercise 1</li>
<li>Exercise 2</li>
<li>Exercise 3</li>
<li>Exercise 8</li>
<li>Exercise 9</li>
</ul>
<!-- {{< include Ch2_Solutions.qmd >}} -->


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch1_StatLearning.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch3_Classification.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>