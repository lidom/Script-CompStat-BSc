<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Parametric vs.&nbsp;Nonparametric Regression ‚Äì Computer-Aided Statistical Analysis (B.Sc.)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch5_Classification.html" rel="next">
<link href="./Ch_MatrixAlgebra.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch_NonParamRegression.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Parametric vs.&nbsp;Nonparametric Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer-Aided Statistical Analysis (B.Sc.)</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch_Intro2R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title"><code>R</code>-Lab: Introduction to <code>R</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch_LinearRegression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch_MatrixAlgebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch_NonParamRegression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Parametric vs.&nbsp;Nonparametric Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_Classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_ResamplingMethods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#k-nearest-neighbors-k-nn-regression" id="toc-k-nearest-neighbors-k-nn-regression" class="nav-link active" data-scroll-target="#k-nearest-neighbors-k-nn-regression"><span class="header-section-number">4.1</span> K-Nearest Neighbors (K-NN) Regression</a></li>
  <li><a href="#sec-mqfit" id="toc-sec-mqfit" class="nav-link" data-scroll-target="#sec-mqfit"><span class="header-section-number">4.2</span> Local Mean Squared (Prediction) Error (MSE)</a>
  <ul class="collapse">
  <li><a href="#the-local-bias-variance-trade-off" id="toc-the-local-bias-variance-trade-off" class="nav-link" data-scroll-target="#the-local-bias-variance-trade-off"><span class="header-section-number">4.2.1</span> The Local Bias-Variance Trade-Off</a></li>
  <li><a href="#local-mse-variance-and-bias-of-knn-regression" id="toc-local-mse-variance-and-bias-of-knn-regression" class="nav-link" data-scroll-target="#local-mse-variance-and-bias-of-knn-regression"><span class="header-section-number">4.2.2</span> Local MSE, Variance, and Bias of KNN-Regression</a></li>
  </ul></li>
  <li><a href="#in-class-coding-exercises" id="toc-in-class-coding-exercises" class="nav-link" data-scroll-target="#in-class-coding-exercises"><span class="header-section-number">4.3</span> In Class Coding Exercises</a></li>
  <li><a href="#under-construction" id="toc-under-construction" class="nav-link" data-scroll-target="#under-construction"><span class="header-section-number">4.4</span> Under Construction</a></li>
  <li><a href="#assessing-model-accuracy-of-nonparametric-regression" id="toc-assessing-model-accuracy-of-nonparametric-regression" class="nav-link" data-scroll-target="#assessing-model-accuracy-of-nonparametric-regression"><span class="header-section-number">4.5</span> Assessing Model Accuracy of Nonparametric Regression</a>
  <ul class="collapse">
  <li><a href="#the-global-bias-variance-trade-off" id="toc-the-global-bias-variance-trade-off" class="nav-link" data-scroll-target="#the-global-bias-variance-trade-off"><span class="header-section-number">4.5.1</span> The Global Bias-Variance Trade-Off</a></li>
  </ul></li>
  <li><a href="#under-revision" id="toc-under-revision" class="nav-link" data-scroll-target="#under-revision"><span class="header-section-number">4.6</span> Under Revision</a>
  <ul class="collapse">
  <li><a href="#population-version-of-the-mean-squared-prediction-error" id="toc-population-version-of-the-mean-squared-prediction-error" class="nav-link" data-scroll-target="#population-version-of-the-mean-squared-prediction-error"><span class="header-section-number">4.6.1</span> Population Version of the Mean Squared (Prediction) Error</a></li>
  <li><a href="#global-training-and-test-mse-in-nonparametric-smoothing-spline-regression" id="toc-global-training-and-test-mse-in-nonparametric-smoothing-spline-regression" class="nav-link" data-scroll-target="#global-training-and-test-mse-in-nonparametric-smoothing-spline-regression">Global Training and Test MSE in Nonparametric Smoothing Spline Regression</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Parametric vs.&nbsp;Nonparametric Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- LTeX: language=en-US -->
<p>The linear regression model as considered in <a href="Ch_LinearRegression.html" class="quarto-xref"><span>Chapter 2</span></a> is an example of a <strong>parametric</strong> regression model because it parametrizes the general regression model <span class="math display">\[
Y_i = f(X_i) + \epsilon_i
\]</span> using a linear model assumption (Assumption 1 in <a href="Ch_LinearRegression.html" class="quarto-xref"><span>Chapter 2</span></a>), such that <span class="math display">\[
f(X_i)=\beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}.
\]</span></p>
<p><strong>Advantages of parametric approaches:</strong></p>
<ul>
<li>Typically easy to fit</li>
<li>Simple interpretation</li>
<li>Simple inference</li>
</ul>
<p><strong>Disadvantages of parametric approaches:</strong></p>
<ul>
<li>The parametric model assumption can be far from true; i.e. <span class="math display">\[
f(X_i) \neq \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}
\]</span></li>
</ul>
<p><strong>Alternative:</strong> <strong>Non-parametric methods</strong> such as <strong>K-nearest neighbors regression</strong> since non-parametric approaches do not explicitly assume a parametric form for <span class="math inline">\(f(X).\)</span></p>
<section id="k-nearest-neighbors-k-nn-regression" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="k-nearest-neighbors-k-nn-regression"><span class="header-section-number">4.1</span> K-Nearest Neighbors (K-NN) Regression</h2>
<p>Let <span class="math display">\[
Y_i = f(X_i) + \epsilon_i
\]</span> denote the <strong>general regression model.</strong></p>
<p>In the following, we do not assume a certain parametric model form for <span class="math inline">\(f(x),\)</span> but only make the <strong>qualitative assumption</strong> that <span class="math inline">\(f\)</span> is a sufficiently <strong>smooth</strong> function, such that <span class="math display">\[
|f(x_1)-f(x_2)|\approx 0\quad\text{if}\quad d(x_1,x_2)\approx 0,
\]</span> where <span class="math inline">\(d(x_1,x_2)\)</span> measures the distance (e.g.&nbsp;<span class="math inline">\(d(x_1,x_2)=||x_1-x_2||\)</span>) between the points <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2.\)</span></p>
<p>Let <span class="math inline">\(x_0\in\mathbb{R}^p\)</span> denote a certain (multivariate) predictor value at which we want to estimate <span class="math display">\[
f(x_0).
\]</span></p>
<p><strong>KNN regression</strong> estimates <span class="math inline">\(f(x_0)\)</span> by <span class="math display">\[
\hat{f}_K(x_0),
\]</span> where <span class="math inline">\(\hat{f}_K(x_0)\)</span> is computed in two steps:</p>
<ol type="1">
<li>Compute the distances between <span class="math inline">\(x_0\)</span> and all training data predictor values <span class="math inline">\(X_1,\dots,X_{n_{Train}}\)</span> <span class="math display">\[
d(x_0,X_1),d(x_0,X_2)\dots,d(x_0,X_{n_{Train}}).
\]</span> Use these distances to identify the <span class="math inline">\(K\)</span> training data predictor values <span class="math inline">\(X_1,\dots,X_{n_{Train}}\)</span> that are closest to <span class="math inline">\(x_0\)</span> and collect their indices the index set <span class="math inline">\(\mathcal{N}_0,\)</span> where <span class="math display">\[
\begin{align*}
\mathcal{N}_0
&amp; =\{i\in\{1,2,\dots,n_{Train}\} \; |\; d(x_0,X_i)\text{ is one of the $K$ smallest distances}\}
\end{align*}
\]</span> such that
<ul>
<li><span class="math inline">\(\mathcal{N}_0\subset\{1,2,\dots,n_{Train}\}\)</span></li>
<li><span class="math inline">\(|\mathcal{N}_0|=K\)</span> (number of elements in <span class="math inline">\(\mathcal{N}_0\)</span>)</li>
</ul></li>
<li>Estimate <span class="math inline">\(f(x_0)\)</span> using the sample average of all the training responses <span class="math inline">\(Y_i\)</span> with <span class="math inline">\(i\in\mathcal{N}_0,\)</span> i.e.&nbsp; <span class="math display">\[
\hat{f}_K(x_0)=\frac{1}{K}\sum_{i\in\mathcal{N}_0}Y_i.
\]</span></li>
</ol>
<p>The above two steps are then repeated for all predictor values <span class="math inline">\(x_0\in\mathbb{R}^p\)</span> of interest.</p>
<p>The performance of the estimator <span class="math inline">\(\hat{f}_K(x_0)\)</span> depends on</p>
<ul>
<li>the choice of <span class="math inline">\(K\)</span> and</li>
<li>the choice of distance <span class="math inline">\(d\)</span></li>
</ul>
<p>For real valued predictors, <span class="math inline">\(X_i\in\mathbb{R}^p\)</span> a usual choice is the <strong>Euclidian distance</strong> <span class="math display">\[
d_E(x_0, X_i) = ||x_0 - X_i||^2 = \sum_{j=1}^p (x_{0j} - X_{ij})^2.
\]</span></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Use Standardized Predictors!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Typically, it is important to compute the distances with respect to the <strong>standardized</strong> (centering, and scaling to unit variance) predictor variables; i.e. <span class="math display">\[
d^*_E(x_0, X_i) = ||x^*_0 - X^*_i||^2 = \sum_{j=1}^p (x^*_{0j} - X^*_{ij})^2,
\]</span> where <span class="math display">\[
x^*_{0j} = \frac{x_{0j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
\]</span> and <span class="math display">\[
X^*_{ij} = \frac{X_{ij} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
\]</span> with <span class="math inline">\(\bar{X}_{j} = \frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}X_{ij}.\)</span></p>
<p>Otherwise, the distance values could be dominated by one of the <span class="math inline">\(p\)</span> predictors.</p>
<p>E.g. when one predictor is age (values between <span class="math inline">\(0\)</span> and <span class="math inline">\(99\)</span>) and another predictor is yearly income (values between <span class="math inline">\(0\)</span> and <span class="math inline">\(12,000,000\)</span>), then the differences in income will dominate the differences in age only because of the different scales. <!-- 
$$
\frac{X_{1j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}},\dots,\frac{X_{n_{Train}j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
$$  --></p>
</div>
</div>
<p>The problem is now, to find the optimal value for <span class="math inline">\(K.\)</span></p>
<p><strong>Idea:</strong> Choose <span class="math inline">\(K\)</span> by minimizing the mean squared (prediction) error.</p>
</section>
<section id="sec-mqfit" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-mqfit"><span class="header-section-number">4.2</span> Local Mean Squared (Prediction) Error (MSE)</h2>
<p>A fair and reliable assessment of the model accuracy requires <strong>testing data</strong>, i.e., data which comes from the same data generating process as the <strong>training data</strong>, but which was not used to compute (train) the estimator.</p>
<!-- In fact, a very flexible (e.g. non-parametric) estimation method will tend to overfit the training data such that $y_i\approx \hat{f}(x_i)$ for all $i=1,\dots,n$ resulting in a training MSE that is close to zero since $\hat{f}(x_i)$ fits also the errors $\epsilon_i.$ -->
<!-- **Example:** Suppose that we are interested in developing an algorithm to predict a stock‚Äôs price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don't really care how well our method predicts last week's stock price. We instead care about how well it will predict tomorrow's price
or next month's price.  -->
<!-- **Example:** Suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements.  -->
<!-- In general, however, we do not really care how well the method works on the training data. We are interested in the accuracy of the predictions that we obtain when we apply our method to **previously unseen test data**. 

Thus, we want to choose the method that gives the **lowest *test* MSE**, as opposed to the lowest *training* MSE.  -->
<section id="local-test-data-mse" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="local-test-data-mse"><strong>Local Test Data MSE</strong></h4>
<p>Let <span class="math inline">\(\hat{f}\)</span> be computed from the training data <span class="math inline">\(\{(X_1,Y_1),\dots,(X_n,Y_{n_{Train}})\}.\)</span> And let <span class="math display">\[
\{(x_{0},Y^{Test}_{1}),(x_{0},Y^{Test}_{2})\dots,(x_{0},Y^{Test}_{n_{Test}})\}
\]</span> denote a specific set of <span class="math inline">\(n_{Test}\)</span> <strong>test data points</strong> <span class="math inline">\(Y^{Test}_{1},\dots,Y^{Test}_{n_{Test}}\)</span> for a <strong>specific predictor value</strong> <span class="math inline">\(x_0.\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>This type of <span class="math inline">\(x_0\)</span>-specific test data is a realization of a <strong>conditional random sample</strong> given <span class="math inline">\(X=x_0,\)</span> <span class="math display">\[
(x_{0},Y^{Test}_{i})\overset{\text{iid}}{\sim}(X,Y)|X=x_0,\quad i=1,\dots,n_{Test}.
\]</span> I.e, the test data <span class="math inline">\((x_{0},Y^{Test}_{i}),\)</span> is generated using iid realization from <span class="math display">\[
Y^{Test}_{i} = f(x_0) +  \epsilon^{Test}_{i}, \quad i=1,\dots,n_{Test}.
\]</span> This test data random sample is independent of the training data random sample whose realization was used to compute <span class="math inline">\(\hat{f}.\)</span></p>
</div>
</div>
<p>Then, the <strong>point-wise (local) test MSE</strong> at <span class="math inline">\(X=x_0\)</span> is given by, <span class="math display">\[\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}(x_0)= \frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y^{Test}_{i} - \hat{f}(x_{0})\right)^2.
\end{align*}\]</span></p>
<p>This <strong>local test MSE</strong> is an <strong>estimator</strong> of the population version of the local Mean Squared (Prediction) Error of <span class="math inline">\(\hat{f}\)</span> <span class="math display">\[
\begin{align*}
\operatorname{MSE}(x_0)
&amp;= E\left[(Y - \hat{Y})^2|X=x_0\right]\\
&amp;= E\left[(Y - \hat{f}(X))^2|X=x_0\right].
\end{align*}
\]</span></p>
<p><strong>KNN-Regression</strong>:</p>
<p>For the KNN-regression estimator <span class="math inline">\(\hat{f}_K(x_{0})\)</span> we have that</p>
<p><span class="math display">\[\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}(x_0,K)= \frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y^{Test}_{i} - \hat{f}_K(x_{0})\right)^2.
\end{align*}\]</span></p>
<p>which estimates the population counterpart <span class="math display">\[\begin{align*}
\operatorname{MSE}(x_0,K)
&amp;= E\left[(Y - \hat{f}_K(X))^2|X=x_0\right].
\end{align*}\]</span></p>
<p>Generally, we‚Äôll find a minimum of <span class="math inline">\(\operatorname{MSE}(x_0,K)\)</span> with respect to the <strong>smoothing parameter</strong> <span class="math inline">\(K\)</span>.</p>
<p>The minimum of <span class="math inline">\(\operatorname{MSE}(x_0,K)\)</span> finds the optimal compromize between the squared bias of <span class="math inline">\(\hat{f}(x_0)\)</span> and the variance of <span class="math inline">\(\hat{f}(x_0).\)</span></p>
</section>
<section id="the-local-bias-variance-trade-off" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="the-local-bias-variance-trade-off"><span class="header-section-number">4.2.1</span> The Local Bias-Variance Trade-Off</h3>
<p>One can show that <span class="math display">\[\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}(x_0)= \frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y^{Test}_{i} - \hat{f}(x_{0})\right)^2.
\end{align*}\]</span> is an <strong>unbiased estimator</strong> of the true (unknown) Mean Squared (Prediction) Error of <span class="math inline">\(\hat{f}(x_0)\)</span> <span class="math display">\[
\begin{align*}
\operatorname{MSE}(x_0)
&amp; = E\left[(Y - \hat{Y})^2|X=x_0\right]\\
&amp;=\underbrace{E\left[\left(f(x_0)-\hat{f}(x_0)\right)^2\right]+\sigma^2}_{\text{Mean Squared Prediction Error of $\hat{f}(x_0)$}},
\end{align*}
\]</span> i.e., that <span id="eq-MSEUnbiased"><span class="math display">\[
E\left[\widehat{\operatorname{MSE}}_{test}(x_0)\right]=\underbrace{E\left[\left(f(x_0)-\hat{f}(x_0)\right)^2\right]+\sigma^2}_{=\operatorname{MSE}(x_0)}.
\qquad(4.1)\]</span></span></p>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of <a href="#eq-MSEUnbiased" class="quarto-xref">Equation&nbsp;<span>4.1</span></a>:
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\begin{align*}
E\left[\widehat{\operatorname{MSE}}_{Test}(x_0)\right]
&amp; =E\left[\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i}^{Test}- \hat{f}(x_0)\right)^2\right]\\[2ex]
&amp; \text{By the linearity of $E()$:}\\[2ex]
&amp; =\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}E\left[\left(Y_{i}^{Test}- \hat{f}(x_0)\right)^2\right]\\[2ex]
%&amp; =\frac{1}{n_{Test}}\,\sum_{i=1}^{n_{Test}}\,E\left[\left(Y_{1}^{Test}- \hat{f}(x_0)\right)^2\right]\\[2ex]
&amp; \text{Using that $Y_{i}^{Test}$ and $\hat{f}$ are iid across $i=1,\dots,n_{Test}$:}\\[2ex]
&amp; =\frac{1}{n_{Test}}\,E\left[\left(Y^{Test}- \hat{f}(x_0)\right)^2\right]\,\sum_{i=1}^{n_{Test}} 1\\[2ex]
&amp; =\frac{1}{n_{Test}}\,n_{Test}\,E\left[\left(Y^{Test}- \hat{f}(x_0)\right)^2\right]\\[2ex]
&amp; =E\left[\left(Y^{Test}- \hat{f}(x_0)\right)^2\right]\\[2ex]
&amp; \text{Using that}\;Y^{Test}=f(x_0)+\epsilon^{Test}\\[2ex]
&amp; =E\left[\left(f(x_0) + \epsilon^{Test} - \hat{f}(x_0)\right)^2\right]\\[2ex]
&amp; =E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2 +2\left(f(x_0)- \hat{f}(x_0)\right)\epsilon^{Test} + (\epsilon^{Test})^2 \right]\\[2ex]
&amp; =E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2\right]\\[2ex]
&amp;+ \underbrace{2E\left[\left(f(x_0)- \hat{f}(x_0)\right)\right]\overbrace{E\left[\epsilon^{Test}\right]}^{=0}}_{\text{using independence between training (in $\hat{f}$) and testing data}}\\[2ex]
&amp;+ \underbrace{E\left[(\epsilon^{Test})^2 \right]}_{=Var(\epsilon^{Test})}\\[2ex]
&amp; =\underbrace{E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2\right]}_{\text{Mean Squared Estimation Error of $\hat{f}(x_0)$}}+0+Var(\epsilon^{Test})\\[2ex]
&amp; =\underbrace{E\left[\left(f(x_0)- \hat{f}(x_0)\right)^2\right]}_{\text{reducable}}+\overbrace{\underbrace{Var(\epsilon^{Test})}_{\text{irreducable}}}^{=\sigma^2}\\[2ex]
\end{align*}
\]</span></p>
</div>
</div>
</div>
<!-- The **expected MSE** at $x_0,$ 
$$
E\left[\operatorname{MSE}_{test}(x_0)\right],
$$ 
refers to the average test MSE that we would obtain if we repeatedly estimated $f$ using training data set, and evaluated each at $x_0.$  
-->
<!-- 
::: {.callout-note}
A computed value of $\operatorname{MSE}_{test}(x_0)$ (as done in the coding challenge) is not able to consistently approximate $E\left[\operatorname{MSE}_{test}(x_0)\right].$

However, to get information about Bias and Variance of a method, we need to approximate $E\left[\operatorname{MSE}_{test}(x_0)\right].$ This will be (among others) the topic of  @sec-resamplingmethods.
::: 
-->
<p>The mean squared estimation error of <span class="math inline">\(\hat{f}(x_0)\)</span> can be <strong>decomposed</strong> into a <strong>variance</strong> component and a <strong>squared bias</strong> component, i.e. <span id="eq-MSEBiasVar"><span class="math display">\[
E\left[\left(f(x_0) - \hat{f}(x_0)\right)^2\right] =
Var\left(\hat{f}(x_0)\right) + \left[\operatorname{Bias}\left(\hat{f}(x_0)\right)\right]^2.
\qquad(4.2)\]</span></span></p>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of <a href="#eq-MSEBiasVar" class="quarto-xref">Equation&nbsp;<span>4.2</span></a>:
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><br></p>
<p><span class="math display">\[
\begin{align*}
&amp;   E\left[\left(f(x_0) - \hat{f}(x_0)\right)^2\right]\\[2ex]
&amp; = E\left[\left(\hat{f}(x_0) - f(x_0)\right)^2\right]\\[2ex]
&amp;\text{Adding $0=E[\hat{f}(x_0)] - E[\hat{f}(x_0)]$ yields}\\[2ex]
&amp; = E\left[\left(\left\{\hat{f}(x_0) - E[\hat{f}(x_0)]\right\} - \left\{ f(x_0)- E[\hat{f}(x_0)]\right\}\right)^2\right]\\[2ex]
&amp; = \overbrace{E\left[\left\{\hat{f}(x_0) - E[\hat{f}(x_0)]\right\}^2\right]}^{=Var\left(\hat{f}(x_0)\right)} + \overbrace{E\left[\left\{ f(x_0)- E[\hat{f}(x_0)]\right\}^2\right]}^{=\left\{E[\hat{f}(x_0)] - f(x_0) \right\}^2}\\[2ex]
&amp; \;\; -2\;\; \underbrace{E\left[\left\{\hat{f}(x_0) - E[\hat{f}(x_0)]\right\} \cdot
                 \left\{ f(x_0)- E[\hat{f}(x_0)]\right\}\right]}_{=E\left[\hat{f}(x_0)f(x_0)
                                                                         -\hat{f}(x_0)E\left[\hat{f}(x_0)\right]
                                                                         -     f(x_0) E\left[\hat{f}(x_0)\right]
                                                                         +\left(E\left[\hat{f}(x_0)\right]\right)^2\right]}\\[2ex]
&amp; = Var\left(\hat{f}(x_0)\right) + \Big\{\;\overbrace{E[\hat{f}(x_0)] - f(x_0)}^{=\operatorname{Bias}\left(\hat{f}(x_0)\right)}\; \Big\}^2\\[2ex]
&amp; \;\; -2\;\; \underbrace{E\left[\hat{f}(x_0)f(x_0) -\hat{f}(x_0)E\left[\hat{f}(x_0)\right]
                                                                         -     f(x_0) E\left[\hat{f}(x_0)\right]
                                                                         +\left(E\left[\hat{f}(x_0)\right]\right)^2\right]}_{= 0
                                                                         }\\[2ex]                  
% = E\left[\hat{f}(x_0)\right]f(x_0) - \left(E\left[\hat{f}(x_0)\right]\right)^2 - f(x_0) E\left[\hat{f}(x_0)\right]+\left(E\left[\hat{f}(x_0)\right]\right)^2                                                                          
&amp; = \underbrace{Var\left(\hat{f}(x_0)\right) + \left[\operatorname{Bias}\left(\hat{f}(x_0)\right)\right]^2}_{\text{reducible}}
\end{align*}
\]</span></p>
</div>
</div>
</div>
<section id="variance-of-hatf-at-x_0" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="variance-of-hatf-at-x_0"><strong>Variance of <span class="math inline">\(\hat{f}\)</span> at <span class="math inline">\(x_0\)</span></strong></h4>
<p><span class="math display">\[
Var(\hat{f}(x_0))=E\left[\left(\hat{f}(x_0) - E\left[\hat{f}(x_0)\right]\right)^2\right]
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Variance
</div>
</div>
<div class="callout-body-container callout-body">
<p>The variance of <span class="math inline">\(\hat{f}\)</span> at <span class="math inline">\(x_0\)</span> refers to the amount by which <span class="math inline">\(\hat{f}(x_0)\)</span> would change if we estimated it using a different training data set. Generally, different training data sets will result in a different <span class="math inline">\(\hat{f}(x_0).\)</span> Ideally the estimate for <span class="math inline">\(f\)</span> should not vary too much between training sets. If a method has high variance then small changes in the training data can result in large changes in <span class="math inline">\(\hat{f}(x_0).\)</span></p>
<p>ü§ì In general, more flexible statistical methods (e.g., KNN-regression with small <span class="math inline">\(K\)</span>) have higher variance‚Äîand vice versa.</p>
</div>
</div>
</section>
<section id="bias-of-hatf-at-x_0" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bias-of-hatf-at-x_0"><strong>Bias of <span class="math inline">\(\hat{f}\)</span> at <span class="math inline">\(x_0\)</span></strong></h4>
<p><span class="math display">\[
\operatorname{Bias}(\hat{f}(x_0))=E\left[\hat{f}(x_0)\right] - f(x_0)
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bias
</div>
</div>
<div class="callout-body-container callout-body">
<p>The bias of <span class="math inline">\(\hat{f}\)</span> at <span class="math inline">\(x_0\)</span> refers to the error that is introduced by approximating <span class="math inline">\(f(x_0)\)</span> using a <strong>nonparametric</strong> estimation approach.</p>
<p>ü§ì In general, more flexible statistical methods (e.g., KNN-regression with small <span class="math inline">\(K\)</span>) have smaller bias‚Äîand vice versa.</p>
</div>
</div>
<p>Note that <span class="math display">\[
Var\left(\hat{f}(x_0)\right)\geq 0
\]</span> and that <span class="math display">\[
\left[\operatorname{Bias}\left(\hat{f}(x_0)\right)\right]^2\geq 0.
\]</span> Thus, the expected test MSE can never lie below of <span class="math inline">\(Var(\epsilon),\)</span> i.e. <span class="math display">\[
\begin{align*}
E\left[\widehat{\operatorname{MSE}}_{test}(x_0)\right]
&amp;=\operatorname{MSE}(x_0) \\
&amp;=E\left[\left(f(x_0)-\hat{f}(x_0)\right)^2\right] + \sigma^2\\
&amp;=Var\left(\hat{f}(x_0)\right)+\left[\operatorname{Bias}\left(\hat{f}(x_0)\right)\right]^2+ \sigma^2\\
&amp; \geq \sigma^2 = Var\left(\epsilon\right).
\end{align*}
\]</span></p>
</section>
</section>
<section id="local-mse-variance-and-bias-of-knn-regression" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="local-mse-variance-and-bias-of-knn-regression"><span class="header-section-number">4.2.2</span> Local MSE, Variance, and Bias of KNN-Regression</h3>
<p>The left panel of Figure 3.16 shows the estimation result for <span class="math inline">\(K=1\)</span> and the right panel for <span class="math inline">\(K=9.\)</span></p>
<p><strong>Case <span class="math inline">\(K=1:\)</span></strong> KNN-regression <strong>interpolates</strong> all the (yellow) training data points.</p>
<p><strong>Case <span class="math inline">\(K=9:\)</span></strong> KNN-regression <strong>smoothes</strong> the (yellow) training data points.</p>
<p><img src="images/Fig_3_16.png" class="img-fluid"></p>
<p><strong>A small value for <span class="math inline">\(K\)</span></strong> provides a very flexible fit, which will have</p>
<ul>
<li>small bias <span class="math display">\[
    |\operatorname{Bias}(\hat{f}_K(x_0))| = |E(\hat{f}_K(x_0)) - f(x_0)| =\;\text{small}
    \]</span></li>
<li>large variance <span class="math display">\[
    Var(\hat{f}_K(x_0)) = \text{large}
    \]</span></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Small <span class="math inline">\(K\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>small bias</strong> is due to the fact that the estimator <span class="math inline">\(\hat{f}_K(x_0)\)</span> only uses a few (<span class="math inline">\(K\)</span> small) and thus <strong>very close</strong> and thus <strong>very good</strong> training data neighbors with <span class="math display">\[
|f(x_0) - f(X_i^{Train})|\approx 0\quad\text{since}\quad d(x_0, X_i^{Train})\approx 0.
\]</span> Thus, if we were able to average multiple estimation results <span class="math inline">\(\hat{f}(x_0)\)</span> computed from many different (independent) training data sets, then this average would be close to <span class="math inline">\(f(x_0),\)</span> i.e.&nbsp;<span class="math inline">\(E(\hat{f}(x_0))\approx f(x_0)\)</span> (small bias).</p>
<p>The <strong>large variance</strong> is due to the fact that the estimator <span class="math inline">\(\hat{f}_K(x_0)\)</span> depends on a small number of <span class="math inline">\(K\)</span> training data points such that the law of larger numbers had no chance to reduce variance yet. Thus the estimation result <span class="math inline">\(\hat{f}(x_0)\)</span> would change a lot if we re-estimated it using a different training data set.</p>
</div>
</div>
<p><strong>A large value of <span class="math inline">\(K\)</span></strong> provides a less flexible fit, which will have</p>
<ul>
<li>large bias <span class="math display">\[
    |\operatorname{Bias}(\hat{f}_K(x_0))| = |E(\hat{f}_K(x_0)) - f(x_0)| =\;\text{large}
    \]</span></li>
<li>small variance <span class="math display">\[
    Var(\hat{f}_K(x_0)) = \;\text{small}
    \]</span></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Large <span class="math inline">\(K\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>large bias</strong> is due to the fact that the estimator <span class="math inline">\(\hat{f}_K(x_0)\)</span> also uses <strong>more distant</strong> and thus <strong>rather bad</strong> training data neighbors with <span class="math display">\[
|f(x_0) - f(X_i^{Train})|\not\approx 0\quad\text{since}\quad d(x_0, X_i^{Train})\not\approx 0.
\]</span> Thus, if we were able to average multiple estimation results <span class="math inline">\(\hat{f}(x_0)\)</span> computed from many different (independent) training data sets, then this average may not be close to <span class="math inline">\(f(x_0),\)</span> i.e.&nbsp;<span class="math inline">\(E(\hat{f}(x_0))\not \approx f(x_0)\)</span> (large bias, potentially).</p>
<p>The <strong>small variance</strong> is due to the fact that the estimator <span class="math inline">\(\hat{f}_K(x_0)\)</span> depends on a larger number of <span class="math inline">\(K\)</span> traing data points such that the law of larger numbers has a chance to reduce variance. Thus the estimation result <span class="math inline">\(\hat{f}(x_0)\)</span> would not change a lot if we re-estimated it using a different training data set.</p>
</div>
</div>
<section id="locally-optimal-smoothing-parameter-k" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="locally-optimal-smoothing-parameter-k"><strong>Locally Optimal Smoothing Parameter <span class="math inline">\(K\)</span></strong></h4>
<p><strong>An optimal value of <span class="math inline">\(K\)</span></strong> can be chosen <strong>locally</strong> for every <span class="math inline">\(x_0\)</span> of interest by choosing that value of <span class="math inline">\(K\)</span> that <strong>minimizes</strong></p>
<p><span class="math display">\[\begin{align*}
\widehat{\operatorname{MSE}}_{Test}(x_0,K)
&amp; =\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i}^{Test}- \hat{f}_K(x_0)\right)^2
\end{align*}\]</span> with respect to <span class="math inline">\(K=1,2,\dots.\)</span></p>
</section>
</section>
</section>
<section id="in-class-coding-exercises" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="in-class-coding-exercises"><span class="header-section-number">4.3</span> In Class Coding Exercises</h2>
<p>Consider the following regression model: <span class="math display">\[\begin{align*}
Y_i
&amp; = f(X_i)+ \epsilon_i\\
&amp; = 5+ 3\sin(2\pi\,X_i) + \epsilon_i,
\end{align*}\]</span> where the training data has a sample size of <span class="math inline">\(n_{Train}=100,\)</span> the error term is iid normal <span class="math display">\[
\epsilon_i\overset{\text{iid}}{\sim}\mathcal{N}(0,1),
\]</span> and the predictor values are deterministic and equidistant, i.e., <span class="math display">\[
X_i=\frac{i}{n},\quad i=1,\dots,n_{Train}.
\]</span></p>
<p><strong>Problems:</strong></p>
<ol type="a">
<li><p>Programm KNN-Regression Estimator <span class="math inline">\(\hat{f}_K(x_0).\)</span> Compute the KNN-Regression estimation <span class="math inline">\(\hat{f}_K(x_0)\)</span> for <span class="math inline">\(K=5\)</span> and <span class="math inline">\(x_0 = 0.2.\)</span> Compare the result with the true regression function value <span class="math inline">\(f(0.2).\)</span></p></li>
<li><p>Compute the KNN-Regression estimation for <span class="math inline">\(K=5\)</span> and a <em>dense</em> <span class="math inline">\(x\)</span>-grid between 0 and 1 (e.g.&nbsp;<code>seq(0,1,len=500)</code>) and compare the results with the true regression function values using a plot.</p></li>
<li><p>Generate <span class="math inline">\(200\)</span> estimation results <span class="math inline">\(\hat{f}_K(x_0)\)</span> for <span class="math inline">\(x0 = 0.2\)</span> using <span class="math inline">\(200\)</span> independent training data set. Plot the results along with their sample means and the true <span class="math inline">\(f(0.2)\)</span> value using plots for different values of <span class="math inline">\(K=1,\dots,K.\)</span></p></li>
</ol>
</section>
<section id="under-construction" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="under-construction"><span class="header-section-number">4.4</span> Under Construction</h2>
</section>
<section id="assessing-model-accuracy-of-nonparametric-regression" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="assessing-model-accuracy-of-nonparametric-regression"><span class="header-section-number">4.5</span> Assessing Model Accuracy of Nonparametric Regression</h2>
<p>A fair and reliable assessment of the model accuracy requires <strong>testing data</strong>, i.e., data which comes from the same data generating process as the <strong>training data</strong>, but which was not used to compute (train) the estimator.</p>
<p>Let <span id="eq-trainingsample"><span class="math display">\[
\{(X_{1}^{Test},Y_{1}^{Test}),(X_{2}^{Test},Y_{2}^{Test}),\dots,(X_{n_{Test}}^{Test},Y_{n_{Test}}^{Test})\},
\qquad(4.3)\]</span></span> denote the <strong>test data random sample</strong>, where <span class="math display">\[
(X_{i}^{Test},Y_{i}^{Test})\overset{\text{iid}}{\sim}(X,Y),\quad i=1,\dots,n_{Test}.
\]</span> with <span class="math inline">\((X,Y)\)</span> being defined by the general regression model <span class="math display">\[
Y=f(X)+\epsilon.
\]</span></p>
<p>That is, the new test data is a <strong>random sample</strong>, which ‚Ä¶</p>
<ol type="1">
<li>is independent of the training data random sample</li>
<li>has the same distribution as the training data random sample</li>
</ol>
<p>The observed realization <span class="math display">\[
\{(X_{1,obs}^{Test},Y_{1,obs}^{Test}),(X_{2,obs}^{Test},Y_{2,obs}^{Test}),\dots,(X_{n_{Test},obs}^{Test},Y_{n_{Test},obs}^{Test})\},
\]</span> of the test data random sample is used to check the accuracy of the estimate <span class="math inline">\(\hat{f}.\)</span></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the following, we will often supress the subscript ‚Äúobs‚Äù, since often both points of view, the random variables points of view, and the observed realizations points of view make sense.</p>
</div>
</div>
<section id="global-training-data-mse" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="global-training-data-mse"><strong>Global Training Data MSE</strong></h4>
<p>A commonly used measure for the model fit is the mean squared (prediction) error (MSE).</p>
<p>The global <strong>training data MSE</strong> is given by <span class="math display">\[\begin{align*}
\widehat{\operatorname{MSE}}_{\text{train}}=\frac{1}{n_{Train}}\sum_{i=1}^n\left(Y_i - \hat{f}(X_i)\right)^2,
\end{align*}\]</span> where</p>
<ul>
<li><span class="math inline">\(\hat{f}\)</span> is computed from the training data</li>
<li><span class="math inline">\(\hat{f}(x_i)\)</span> is the prediction that <span class="math inline">\(\hat{f}\)</span> gives for the <span class="math inline">\(i\)</span>th training data observation.</li>
</ul>
<p>In general, however, we do not really care how well the method works on the training data. We are interested in the accuracy of the predictions that we obtain when we apply our method to <strong>previously unseen test data</strong>.</p>
<p>Thus, we want to choose the method that gives the <strong>lowest <em>test</em> MSE</strong>, as opposed to the lowest <em>training</em> MSE.</p>
</section>
<section id="global-test-data-mse" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="global-test-data-mse"><strong>Global Test Data MSE</strong></h4>
<p>Typically, we do not have access to such <span class="math inline">\(x_0\)</span>-specific test data.</p>
<p>Moreover, we typically we want a <strong>global</strong> measure of fit, i.e.&nbsp;for all predictor values in the range of <span class="math inline">\(X.\)</span></p>
<p>Let <span class="math display">\[
\{(X^{Test}_{1},Y^{Test}_{1}),(X^{Test}_{2},Y^{Test}_{2})\dots,(X_{n_{Test}},Y_{n_{Test}}^{Test})\}
\]</span> denote the <strong>test data</strong> with different predictor values <span class="math inline">\(X_{1}^{Test},\dots,X_{n_{Test}}^{Test}.\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>This type of test data is a realization of a random sample <span class="math display">\[
(X^{Test}_{i},Y^{Test}_{i})\overset{\text{iid}}{\sim}(X,Y),\quad i=1,\dots,n_{Test}.
\]</span> This test data random sample is independent of the training data random sample whose realization was used to compute <span class="math inline">\(\hat{f}.\)</span></p>
</div>
</div>
<p>Then, the <strong>global test MSE</strong> is given by, <span class="math display">\[
\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}=\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i}^{Test} - \hat{f}(X_{i}^{Test})\right)^2.
\end{align*}
\]</span></p>
<p>The <strong>global test MSE</strong> is an <strong>estimator</strong> of the population version of the global Mean Squared (Prediction) Error of <span class="math inline">\(\hat{f}\)</span> <span class="math display">\[
\begin{align*}
\operatorname{MSE}
&amp;= E\left[(Y - \hat{Y})^2\right]\\
&amp;= E\left[(Y - \hat{f}(X))^2\right].
\end{align*}
\]</span></p>
</section>
<section id="the-global-bias-variance-trade-off" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1" class="anchored" data-anchor-id="the-global-bias-variance-trade-off"><span class="header-section-number">4.5.1</span> The Global Bias-Variance Trade-Off</h3>
<p>using, e.g., cross-validation; see <a href="Ch6_ResamplingMethods.html" class="quarto-xref"><span>Chapter 6</span></a>.</p>
<p>The overall, i.e., <strong>global MSE</strong> can be computed by averaging the local <span class="math inline">\(\operatorname{MSE}(x_0)\)</span> over all possible values of <span class="math inline">\(x_0,\)</span> i.e. <span class="math display">\[
\operatorname{MSE} = E(\operatorname{MSE}(X)) = \int f_X(x)\operatorname{MSE}(x) dx,
\]</span> where <span class="math inline">\(f_X\)</span> denotes the design density of the predicors <span class="math inline">\(X.\)</span></p>
<p>We can <strong>estimate</strong> the global MSE using <span class="math display">\[
\widehat{\operatorname{MSE}}_{test} = \frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i}^{Test}-\hat{f}(X_{i}^{Test})\right)^2.
\]</span></p>
<p>Generally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of <span class="math inline">\(f\)</span> and vice versa.</p>
<p><strong>Figure 3.17</strong> provides an example with data generated from a one-dimensional linear regression model:</p>
<ul>
<li>black solid lines: true <span class="math inline">\(f(x)\)</span></li>
<li>blue curves: KNN fits <span class="math inline">\(\hat{f}_K(x)\)</span> using <span class="math inline">\(K = 1\)</span> (left plot) and <span class="math inline">\(K = 9\)</span> (right plot).</li>
</ul>
<p>Observations:</p>
<ul>
<li>The KNN fit <span class="math inline">\(\hat{f}_K(x)\)</span> using <span class="math inline">\(K = 1\)</span> is far too wiggly</li>
<li>The KNN fit <span class="math inline">\(\hat{f}_K(x)\)</span> using <span class="math inline">\(K = 9\)</span> is much closer to the true <span class="math inline">\(f(X).\)</span></li>
</ul>
<p>However, since the true regression function is here linear, it is hard for a non-parametric approach to compete with simple linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. <img src="images/Fig_3_17.png" class="img-fluid"></p>
<p>The blue dashed line in the left-hand panel of <strong>Figure 3.18</strong> represents the simple linear regression fit to the same data. It is almost perfect.</p>
<p>The right-hand panel of <strong>Figure 3.18</strong> reveals that linear regression outperforms KNN for this data across different choices of <span class="math inline">\(K=1,2,\dots,10.\)</span> <img src="images/Fig_3_18.png" class="img-fluid"></p>
<p><strong>Figure 3.19</strong> displays a non-linear situations in which KNN performs much better than simple linear regression. <img src="images/Fig_3_19.png" class="img-fluid"></p>
<section id="curse-of-dimensionality" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="curse-of-dimensionality">Curse of Dimensionality</h4>
<p>Unfortunately, in higher dimensions, KNN often performs worse than simple/multiple linear regression, since non-parametric approaches suffer from the <strong>curse of dimensionality</strong>.</p>
<p><strong>Figure 3.20</strong> considers the same strongly non-linear situation as in the second row of <strong>Figure 3.19</strong>, except that we have added additional noise (i.e.&nbsp;redundant) predictors that are not associated with the response.</p>
<ul>
<li>When <span class="math inline">\(p = 1\)</span> or <span class="math inline">\(p = 2,\)</span> KNN outperforms linear regression.</li>
<li>But for <span class="math inline">\(p = 3\)</span> the results are mixed, and for <span class="math inline">\(p\geq 4\)</span> linear regression is superior to KNN. <img src="images/Fig_3_20.png" class="img-fluid"></li>
</ul>
<p>Observations:</p>
<ul>
<li>When <span class="math inline">\(p=1\)</span>, a sample size of <span class="math inline">\(n=50\)</span> can provide enough information to estimate <span class="math inline">\(f(X)\)</span> accurately using non-parametric methods since the <span class="math inline">\(K\)</span> nearest neighbors can actually be close to a given test observation <span class="math inline">\(x_0.\)</span></li>
<li>However, when spreading the <span class="math inline">\(n=50\)</span> data points over a large number of, for instance, <span class="math inline">\(p=20\)</span> dimensions, the <span class="math inline">\(K\)</span> nearest neighbors tend to become far away from <span class="math inline">\(x_0\)</span> causing a large bias.</li>
</ul>
</section>
</section>
</section>
<section id="under-revision" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="under-revision"><span class="header-section-number">4.6</span> Under Revision</h2>
<section id="population-version-of-the-mean-squared-prediction-error" class="level3" data-number="4.6.1">
<h3 data-number="4.6.1" class="anchored" data-anchor-id="population-version-of-the-mean-squared-prediction-error"><span class="header-section-number">4.6.1</span> Population Version of the Mean Squared (Prediction) Error</h3>
<p>Consider a <strong>given</strong> estimate <span class="math inline">\(\hat{f}\)</span> and a <strong>given</strong> predictor <span class="math inline">\(X,\)</span> which yields a <strong>given</strong> prediction <span class="math inline">\(\hat{Y} = \hat{f}(X).\)</span> That is, assume for a moment that both <span class="math inline">\(\hat{f}\)</span> and <span class="math inline">\(X\)</span> are <strong>fixed</strong>, so that the only variability comes from <span class="math inline">\(\epsilon.\)</span> Then, it is easy to show that <span id="eq-MSEDecompFixed"><span class="math display">\[
\begin{align*}
\overbrace{E\left[(Y - \hat{Y})^2\right]}^{\text{Mean Squared (Prediction) Error}}
=\underbrace{\left(f(X) -\hat{f}(X)\right)^2}_{\text{reducable}} + \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}},
\end{align*}
\qquad(4.4)\]</span></span> where</p>
<ul>
<li><span class="math inline">\(E\left[(Y - \hat{Y})^2\right]\)</span> represents the expected value, of the squared difference between the predicted <span class="math inline">\(\hat{Y}=\hat{f}(X)\)</span> and actual value of <span class="math inline">\(Y,\)</span></li>
<li>and <span class="math inline">\(Var(\epsilon)\)</span> represents the variance associated with the error term <span class="math inline">\(\epsilon.\)</span></li>
</ul>
<p>Derivation of <a href="#eq-MSEDecompFixed" class="quarto-xref">Equation&nbsp;<span>4.4</span></a> for a <strong>given</strong> <span class="math inline">\(\hat{f}\)</span> and a <strong>given</strong> <span class="math inline">\(X;\)</span> i.e.&nbsp;only <span class="math inline">\(\epsilon\)</span> is random: <span class="math display">\[
\begin{align*}
&amp;E\left[(Y - \hat{Y})^2\right]\\[2ex]
&amp;\text{[using $Y=f(X)+\epsilon$]}\\[2ex]
&amp;=E\left[(f(X) + \epsilon - \hat{f}(X))^2\right]\\[2ex]
&amp;=E\left[((f(X)- \hat{f}(X)) + \epsilon )^2\right]\\[2ex]
&amp;\text{[binomial formula]}\\[2ex]
&amp;=E\left[\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right)\epsilon + \epsilon^2\right]\\[2ex]
&amp;\text{[using that $X$ and $\hat{f}$ are fixed]}\\[2ex]
% &amp;=E\left[\left(f(X) -\hat{f}(X)\right)^2\right] - 2E\left[\left(f(X) -\hat{f}(X)\right)\epsilon\right] + E\left[\epsilon^2\right] \\[2ex]
&amp;=\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right) E\left[\epsilon\right] + E\left[\epsilon^2\right] \\[2ex]
&amp;\text{[using that $E(\epsilon)=0$ and $E(\epsilon^2)=Var(\epsilon)=\sigma^2$]}\\[2ex]
&amp;=\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right) \cdot 0 + Var\left(\epsilon\right) \\[2ex]
&amp;=\underbrace{\left(f(X) -\hat{f}(X)\right)^2}_{\text{reducable}} + \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}}
\end{align*}
\]</span></p>
<p>Thus, the variance of the irreducible prediction error equals the <strong>lowest possible value</strong> of the mean squared (prediction) error, i.e.&nbsp; <span class="math display">\[
E\left[(Y - \hat{Y})^2\right]\geq Var\left(\epsilon\right).
\]</span></p>
<p>The same can be done for the local version of the MSE: <span class="math display">\[
\begin{align*}
&amp;E\left[(Y - \hat{Y})^2|X=x_0\right]\\[2ex]
&amp;\text{[using $Y=f(X)+\epsilon$]}\\[2ex]
&amp;=E\left[(f(X) + \epsilon - \hat{f}(X))^2\right]\\[2ex]
&amp;=E\left[((f(X)- \hat{f}(X)) + \epsilon )^2\right]\\[2ex]
&amp;\text{[binomial formula]}\\[2ex]
&amp;=E\left[\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right)\epsilon + \epsilon^2\right]\\[2ex]
&amp;\text{[using that $X$ and $\hat{f}$ are fixed]}\\[2ex]
% &amp;=E\left[\left(f(X) -\hat{f}(X)\right)^2\right] - 2E\left[\left(f(X) -\hat{f}(X)\right)\epsilon\right] + E\left[\epsilon^2\right] \\[2ex]
&amp;=\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right) E\left[\epsilon\right] + E\left[\epsilon^2\right] \\[2ex]
&amp;\text{[using that $E(\epsilon)=0$ and $E(\epsilon^2)=Var(\epsilon)=\sigma^2$]}\\[2ex]
&amp;=\left(f(X) -\hat{f}(X)\right)^2 + 2\left(f(X) -\hat{f}(X)\right) \cdot 0 + Var\left(\epsilon\right) \\[2ex]
&amp;=\underbrace{\left(f(X) -\hat{f}(X)\right)^2}_{\text{reducable}} + \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}}
\end{align*}
\]</span></p>
<p>For a really good estimate <span class="math inline">\(\hat{f}\)</span> of <span class="math inline">\(f,\)</span> we expect that <span class="math inline">\(\hat{f}(X)\approx f(X)\)</span> such that <span class="math display">\[
E\left[(Y - \hat{Y})^2\right] \approx \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}}.
\]</span> For a bad estimate <span class="math inline">\(\hat{f}\)</span> of <span class="math inline">\(f,\)</span> however, we expect that<br>
<span class="math display">\[
E\left[(Y - \hat{Y})^2\right] \gg \underbrace{Var\left(\epsilon\right)}_{\text{irreducable}}.
\]</span></p>
<p>Connecting the <strong>global test MSE</strong> with the population MSE:</p>
<p>The unknown MSE <span class="math inline">\(E\left[(Y - \hat{Y})^2\right]\)</span> can be estimated using the <strong>global test MSE:</strong> <span class="math display">\[
E\left[(Y - \hat{Y})^2\right] \approx \widehat{\operatorname{MSE}}_{\text{test}}.
\]</span></p>
<p>Thus, if <span class="math inline">\(\hat{f}\)</span> is a really good estimate of <span class="math inline">\(f,\)</span> i.e.&nbsp;if <span class="math inline">\(\hat{f}(X_i^{Test})\approx f(X_i^{Test}),\)</span> then <span class="math display">\[
\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}
&amp;=\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i} - \hat{f}(X_{i}^{Test})\right)^2\\
&amp;\approx\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i}^{Test} - f(X_{i}^{Test})\right)^2\\
&amp;=\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}(\epsilon_{i}^{Test})^2\\
&amp;=\hat{\sigma}^2\\
&amp;\approx \sigma^2.
\end{align*}
\]</span> For a bad estimate <span class="math inline">\(\hat{f}\)</span> of <span class="math inline">\(f,\)</span> however, we expect that<br>
<span class="math display">\[
\begin{align*}
\widehat{\operatorname{MSE}}_{\text{test}}
&amp;=\frac{1}{n_{Test}}\sum_{i=1}^{n_{Test}}\left(Y_{i} - \hat{f}(X_{i}^{Test})\right)^2\\
&amp;\gg \sigma^2.
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The training data MSE is <strong>not able</strong> to estimate the mean squared (prediction) error <span class="math display">\[
E\left[(Y - \hat{Y})^2\right].
\]</span></p>
</div>
</div>
<p><br></p>
</section>
<section id="global-training-and-test-mse-in-nonparametric-smoothing-spline-regression" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="global-training-and-test-mse-in-nonparametric-smoothing-spline-regression">Global Training and Test MSE in Nonparametric Smoothing Spline Regression</h3>
<p>Figure 2.9 shows training and test MSEs for smoothing spline (<code>R</code> command <code>smooth.spline()</code>) estimates <span class="math inline">\(\hat{f}\)</span> in the case of</p>
<ul>
<li>a moderately complex <span class="math inline">\(f\)</span></li>
<li>a moderate signal-to-noise ratio <span class="math inline">\(\frac{Var(f(X))}{Var(\epsilon)}\)</span></li>
</ul>
<p><img src="images/Fig_2_9.png" class="img-fluid"></p>
<p><br></p>
<p>Figure 2.10 shows training and test MSEs for smoothing spline estimates <span class="math inline">\(\hat{f}\)</span> in the case of</p>
<ul>
<li>a very simple <span class="math inline">\(f\)</span></li>
<li>a moderate signal-to-noise ratio <span class="math inline">\(\frac{Var(f(X))}{Var(\epsilon)}\)</span></li>
</ul>
<p><img src="images/Fig_2_10.png" class="img-fluid"></p>
<p><br></p>
<p>Figure 2.11 shows training and test MSEs for smoothing spline estimates <span class="math inline">\(\hat{f}\)</span> in the case of</p>
<ul>
<li>a moderately complex <span class="math inline">\(f\)</span></li>
<li>a very large signal-to-noise ratio <span class="math inline">\(\frac{Var(f(X))}{Var(\epsilon)}\)</span></li>
</ul>
<p><img src="images/Fig_2_11.png" class="img-fluid"></p>
<p><br></p>
<p>In practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably more difficult because usually no test data are available.</p>
<p>As the three examples in Figures 2.9, 2.10, and 2.11 of our textbook illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably.</p>
<p>Throughout this book, we discuss a variety of approaches that can be used in practice to estimate the minimum point of the test MSE.</p>
<p>One important method is <strong>cross-validation</strong>, which is a method for estimating the test MSE using the training data.</p>
<!-- $$
E\left[E[(Y_0- \hat{f}(X))^2|X]\right]
$$ -->
<p><img src="images/Fig_2_12.png" class="img-fluid"></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch_MatrixAlgebra.html" class="pagination-link" aria-label="Matrix Algebra">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch5_Classification.html" class="pagination-link" aria-label="Classification">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>