<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>2&nbsp; Linear Regression – Computer-Aided Statistical Analysis (B.Sc.)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch3_MatrixAlgebra.html" rel="next">
<link href="./Ch1_Intro2R.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch4_LinearRegression.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer-Aided Statistical Analysis (B.Sc.)</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Intro2R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title"><code>R</code>-Lab: Introduction to <code>R</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_LinearRegression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_MatrixAlgebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_Classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_ResamplingMethods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-LinModAssumptions" id="toc-sec-LinModAssumptions" class="nav-link active" data-scroll-target="#sec-LinModAssumptions"><span class="header-section-number">2.1</span> Assumptions</a>
  <ul class="collapse">
<li><a href="#discussion-of-the-model-assumptions" id="toc-discussion-of-the-model-assumptions" class="nav-link" data-scroll-target="#discussion-of-the-model-assumptions"><span class="header-section-number">2.1.1</span> Discussion of the Model Assumptions</a></li>
  </ul>
</li>
  <li><a href="#deriving-the-expression-of-the-ols-estimator" id="toc-deriving-the-expression-of-the-ols-estimator" class="nav-link" data-scroll-target="#deriving-the-expression-of-the-ols-estimator"><span class="header-section-number">2.2</span> Deriving the Expression of the OLS Estimator</a></li>
  <li><a href="#fitted-values-residuals-and-projection-matrices" id="toc-fitted-values-residuals-and-projection-matrices" class="nav-link" data-scroll-target="#fitted-values-residuals-and-projection-matrices"><span class="header-section-number">2.3</span> Fitted Values, Residuals, and Projection Matrices</a></li>
  <li><a href="#assessing-the-accuracy-of-the-model-fit" id="toc-assessing-the-accuracy-of-the-model-fit" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-model-fit"><span class="header-section-number">2.4</span> Assessing the Accuracy of the Model Fit</a></li>
  <li>
<a href="#assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" id="toc-assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-coefficient-estimators-hatbeta"><span class="header-section-number">2.5</span> Assessing the Accuracy of the Coefficient Estimators <span class="math inline">\(\hat{\beta}\)</span></a>
  <ul class="collapse">
<li><a href="#bias-of-hatbeta" id="toc-bias-of-hatbeta" class="nav-link" data-scroll-target="#bias-of-hatbeta"><span class="header-section-number">2.5.1</span> Bias of <span class="math inline">\(\hat{\beta}\)</span></a></li>
  <li><a href="#standard-error-of-hatbeta_j" id="toc-standard-error-of-hatbeta_j" class="nav-link" data-scroll-target="#standard-error-of-hatbeta_j"><span class="header-section-number">2.5.2</span> Standard Error of <span class="math inline">\(\hat{\beta}_j\)</span></a></li>
  </ul>
</li>
  <li>
<a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference"><span class="header-section-number">2.6</span> Inference</a>
  <ul class="collapse">
<li><a href="#confidence-intervals-for-beta_j" id="toc-confidence-intervals-for-beta_j" class="nav-link" data-scroll-target="#confidence-intervals-for-beta_j"><span class="header-section-number">2.6.1</span> Confidence Intervals for <span class="math inline">\(\beta_j\)</span></a></li>
  <li><a href="#confidence-intervals-for-statistical-hypothesis-testing" id="toc-confidence-intervals-for-statistical-hypothesis-testing" class="nav-link" data-scroll-target="#confidence-intervals-for-statistical-hypothesis-testing"><span class="header-section-number">2.6.2</span> Confidence Intervals for Statistical Hypothesis Testing</a></li>
  <li><a href="#t-test" id="toc-t-test" class="nav-link" data-scroll-target="#t-test"><span class="header-section-number">2.6.3</span> <span class="math inline">\(t\)</span>-Test</a></li>
  <li><a href="#f-test" id="toc-f-test" class="nav-link" data-scroll-target="#f-test"><span class="header-section-number">2.6.4</span> <span class="math inline">\(F\)</span>-Test</a></li>
  <li><a href="#probability-of-a-type-i-error-power-and-consistency" id="toc-probability-of-a-type-i-error-power-and-consistency" class="nav-link" data-scroll-target="#probability-of-a-type-i-error-power-and-consistency"><span class="header-section-number">2.6.5</span> Probability of a Type I Error, Power and Consistency</a></li>
  </ul>
</li>
  <li>
<a href="#other-considerations-in-the-regression-model" id="toc-other-considerations-in-the-regression-model" class="nav-link" data-scroll-target="#other-considerations-in-the-regression-model"><span class="header-section-number">2.7</span> Other Considerations in the Regression Model</a>
  <ul class="collapse">
<li><a href="#qualitative-predictors" id="toc-qualitative-predictors" class="nav-link" data-scroll-target="#qualitative-predictors"><span class="header-section-number">2.7.1</span> Qualitative Predictors</a></li>
  <li><a href="#extensions-of-the-linear-model" id="toc-extensions-of-the-linear-model" class="nav-link" data-scroll-target="#extensions-of-the-linear-model"><span class="header-section-number">2.7.2</span> Extensions of the Linear Model</a></li>
  <li><a href="#detecting-potential-problems" id="toc-detecting-potential-problems" class="nav-link" data-scroll-target="#detecting-potential-problems"><span class="header-section-number">2.7.3</span> Detecting Potential Problems</a></li>
  </ul>
</li>
  <li>
<a href="#comparison-linear-regression-vs.-k-nn-regression" id="toc-comparison-linear-regression-vs.-k-nn-regression" class="nav-link" data-scroll-target="#comparison-linear-regression-vs.-k-nn-regression"><span class="header-section-number">2.8</span> Comparison: Linear Regression vs.&nbsp;K-NN Regression</a>
  <ul class="collapse">
<li><a href="#k-nearest-neighbors-k-nn-regression" id="toc-k-nearest-neighbors-k-nn-regression" class="nav-link" data-scroll-target="#k-nearest-neighbors-k-nn-regression"><span class="header-section-number">2.8.1</span> K-Nearest Neighbors (K-NN) Regression</a></li>
  </ul>
</li>
  <li>
<a href="#self-study-exercises" id="toc-self-study-exercises" class="nav-link" data-scroll-target="#self-study-exercises"><span class="header-section-number">2.9</span> Self-Study: Exercises</a>
  <ul class="collapse">
<li><a href="#solutions" id="toc-solutions" class="nav-link" data-scroll-target="#solutions">Solutions</a></li>
  </ul>
</li>
  <li>
<a href="#self-study-r-lab-linear-regression" id="toc-self-study-r-lab-linear-regression" class="nav-link" data-scroll-target="#self-study-r-lab-linear-regression"><span class="header-section-number">2.10</span> Self-Study: <code>R</code>-Lab Linear Regression</a>
  <ul class="collapse">
<li><a href="#libraries" id="toc-libraries" class="nav-link" data-scroll-target="#libraries"><span class="header-section-number">2.10.1</span> Libraries</a></li>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression"><span class="header-section-number">2.10.2</span> Simple Linear Regression</a></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">2.10.3</span> Multiple Linear Regression</a></li>
  <li><a href="#interaction-terms" id="toc-interaction-terms" class="nav-link" data-scroll-target="#interaction-terms"><span class="header-section-number">2.10.4</span> Interaction Terms</a></li>
  <li><a href="#non-linear-transformations-of-the-predictors" id="toc-non-linear-transformations-of-the-predictors" class="nav-link" data-scroll-target="#non-linear-transformations-of-the-predictors"><span class="header-section-number">2.10.5</span> Non-linear Transformations of the Predictors</a></li>
  <li><a href="#qualitative-predictors-1" id="toc-qualitative-predictors-1" class="nav-link" data-scroll-target="#qualitative-predictors-1"><span class="header-section-number">2.10.6</span> Qualitative Predictors</a></li>
  <li><a href="#writing-functions" id="toc-writing-functions" class="nav-link" data-scroll-target="#writing-functions"><span class="header-section-number">2.10.7</span> Writing Functions</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-linearRegCh" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><!-- LTeX: language=en-US --><div class="hidden">
<p><span class="math display">\[
\require{color}
%% Colorbox within equation-environments:
\newcommand{\highlight}[2][yellow]{\mathchoice%
  {\colorbox{#1}{$\displaystyle#2$}}%
  {\colorbox{#1}{$\displaystyle#2$}}%
  {\colorbox{#1}{$\displaystyle#2$}}%
  }%
\]</span></p>
</div>
<p>This chapter is based on the following references:</p>
<ul>
<li>Chapter 3 of <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a>
</li>
<li>Chapter 1 of Econometrics by Fumio Hayashi</li>
</ul>
<section id="sec-LinModAssumptions" class="level2" data-number="2.1"><h2 data-number="2.1" class="anchored" data-anchor-id="sec-LinModAssumptions">
<span class="header-section-number">2.1</span> Assumptions</h2>
<p>The (multiple) linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:</p>
<section id="assumption-1-model-and-sampling" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="assumption-1-model-and-sampling"><strong>Assumption 1: Model and Sampling</strong></h4>
</section><section id="part-a-linear-model" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="part-a-linear-model"><strong>Part (a): Linear Model</strong></h4>
<p><span id="eq-LinMod"><span class="math display">\[
\begin{align}
  Y_i
  &amp; = \beta_0 X_{i0}+\beta_1 X_{i1}+\dots+\beta_p X_{ip}+\epsilon_i\\
  &amp; = \sum_{k=0}^p\beta_k X_{ik}+\epsilon_i,
\end{align}
\qquad(2.1)\]</span></span> where <span class="math inline">\(i=1,\dots,n\)</span> is the index for denoting the statistical units (individuals, companies).</p>
<p>We set the values of the first predictor equal to one <span class="math display">\[
X_{i0}=1
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> to model <span class="math inline">\(\beta_0\)</span> as the intercept.</p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span> is called <strong>dependent variable</strong>, <strong>outcome variable</strong> or <strong>regressand</strong>
</li>
<li>
<span class="math inline">\(X_{ik}\)</span> is called the <span class="math inline">\(k\)</span>th <strong>independent variable</strong>, <strong>predictor variable</strong>, <strong>regressor</strong>, <strong>explanatory variable</strong>, or <strong>control variable</strong>.</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> is the error term.</li>
</ul>
<p>It is convenient to write <a href="#eq-LinMod" class="quarto-xref">Equation&nbsp;<span>2.1</span></a> using matrix notation <span class="math display">\[
\begin{eqnarray*}
  Y_i&amp;=&amp;\underset{(1\times (p+1))}{X_i^{\top}}\underset{((p+1)\times 1)}{\beta} +\epsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}
\]</span> where <span class="math display">\[
X_i=\left(\begin{matrix}X_{i0}\\ X_{i1}\\ \vdots\\  X_{ip}\end{matrix}\right)
=\left(\begin{matrix}1\\ X_{i1}\\ \vdots\\  X_{ip}\end{matrix}\right)
  \quad\text{and}\quad
\beta=\left(\begin{matrix}\beta_0\\ \beta_1\\ \vdots\\ \beta_p\end{matrix}\right).
\]</span> Stacking the components for all <span class="math inline">\(i=1,\dots,n,\)</span> leads to <span class="math display">\[
\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&amp;=&amp;\underset{(n\times (p+1))}{X}\underset{((p+1)\times 1)}{\beta} + \underset{(n\times 1)}{\epsilon},
\end{eqnarray*}
\]</span> where <span class="math display">\[
\begin{align*}
Y=\left(\begin{matrix}Y_1
\\ \vdots\\
Y_n
\end{matrix}\right),
\quad
X&amp;=\left(\begin{matrix}
X_{10}&amp;\dots&amp;X_{1p}\\
\vdots&amp;\ddots&amp;\vdots\\
X_{n0}&amp;\dots&amp;X_{np}\\
\end{matrix}\right),
\quad
\text{and}
\quad
\epsilon=
\left(\begin{matrix}
\epsilon_1\\ \vdots\\
\epsilon_n
\end{matrix}\right)\\
&amp;=\left(\begin{matrix}
\;\;1\;\;&amp;\dots&amp;X_{1p}\\
\vdots&amp;\ddots&amp;\vdots\\
\;\;1\;\;&amp;\dots&amp;X_{np}\\
\end{matrix}\right)
\end{align*}
\]</span></p>
<!-- 
::: {.callout-note}
The assumption $f(X_i) = X_i^{\top}\beta$ may be a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple.  
::: 
-->
</section><section id="part-b-random-sampling" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="part-b-random-sampling"><strong>Part (b): Random Sampling</strong></h4>
<p>We assume that the <span class="math inline">\(n\)</span> observed data points <span class="math display">\[
((Y_{1,obs},X_{10,obs},\dots,X_{1p,obs}),\dots,(Y_{n,obs},X_{n0,obs},\dots,X_{np,obs}))
\]</span> are a realization of an <strong>(iid) random sample</strong> <span class="math display">\[
((Y_{1},X_{10},\dots,X_{1p}),\dots,(Y_{n},X_{n0},\dots,X_{np})).
\]</span></p>
<p>This means that we consider a situation, where the <span class="math inline">\(p+2\)</span>-dimensional random vectors <span class="math inline">\((Y_{i},X_{i0},\dots,X_{ip})\)</span> are <strong>indepenent and identically distributed (iid)</strong> across <span class="math inline">\(i=1,\dots,n\)</span>, i.e.&nbsp;</p>
<ol type="1">
<li>
<strong>Independent:</strong> The two <span class="math inline">\(p+2\)</span>-dimensional random vectors <span class="math display">\[
(Y_{i},X_{i0},\dots,X_{ip})\quad\text{and}\quad (Y_{j},X_{j0},\dots,X_{jp})
\]</span> are independent for all <span class="math inline">\((i,j)\)</span>-pairs with <span class="math inline">\(i,j=1,\dots,n\)</span> and <span class="math inline">\(i\neq j.\)</span>
</li>
<li>
<strong>Identical Distributions:</strong> The <span class="math inline">\(p+2\)</span>-dimensional random vector <span class="math display">\[
(Y_{i},X_{i0},\dots,X_{ip})
\]</span> has the same distribution for all <span class="math inline">\(i=1,\dots,n.\)</span>
</li>
</ol>
<!-- 
::: {.callout-note}
Due to @eq-LinMod, this i.i.d. assumption is equivalent to assuming that the multivariate random variables 
$$
(\epsilon_i,X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+2}
$$ 
are i.i.d. across $i=1,\dots,n$. 
::: 
--><!-- 
::: {.callout-caution}
**Remark:** Often, we do not use a different notation for observed realizations 
$$
(Y_{i,obs},X_{i0,obs},\dots,X_{ip,obs})
$$
and the corresponding (multivariate) random variable 
$$
(Y_{i},X_{i0},\dots,X_{ip})
$$ 
since often both interpretations (random variable and its realizations) can make sense 
in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.
::: 
--></section><section id="assumption-2-exogeneity" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="assumption-2-exogeneity"><strong>Assumption 2: Exogeneity</strong></h4>
<p><span id="eq-assExogen"><span class="math display">\[
E(\epsilon_i|X_i)=0,\quad i=1,\dots,n
\qquad(2.2)\]</span></span></p>
<p>This assumption demands that the mean of the random error term <span class="math inline">\(\epsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_i\in\mathbb{R}^{p+1}.\)</span></p>
<p>The exogeneity assumption is also called</p>
<ul>
<li>
<strong>orthogonality assumption</strong> or</li>
<li>
<strong>mean independence assumption</strong>.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Together with the random sample assumption (Assumption 1, Part (b)) <a href="#eq-assExogen" class="quarto-xref">Equation&nbsp;<span>2.2</span></a> even implies <strong>strict exogeneity</strong> <span class="math display">\[
E(\epsilon|X) = \underset{(n\times 1)}{0},
\]</span> since we have independence across <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<p>Under strict exogeneity, the mean of the random error <strong>vector</strong> <span class="math inline">\(\epsilon\in\mathbb{R}^n\)</span> is zero irrespective of the realizations of the <span class="math inline">\((n\times (p+1))\)</span>-dimensional random predictor matrix <span class="math inline">\(X.\)</span></p>
</div>
</div>
</section><section id="assumption-3-rank-condition-no-perfect-multicollinearity" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="assumption-3-rank-condition-no-perfect-multicollinearity"><strong>Assumption 3: Rank Condition (No Perfect Multicollinearity)</strong></h4>
<p><span class="math display">\[
\begin{align*}
\operatorname{rank}(X)&amp;=p+1\quad\text{a.s.}\\[2ex]
\Leftrightarrow P\big(\operatorname{rank}(X)&amp;= p+1 \big)=1
\end{align*}
\]</span> This assumption demands that, with probability one, no predictor variable <span class="math inline">\(X_{k}\in\mathbb{R}^n\)</span> (i.e.&nbsp;no column in <span class="math inline">\(X\)</span>) is linearly dependent of the others. (This is the literal translation of the “almost surely (a.s.)” concept.)</p>
<p><strong>Note:</strong> The assumption implies that <span class="math inline">\(n\geq (p+1),\)</span> since <span class="math display">\[
\operatorname{rank}(X)\leq \min\{n,(p+1)\}\quad(a.s.)
\]</span></p>
<!-- 
This rank assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation).   
-->
<p>This assumption rules out redundant predictors. For instance, when income measured in Euros is a predictor variable, then adding the same income variable, but measured in Cents would be redundant. We would have a perfect linear dependency between both variables violating Assumption 3.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Under Assumption 3, we have that <span class="math display">\[
\operatorname{rank}(X)=(p+1)\quad\text{(a.s.)}
\]</span></p></li>
<li><p>This implies that the <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix <span class="math inline">\(X^{\top}X\)</span> has full rank, i.e.&nbsp;that <span class="math display">\[
\operatorname{rank}(X^{\top}X)=(p+1)\quad\text{(a.s.)}
\]</span></p></li>
<li><p>Thus <span class="math inline">\((X^{\top}X)\)</span> is invertible; i.e.&nbsp;there exists a <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix <span class="math inline">\((X^{\top}X)^{-1}\)</span> such that <span class="math display">\[
(X^{\top}X)(X^{\top}X)^{-1} = (X^{\top}X)^{-1}(X^{\top}X) = I_{(p+1)}.
\]</span></p></li>
</ul>
</div>
</div>
</section><section id="assumption-4-error-variance" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="assumption-4-error-variance"><strong>Assumption 4: Error Variance</strong></h4>
<!-- 
The $n$-vector of error terms 
$$
\epsilon=\left(\begin{matrix}
\epsilon_1\\ \vdots\\ 
\epsilon_n
\end{matrix}\right)
$$
has to fulfill the mean independence assumption (Assumption 2), i.e. 
$$
E(\epsilon|X)=\underset{(n\times 1)}{0}.
$$
But Assumption 2 is fulled by many different error term distributions. Typically, we do not want to specify the total distribution of $\epsilon$, since typically we simply do not know the exact distribution of the error terms.  
-->
<p>For doing inference, we need to make a realistic assumption about the conditional variance of <span class="math inline">\(\epsilon\)</span>, given <span class="math inline">\(X\)</span> <span class="math display">\[
\begin{align*}
&amp;\underset{(n\times n)}{Var\left(\epsilon|X\right)}=\\[2ex]
&amp; = \left(\begin{matrix}
Var(\epsilon_1|X)&amp;Cov(\epsilon_1,\epsilon_2|X)&amp;\dots&amp;Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&amp;Var(\epsilon_2|X)&amp;\dots&amp;Cov(\epsilon_2,\epsilon_n|X)\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
Cov(\epsilon_n,\epsilon_1|X)&amp;Cov(\epsilon_n,\epsilon_2|X)&amp;\dots&amp;Var(\epsilon_n|X)
\end{matrix}\right)
\end{align*}
\]</span> By the (iid) random sample assumption (Assumption 1, Part (b)), we have independence between individual error terms, such that <span class="math display">\[
\begin{align*}
&amp;\underset{(n\times n)}{Var\left(\epsilon|X\right)}=\\[2ex]
&amp; = \left(\begin{matrix}
Var(\epsilon_1|X_1)&amp;0&amp;\dots&amp;0\\
0&amp;Var(\epsilon_2|X_2)&amp;\dots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\dots&amp;Var(\epsilon_n|X_n)
\end{matrix}\right)
\end{align*}
\]</span></p>
<p>Under the (iid) random sampling assumption, we distinguishes two cases: <strong>heteroskedastic</strong> and <strong>homoskedastic</strong> errors.</p>
<p><strong>Heteroskedastic errors</strong> allow for different conditional variances <span class="math display">\[
Var(\epsilon_i|X_i)=\sigma^2(X_i)&gt;0
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> such that <span class="math display">\[
\begin{align*}
&amp;\underset{(n\times n)}{Var\left(\epsilon|X\right)}=\\[2ex]
&amp; = \left(\begin{matrix}
\sigma^2(X_1)&amp;0&amp;\dots&amp;0\\
0&amp;\sigma^2(X_2)&amp;\dots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\dots&amp;\sigma^2(X_n)
\end{matrix}\right)
\end{align*}
\]</span></p>
<p><strong>Homoskedastic errors</strong> assume equal conditional variances <span class="math display">\[
Var(\epsilon_i|X_i)=sigma^2&gt;0
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> such that <span class="math display">\[
\begin{align*}
&amp;\underset{(n\times n)}{Var\left(\epsilon|X\right)}=\\[2ex]
&amp; = \left(\begin{matrix}
\sigma^2&amp;0&amp;\dots&amp;0\\
0&amp;\sigma^2&amp;\dots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\dots&amp;\sigma^2
\end{matrix}\right)
= \sigma^2 I_n,
\end{align*}
\]</span></p>
<!--
List of typical classes of error term distributions:

- **Conditional distribution with sufficiently many moments:** 
$$
\epsilon_i|X_i \sim f_{\epsilon|X}
$$ 
for all $i=1,\dots,n$ and for any distribution $f_{\epsilon|X}$ with two (or more) finite moments.
  - **Example: Conditional normal distribution** 
  $$
  \epsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))
  $$ 
  for all $i=1,\dots,n$.

- **Independence between error and predictors:** $\epsilon_i\sim f_\epsilon$ for all $i=1,\dots,n$ such that $f_\epsilon=f_{\epsilon|X}$ and such that $f_\epsilon$ has two (or more) finite moments.  
   - **Example: Independence between a Gaussian error and the predictors** 
   $$
   f_\epsilon=\mathcal{N}(0,\sigma^2),
   $$ 
   where $\sigma^2$ does not depend on $X.$

- **Spherical errors:** The conditional distributions of  $\epsilon_i|X_i$ may generally depend on $X_i$ for all $i=1,\dots,n,$ but only such that
$$
E(\epsilon|X)=\underset{(n\times 1)}{0}
$$
and 
$$
\begin{align*}
&\underset{(n\times n)}{Var\left(\epsilon|X\right)}=\\[2ex] 
& = \left(\begin{matrix}
Var(\epsilon_1|X)&Cov(\epsilon_1,\epsilon_2|X)&\dots&Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&Var(\epsilon_2|X)&\dots&Cov(\epsilon_2,\epsilon_n|X)\\
\vdots&\vdots&\ddots&\vdots\\
Cov(\epsilon_n,\epsilon_1|X)&Cov(\epsilon_n,\epsilon_2|X)&\dots&Var(\epsilon_n|X)
\end{matrix}\right)\\[2ex]
& = \left(\begin{matrix}
\sigma^2&0&\dots&0\\
0&\sigma^2&\dots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\dots&\sigma^2
\end{matrix}\right)
= \sigma^2 I_n,
\end{align*}
$$
where $I_n$ denotes the $(n\times n)$ identity matrix with ones on the diagonal and zeros else.</br> 
Thus, under the spherical errors assumption, one has, for all possible realizations of $X$, that: 
   * **uncorrelated:** $Cov(\epsilon_i,\epsilon_j|X)=0$ for all $i=1,\dots,n$ and all $j=1,\dots,n$ such that $i\neq j$ 
   * **homoskedastic:** $Var(\epsilon_i|X)=\sigma^2$ for all $i=1,\dots,n$
-->
</section><section id="discussion-of-the-model-assumptions" class="level3" data-number="2.1.1"><h3 data-number="2.1.1" class="anchored" data-anchor-id="discussion-of-the-model-assumptions">
<span class="header-section-number">2.1.1</span> Discussion of the Model Assumptions</h3>
<section id="assumption-1-part-a" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="assumption-1-part-a"><strong>Assumption 1, Part (a)</strong></h4>
<p>Particularly, the <strong>linearity</strong> assumption is often misunderstood.</p>
<p>A function <span class="math inline">\(f:\mathbb{R}^{p+1}\to\mathbb{R}\)</span> is called a <strong>linear</strong> function if the following two conditions hold:</p>
<ul>
<li>
<span class="math inline">\(f(X_i + \tilde{X}_i) = f(X_i) + f(\tilde{X}_i)\)</span> for predictors <span class="math inline">\(X_i,\tilde{X}_i\in\mathbb{R}^{p+1}\)</span>
</li>
<li>
<span class="math inline">\(f(a\cdot X_i) = a\cdot f( X_i)\)</span> for any scalar <span class="math inline">\(a\in\mathbb{R}\)</span>
</li>
</ul>
<p>The multiple linear regression function is given by <span class="math display">\[
f(X_i)  = E(Y_i|X_i) = X_i^{\top}\beta.
\]</span> Derivation: <span class="math display">\[
\begin{align*}
f(X_i)
&amp; = E(Y_i|X_i) \\
&amp; = E(\beta_0 + \beta_1 X_{i1} +\dots + \beta_p X_{ip} + \epsilon_i |X_i)\\
&amp; = \beta_0 + \beta_1 X_{i1} +\dots + \beta_p X_{ip} + E(\epsilon_i |X_i)\\
&amp; = \beta_0 + \beta_1 X_{i1} +\dots + \beta_p X_{ip}\\
&amp; = X_i^{\top}\beta
\end{align*}
\]</span></p>
<p>Checking the two linearity conditions for <span class="math inline">\(f(X_i) = X_i^{\top}\beta\colon\)</span></p>
<ul>
<li><span class="math inline">\(f(X_i + \tilde{X}_i)=(X_i + \tilde{X}_i)^{\top}\beta = X_i^{\top}\beta + \tilde{X}_i^{\top}\beta=f(X_i) + f(\tilde{X}_i)\)</span></li>
<li><span class="math inline">\(f(aX_i)=(aX_i)^{\top}\beta=a(X_i^{\top}\beta)=a\,f(X_i)\)</span></li>
</ul>
<p>Thus, the multiple linear regression function <span class="math inline">\(f(X_i)=X_i^{\top}\beta\)</span> is a linear function.</p>
<p>Linear functions are generally <strong>quite flexible</strong> functions and provide a structure that can be <strong>interpreted easily</strong> using partial derivatives: <span class="math display">\[
\frac{\partial}{\partial X_{ik}}f(X_i) = \beta_k
\]</span> That is, changing <span class="math inline">\(X_{ik}\)</span> by one unit, changes <span class="math inline">\(f(X_i)=E(Y_i|X_i)\)</span> by <span class="math inline">\(\beta_k\)</span> units.</p>
<p>Often we say: “On average, changing <span class="math inline">\(X_{ik}\)</span> by one unit, changes <span class="math inline">\(Y_i\)</span> by <span class="math inline">\(\beta_k\)</span> units.”</p>
<p>Frequently, the linearity assumption is misunderstood as a straight line fitting assumption. However, only in the special case of the <strong>simple linear regression function</strong> <span class="math display">\[
f(X_i) = \beta_0 + \beta_1 X_{i1}
\]</span> <strong>linearity</strong> means <strong>straight line</strong>.</p>
<p>Generally, a linear regression function can have quite complex functional forms. The multiple linear regression model can, for instance, fit polynomials (clearly not straight line).</p>
<p>The <strong>polynomial regression model</strong> is a special case of the multiple linear regression model, where <span class="math display">\[
\begin{align*}
X_{i2} &amp;:= X_{i1}^2\\
X_{i3} &amp;:= X_{i1}^3\\
&amp;\vdots\\
X_{ip} &amp;:= X_{i1}^p
\end{align*}
\]</span> such that <span class="math display">\[
f(X_i) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i1}^2 +\dots + \beta_p X_{i1}^p.
\]</span></p>
<p>Partial effects remain easy to interpret in polynomial regression functions: <span class="math display">\[
\frac{\partial}{\partial X_{i1}}f(X_i) = \beta_1 + 2\beta_2 X_{i1} + \dots + p\beta_2 X_{i1}^{p-1}
\]</span> That is, changing <span class="math inline">\(X_{i1}\)</span> by one unit, changes <span class="math inline">\(f(X_i)=E(Y_i|X_i)\)</span> by <span class="math display">\[
\beta_1 + 2\beta_2 X_{i1} + \dots + p\beta_2 X_{i1}^{p-1}
\]</span> units.</p>
<p>Of course, further predictor variables <span class="math inline">\(X_{i2},\dots,X_{ip}\)</span> can be added to the polynomial regression function.</p>
</section><section id="assumption-1-part-b" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="assumption-1-part-b"><strong>Assumption 1, Part (b)</strong></h4>
<p>The iid random sample assumption is a rather simple sampling assumption and applies to situations where the subjects <span class="math inline">\(i=1,\dots,n\)</span> were randomly selected from a population.</p>
<p>The iid random sample is typically violated in</p>
<ul>
<li>
<strong>time-series data</strong>, where data points from one time-unit <span class="math inline">\(i\)</span> typically depend on previous time-units <span class="math inline">\(i-1\)</span>, <span class="math inline">\(i-2\)</span>, etc.</li>
<li>
<strong>household data</strong>, where one may have independence across households, but not necessarily within the households (family members often share similar characteristics like height, income, etc.)</li>
</ul></section><section id="assumption-2-exogeneity-1" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="assumption-2-exogeneity-1"><strong>Assumption 2: Exogeneity</strong></h4>
<p>The exogeneity assumption <span class="math display">\[
E(\epsilon_i|X_i)=0
\]</span> implies that <span class="math display">\[
\operatorname{Cov}(\epsilon_i,X_{ik})=0\quad \text{for all}\quad k=0,\dots,p,
\]</span> since <span class="math display">\[
\begin{align*}
\operatorname{Cov}(\epsilon_i,X_{ik})
&amp;=E(\epsilon_i X_{ik})- \overbrace{E(\epsilon_i)}^{=0}\; E(X_{ik})\\
&amp;=E(\epsilon_i X_{ik})\\
&amp;=E(E(\epsilon_i X_{ik}|X_i))\\
&amp;=E(X_{ik} \; \underbrace{E(\epsilon_i |X_k)}_{=0}\,) = 0\quad \text{for all}\quad k=0,\dots,p.
\end{align*}
\]</span> Thus <span class="math display">\[
\underbrace{E(\epsilon_i|X_i)=0}_{A} \Rightarrow \underbrace{\operatorname{Cov}(\epsilon_i,X_{ik})=0\quad \text{for all}\quad k=0,\dots,p}_{B}
\]</span></p>
<p><strong>Remember:</strong> Any logical implication <span class="math display">\[
A\Rightarrow B
\]</span> has a logically equivalent contrapositive: <span class="math display">\[
\neg B\Rightarrow \neg A
\]</span> Therefore, we also have that <span class="math display">\[
\underbrace{\operatorname{Cov}(\epsilon_i,X_{ik})\neq 0\;\text{for at least one}\; k=0,\dots,p}_{\neg B} \Rightarrow \underbrace{E(\epsilon_i|X_i)\neq 0}_{\neg A}
\]</span> I.e., if any of the regressors <span class="math inline">\(X_{ik}\)</span>, <span class="math inline">\(k=0,\dots,p,\)</span> correlates with the error term <span class="math inline">\(\epsilon_i\)</span>, then the exogeneity assumption is violated.</p>
<p>Unfortunately, this violation often happens in economic data problems.</p>
<!-- 
The exogeneity assumption 
$$
E(\epsilon_i|X_i)=0,\quad i=1,\dots,n
$$
hold trivially, if $\epsilon_i$ is independent of all regressors $X_{i0},X_{i1}\dots,X_{ip}$
$$
X_{i0},X_{i1}\dots,X_{ip} \perp\!\!\!\!\perp \epsilon_i.
$$
Assuming independence between the error term $\epsilon_i$ and the regressors in $X_i$ is, however, typically unrealistic in economic data.   
-->
<p><strong>Classical Example:</strong> Wage regression</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i,
\]</span> where</p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span> denotes subject <span class="math inline">\(i\)</span>’s hourly <strong>wage</strong>,<br>
</li>
<li>
<span class="math inline">\(X_{i1}\)</span> denotes subjects <span class="math inline">\(i\)</span>’s years of <strong>education</strong>, and</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> denotes the error term</li>
</ul>
<p>Typically, however, the predictor <strong>ability</strong> (natural skill, intelligence, motivation) also affects wages. Let’s say, the actually true regression model would be given by <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \eta_i,
\]</span> where</p>
<ul>
<li>
<span class="math inline">\(X_{i2}\)</span> denotes subject <span class="math inline">\(i\)</span>’s <strong>ability</strong>,</li>
<li>
<span class="math inline">\(\eta_i\)</span> denotes a well-behaved error term, for which the exogeneity assumption <span class="math inline">\(E(\eta_i|X_{i})=0\)</span> is fulfilled, which implies <span class="math display">\[
\operatorname{Cov}(\eta_i,X_{i1})=\operatorname{Cov}(\eta_i,X_{i2})=0.
\]</span>
</li>
</ul>
<p>Subjects with high <strong>ability</strong> typically stay longer in school and at the university, such that <strong>ability</strong> and <strong>eduction</strong> correlate positively, i.e., <span class="math display">\[
\operatorname{Cov}(X_{i2},X_{i1})&gt;0.
\]</span></p>
<p><strong>Ability</strong> is a hardly measurable predictor and thus usually must be left omitted (<strong>omitted variable</strong>). In this case, the <strong>ability</strong> effect becomes part of the error term in our wage regression model: <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \underbrace{\epsilon_i.}_{=\beta_2 X_{i2} + \eta_i}
\]</span></p>
<p>This omitted variable situation violates the exogeneity assumption for <span class="math inline">\(\epsilon_i\)</span> (by the <span class="math inline">\(\neg B\Rightarrow \neg A\)</span> argument above): <span class="math display">\[
\begin{align*}
\operatorname{Cov}(\epsilon_i, X_{i1})
&amp;=\operatorname{Cov}(\beta_2 X_{i2} + \eta_i, X_{i1})\\[2ex]
&amp;=\beta_2 \;\underbrace{\operatorname{Cov}(X_{i2}, X_{i1})}_{\neq 0} +
\underbrace{\operatorname{Cov}(\eta_i, X_{i1})}_{=0} \neq 0\\[2ex]
\Rightarrow\;E(\epsilon_i|X_i)&amp;\neq 0,
\end{align*}
\]</span> if <span class="math inline">\(\beta_2\neq 0.\)</span></p>
<p>Used covariance calculation rules:</p>
<ul>
<li><span class="math inline">\(\operatorname{Cov}(aX,Z)=a \operatorname{Cov}(X,Z)\)</span></li>
<li><span class="math inline">\(\operatorname{Cov}(X+Y,Z)=\operatorname{Cov}(Y,Z) + \operatorname{Cov}(X,Z)\)</span></li>
</ul>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Other scientific fields such as, for instance, physics, usually assume <strong>independence</strong> between the error term and the predictors: <span class="math display">\[
\epsilon_i\perp\!\!\!\!\perp X_{ik}\quad\text{for all}\quad k=0,\dots,q,
\]</span> which is justified, if the error term only captures purely random stuff like, for instance, measurement errors of some measurement device.</p>
<p>In this case, the assumption of exogeneity (Assumption 2) is trivially fulfilled, since by the independence between <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_i=(X_{i0},\dots,X_{ip})^{\top}\)</span> we have that <span class="math display">\[
E(\epsilon_i|X_i) = E(\epsilon_i) = 0
\]</span></p>
<p>Note: The assumption <span class="math inline">\(E(\epsilon_i)=0\)</span> is not critical (i.e.&nbsp;not restrictive) due to the intercept term <span class="math inline">\(X_{i0}=1\)</span> in <a href="#eq-assExogen" class="quarto-xref">Equation&nbsp;<span>2.2</span></a>.</p>
</div>
</div>
</div>
<!-- 
::: {.callout icon="false"}

# Example: Heteroskedastic Error

Let

* $\epsilon_i\sim\mathcal{N}(0,\sigma_i^2),$ where 
* $\sigma_i = |X_{i1}|$ 

Here the assumption of exogeneity (Assumption 2) is fulfilled since realizations of $X_i$ do not affect the mean of $\epsilon_i,$ thus
$$
E(\epsilon_i|X_i) = 0. 
$$

However, $\epsilon_i$ and $X_i$ are not independent of each other, since the conditional variance of $\epsilon_i$ is a function of $X_{i1}$
$$
Var(\epsilon_i|X_i) = |X_{i1}|^2.
$$
::: 
-->
</section><section id="assumption-4-error-variance-1" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="assumption-4-error-variance-1"><strong>Assumption 4: Error Variance</strong></h4>
<p>Here are two specific examples of a heteroskedastic and a homoskedastic error, which you can use in computer simulations.</p>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Heteroskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(X_i\in\mathbb{R}^2\)</span> with <span class="math inline">\(X_{i0}=1\)</span> and <span class="math display">\[
X_{i2}\sim \mathcal{U}[-4,4],\quad i=1,\dots,n,
\]</span> and let <span class="math display">\[
\epsilon_i|X_i\sim \mathcal{U}[-0.5|X_{i2}|, 0.5|X_{i2}|],\quad i=1,\dots,n
\]</span> where <span class="math inline">\(\mathcal{U}[a,b]\)</span> denotes the uniform distribution over <span class="math inline">\([a,b].\)</span></p>
<p>This error term fulfills the assumption of exogeneity (Assumption 2) since <span class="math display">\[
E(\epsilon_i|X_i)=\underbrace{\frac{1}{2}(- 0.5|X_{i2}| + 0.5|X_{i2}|)}_{=\frac{1}{2}(a+b)}=0
\]</span> for every possible realization of <span class="math inline">\(X_i.\)</span></p>
<p>However, the error term <span class="math inline">\(\epsilon_i\)</span> is heteroskedastic, since its conditional variance is a function of the realizations of <span class="math inline">\(X_{i2}\colon\)</span> <span class="math display">\[
Var(\epsilon_i|X_i)=\underbrace{\frac{1}{12}(0.5|X_{i2}| - (-0.5|X_{i2}|))^2}_{=\frac{1}{12}(b-a)^2}=\frac{1}{12}X_{i2}^2
\]</span></p>
<p><code>R</code>-code:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">n</span>   <span class="op">&lt;-</span> <span class="fl">100</span> <span class="co"># sample size</span></span>
<span><span class="va">X_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, min <span class="op">=</span> <span class="op">-</span><span class="fl">4</span>,              </span>
<span>                    max <span class="op">=</span>  <span class="fl">4</span><span class="op">)</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, min <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">X_2</span><span class="op">)</span>, </span>
<span>                    max <span class="op">=</span>  <span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">X_2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>  y    <span class="op">=</span> <span class="va">eps</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">X_2</span><span class="op">)</span><span class="op">]</span>,</span>
<span>  x    <span class="op">=</span> <span class="va">X_2</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">X_2</span><span class="op">)</span><span class="op">]</span>,</span>
<span>  xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>,</span>
<span>  ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">epsilon</span><span class="op">)</span>,</span>
<span>  main <span class="op">=</span> <span class="st">"Scatter-Plot: Error Terms agains Predictor"</span> </span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Homoskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(X_i\in\mathbb{R}^2\)</span> with <span class="math inline">\(X_{i0}=1\)</span> and <span class="math display">\[
X_{i2}\sim \mathcal{U}[-4,4],\quad i=1,\dots,n,
\]</span> and let <span class="math display">\[
\epsilon_i|X_i\sim \mathcal{U}[-1, 1],\quad i=1,\dots,n
\]</span> where <span class="math inline">\(\mathcal{U}[a,b]\)</span> denotes the uniform distribution over <span class="math inline">\([a,b].\)</span></p>
<p>This error term fulfills the assumption of exogeneity (Assumption 2) since <span class="math display">\[
E(\epsilon_i|X_i)=\underbrace{\frac{1}{2}(-1 + 1)}_{=\frac{1}{2}(a+b)}=0
\]</span> for every possible realization of <span class="math inline">\(X_i.\)</span></p>
<p>Moreover, the error term <span class="math inline">\(\epsilon_i\)</span> is also homoskedastic, since its conditional variance is not a function of <span class="math inline">\(X_{i2}\colon\)</span> <span class="math display">\[
Var(\epsilon_i|X_i)=\underbrace{\frac{1}{12}(1 - (-1))^2}_{=\frac{1}{12}(b-a)^2}=\frac{1}{12}2^2
\]</span></p>
<p><code>R</code>-code:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">n</span>   <span class="op">&lt;-</span> <span class="fl">100</span> <span class="co"># sample size</span></span>
<span><span class="va">X_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, min <span class="op">=</span> <span class="op">-</span><span class="fl">4</span>,              </span>
<span>                    max <span class="op">=</span>  <span class="fl">4</span><span class="op">)</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, min <span class="op">=</span> <span class="op">-</span><span class="fl">1</span>, </span>
<span>                    max <span class="op">=</span>  <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>  y    <span class="op">=</span> <span class="va">eps</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">X_2</span><span class="op">)</span><span class="op">]</span>,</span>
<span>  x    <span class="op">=</span> <span class="va">X_2</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">X_2</span><span class="op">)</span><span class="op">]</span>,</span>
<span>  xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>,</span>
<span>  ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">epsilon</span><span class="op">)</span>,</span>
<span>  main <span class="op">=</span> <span class="st">"Scatter-Plot: Error Terms agains Predictor"</span> </span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section></section></section><section id="deriving-the-expression-of-the-ols-estimator" class="level2" data-number="2.2"><h2 data-number="2.2" class="anchored" data-anchor-id="deriving-the-expression-of-the-ols-estimator">
<span class="header-section-number">2.2</span> Deriving the Expression of the OLS Estimator</h2>
<p>See Section <a href="Ch3_MatrixAlgebra.html" class="quarto-xref"><span>Chapter 3</span></a> for a refresher on matrix algebra.</p>
<p>We derive the expression for the OLS estimator <span class="math display">\[
\hat\beta=(\hat\beta_0,\dots,\hat\beta_p)^{\top}\in\mathbb{R}^{p+1}
\]</span> as the vector-valued minimizing argument of the sum of squared residuals, <span class="math display">\[
\operatorname{RSS}(b)=\sum_{i=1}^n\big(\underbrace{Y_i-X_i^{\top}b}_{\text{$i$th residual}}\big)^2
\]</span> with <span class="math inline">\(b\in\mathbb{R}^{(p+1)}.\)</span></p>
<p>Using matrix/vector notation we can write <span class="math inline">\(\operatorname{RSS}(b)\)</span> as <span class="math display">\[
\begin{align*}
\operatorname{RSS}(b)
&amp;=\sum_{i=1}^n(Y_i-X_i^{\top}b)^2\\[2ex]
&amp;=(Y-X b)^{\prime}(Y-X b)\\[2ex]
&amp;=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}
\]</span> To find the minimizing argument <span class="math display">\[
\hat\beta = \arg\min_{b\in\mathbb{R}^{p+1}}\operatorname{RSS}(b)
\]</span> we compute the vector containing all partial derivatives <span class="math display">\[
\begin{align*}
\underset{((p+1)\times 1)}{\frac{\partial \operatorname{RSS}(b)}{\partial b}} &amp;=-2\left(X^{\prime}Y -X^{\prime} Xb\right).
\end{align*}
\]</span> Setting each partial derivative to zero leads to <span class="math inline">\((p+1)\)</span> linear equations (<strong>normal equations</strong>) in <span class="math inline">\((p+1)\)</span> unknowns. This linear system of equations defines the OLS estimates, <span class="math inline">\(\hat{\beta}\)</span>, for a given dataset: <span class="math display">\[
\begin{align*}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)
&amp;=\underset{((p+1)\times 1)}{0}\\[2ex]
X^{\prime} X\hat{\beta}
&amp;=\underset{((p+1)\times 1)}{X^{\prime}Y}.
\end{align*}
\]</span> From our full rank assumption (Assumption 3) it follows that <span class="math inline">\(X^{\prime}X\)</span> is invertible which allows us to solve the equation system by <span class="math display">\[
\begin{align*}
\underset{((p+1)\times 1)}{\hat{\beta}} &amp;=\left(X^{\prime} X\right)^{-1} X^{\prime} Y.
\end{align*}
\]</span></p>
<p>The following codes computes the estimate <span class="math inline">\(\hat{\beta}\)</span> for a given dataset with <span class="math inline">\(X_i\in\mathbb{R}^{p+1}\)</span>, <span class="math inline">\(p=2\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Some given data</span></span>
<span><span class="va">X_1</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.9</span>,<span class="fl">0.8</span>,<span class="fl">1.1</span>,<span class="fl">0.1</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">4.4</span>,<span class="fl">4.6</span>,<span class="fl">1.6</span>,<span class="fl">5.5</span>,<span class="fl">3.4</span><span class="op">)</span></span>
<span><span class="va">X_2</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">66</span>, <span class="fl">62</span>, <span class="fl">64</span>, <span class="fl">61</span>, <span class="fl">63</span>, <span class="fl">70</span>, <span class="fl">68</span>, <span class="fl">62</span>, <span class="fl">68</span>, <span class="fl">66</span><span class="op">)</span></span>
<span><span class="va">Y</span>       <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.7</span>,<span class="op">-</span><span class="fl">1.0</span>,<span class="op">-</span><span class="fl">0.2</span>,<span class="op">-</span><span class="fl">1.2</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">3.4</span>,<span class="fl">0.0</span>,<span class="fl">0.8</span>,<span class="fl">3.7</span>,<span class="fl">2.0</span><span class="op">)</span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="st">"X_1"</span> <span class="op">=</span> <span class="va">X_1</span>, <span class="st">"X_2"</span> <span class="op">=</span> <span class="va">X_2</span>, <span class="st">"Y"</span> <span class="op">=</span> <span class="va">Y</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Compute the OLS estimation</span></span>
<span><span class="va">lmobj</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span>, data <span class="op">=</span> <span class="va">dataset</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Plot sample regression surface</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"scatterplot3d"</span><span class="op">)</span> <span class="co"># library for 3d plots</span></span>
<span><span class="va">plot3d</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/scatterplot3d/man/scatterplot3d.html">scatterplot3d</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">X_1</span>, y <span class="op">=</span> <span class="va">X_2</span>, z <span class="op">=</span> <span class="va">Y</span>,</span>
<span>            angle <span class="op">=</span> <span class="fl">33</span>, scale.y <span class="op">=</span> <span class="fl">0.8</span>, pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>            color <span class="op">=</span><span class="st">"red"</span>, </span>
<span>            xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,</span>
<span>            ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>,</span>
<span>            main <span class="op">=</span><span class="st">"OLS Regression Surface"</span><span class="op">)</span></span>
<span><span class="va">plot3d</span><span class="op">$</span><span class="fu">plane3d</span><span class="op">(</span><span class="va">lmobj</span>, lty.box <span class="op">=</span> <span class="st">"solid"</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.5</span><span class="op">)</span>, draw_polygon<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<!-- 
#### Special Case: Simple Linear Regression Model {-}

Let's consider the simple linear regression model $(p=1)$
$$
Y_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i,\qquad i=1,\dots,n.
$$
For this case, the expression of the OLS-estimators simplifies. For a given observed realization of the training data random sample 
$$
(x_1,y_1),\dots,(x_n,y_n)
$$
the values of the OLS estimator
$$
\hat\beta = 
\begin{pmatrix}
\hat\beta_0\\
\hat\beta_1
\end{pmatrix}
$$
are given by 
$$
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
$$
and
$$
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
$$
where 
$\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i$ 
and 
$\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$. 

![](images/Fig_3_1.png) 
-->
<!-- 
![](images/Fig_3_2.png) 
-->
</section><section id="fitted-values-residuals-and-projection-matrices" class="level2" data-number="2.3"><h2 data-number="2.3" class="anchored" data-anchor-id="fitted-values-residuals-and-projection-matrices">
<span class="header-section-number">2.3</span> Fitted Values, Residuals, and Projection Matrices</h2>
<section id="predictedfitted-values-and-residuals" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="predictedfitted-values-and-residuals"><strong>Predicted/Fitted Values and Residuals</strong></h4>
<ul>
<li><p>The (OLS) <strong>Predicted/Fitted Values:</strong> <span class="math display">\[
\hat{Y}_i=X_i^{\top}\hat\beta, \quad i=1,\dots,n
\]</span> The <span class="math inline">\((n\times 1)\)</span> vector of predicted (fitted) values <span class="math display">\[
\begin{align*}
\hat{Y} = \left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)
&amp;=X\hat{\beta}\\[-2ex]
&amp;=\underbrace{X(X^{\top}X)^{-1}X^{\top}}_{=P_X}Y\\[2ex]
&amp;=P_X Y
\end{align*}
\]</span></p></li>
<li><p>The (OLS) <strong>Residuals:</strong> <span class="math display">\[
e_i=Y_i-\hat{Y}_i, \quad i=1,\dots,n
\]</span> The <span class="math inline">\((n\times 1)\)</span> vector of residuals <span class="math display">\[
\begin{align*}
e =
\left(\begin{matrix}e_1\\e_2\\ \vdots\\ e_n\end{matrix}\right)
&amp;=
\left(\begin{matrix}Y_1\\[.5ex]Y_2\\[.5ex] \vdots\\[.5ex] Y_n\end{matrix}\right)-
\left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)\\[2ex]
&amp;=Y - \hat{Y}\\[2ex]
%&amp;=Y - X\hat{\beta}\\[-2ex]
%&amp;=Y - \underbrace{X(X^{\top}X)^{-1}X^{\top}}_{=P_X}Y\\[2ex]
&amp;=Y - P_X Y\\[2ex]
&amp;=\underbrace{(I_n - P_X)}_{=M_X} Y\\[2ex]
&amp;=M_XY
\end{align*}
\]</span></p></li>
</ul></section><section id="projection-matrices" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="projection-matrices"><strong>Projection Matrices</strong></h4>
<p>The matrix <span class="math display">\[
P_X=X(X^{\top}X)^{-1}X^{\top}
\]</span> is the <span class="math inline">\((n\times n)\)</span> <strong>projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the column space spanned by the column vectors of <span class="math inline">\(X\)</span> and <span class="math display">\[
M_X=I_n-X(X^{\top}X)^{-1}X^{\top}=I_n-P_X
\]</span> is the associated <span class="math inline">\((n\times n)\)</span> <strong>orthogonal projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the vector space that is orthogonal to that spanned by the column vectors of <span class="math inline">\(X.\)</span></p>
</section></section><section id="assessing-the-accuracy-of-the-model-fit" class="level2" data-number="2.4"><h2 data-number="2.4" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-model-fit">
<span class="header-section-number">2.4</span> Assessing the Accuracy of the Model Fit</h2>
<p>The larger the proportion of the explained variance, the better is the fit of the estimated model <span class="math inline">\(\hat{Y}_i=\hat{f}(X_i)\)</span> to the data <span class="math inline">\((Y_i,X_i^{\top})\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<p>This motivates the definition of the so-called <span class="math inline">\(R^2\)</span> coefficient of determination: <span class="math display">\[
\begin{eqnarray*}
R^2
%&amp;=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&amp;=1-\frac{\sum_{i=1}^ne_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&amp;=1-\frac{\operatorname{RSS}}{\operatorname{TSS}}
\end{eqnarray*}
\]</span> with <span class="math display">\[
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat\beta)=\sum_{i=1}^n\left(Y_i-X_i^{\top}\hat\beta\right)^2=\sum_{i=1}^ne_i^2
\end{align*}
\]</span> and <span class="math display">\[
\begin{align*}
\operatorname{TSS}=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2.
\end{align*}
\]</span></p>
<p><span class="math inline">\(\operatorname{TSS}\)</span> “Total Sum of Squares”</p>
<p><span class="math inline">\(\operatorname{RSS}\)</span> “Residual Sum of Squares”</p>
<ul>
<li><p>Obviously, we have that <span class="math inline">\(0\leq R^2\leq 1\)</span>.</p></li>
<li><p>The closer <span class="math inline">\(R^2\)</span> lies to <span class="math inline">\(1\)</span>, the better is the fit of the model to the observed training data.</p></li>
</ul>
<p>In tendency an accurate model has …</p>
<ul>
<li><p>a low residual standard error <span class="math inline">\(\operatorname{RSE}\)</span> <span class="math display">\[
\operatorname{RSE}=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}
\]</span></p></li>
<li><p>a high <span class="math inline">\(R^2\)</span></p></li>
</ul>
<p><span class="math display">\[
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
\]</span> where <span class="math inline">\(0\leq R^2\leq 1.\)</span></p>
<!-- 
::: {.callout-caution}

**Cautionary Note Nr 1:** 

Do not forget that there is a **irreducible error** $Var(\epsilon)=\sigma^2>0$. Thus 

* very low $\operatorname{RSE}$ values $(\operatorname{RSE}\approx 0)$ and 
* very high $R^2$ values $(R^2\approx 1)$ 

can be warning signals indicating overfitting. While overfitting typically does not happen with a simple linear regression model, it can happen with multiple linear regression models containing a lot of parameters (large $p$).


**Cautionary Note Nr 2:** 

The $R^2$ and $\operatorname{RSE}$ are only based on **training data**. In @sec-SL, we have seen that a proper assessment of the model accuracy needs to take into account **test data**.
:::
 -->
<section id="r2-and-correlation-coefficient" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="r2-and-correlation-coefficient"><strong><span class="math inline">\(R^2\)</span> and Correlation Coefficient</strong></h4>
<p>In the case of the <strong>simple linear regression model</strong> <span class="math display">\[
Y_i=\beta_0 + \beta_1 X_{i1} + \epsilon_i,\quad i=1,\dots,n
\]</span> the <span class="math inline">\(R^2\)</span> equals the squared sample correlation coefficient between <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_{i1}\)</span>, <span class="math display">\[
R^2 = r_{Y,X_1}^2,
\]</span> where <span class="math display">\[
r_{Y,X_1}=\frac{\sum_{i=1}^n(X_{i1}-\bar{X}_1)(Y_i-\bar{Y})}{\sqrt{\sum_{i=1}^n(X_{i1}-\bar{X}_1)^2}\sqrt{\sum_{i=1}^n(Y_i-\bar{Y})^2}},
\]</span> where <span class="math inline">\(\bar{X}_1=n^{-1}\sum_{i=1}^nX_{i1}.\)</span></p>
<p>In case of the <strong>multiple linear regression model</strong> <span class="math display">\[
Y_i=\beta_0+\sum_{k=1}^p\beta_k X_{ik}+\epsilon_i,\quad i=1,\dots,n
\]</span> the <span class="math inline">\(R^2\)</span> equals the squared correlation between <span class="math inline">\(Y_i\)</span> and the fitted values <span class="math inline">\(\hat{Y}_i\)</span>: <span class="math display">\[
R^2=r^2_{Y,\hat{Y}}
\]</span> with <span class="math display">\[
r_{Y,\hat{Y}}=\frac{\sum_{i=1}^n(Y_i-\bar{Y})(\hat{Y}_i-\bar{\hat{Y}})}{\sqrt{\sum_{i=1}^n(Y_i-\bar{Y})^2}\sqrt{\sum_{i=1}^n(\hat{Y}_i-\bar{\hat{Y}})^2}},
\]</span> where <span class="math inline">\(\bar{Y}=n^{-1}\sum_{i=1}^nY_{i}.\)</span></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>A high/low <span class="math inline">\(R^2\)</span> does not mean a validation/falsification of the estimated model. Any econometric/statistical model needs a plausible explanation from relevant (economic) theory.</p>
</div>
</div>
<p>The most often criticized disadvantage of the <span class="math inline">\(R^2\)</span> is that additional regressors (relevant or not) will increase the <span class="math inline">\(R^2\)</span>. The below <code>R</code>-code demonstrates this problem.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>     <span class="op">&lt;-</span> <span class="fl">100</span>                  <span class="co"># Sample size</span></span>
<span><span class="va">X</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>      <span class="co"># Relevant X variable</span></span>
<span><span class="va">X_ir</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">5</span>, <span class="fl">20</span><span class="op">)</span>      <span class="co"># Irrelevant X variable</span></span>
<span><span class="va">error</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">*</span><span class="fl">10</span>    <span class="co"># True (usually unknown) error</span></span>
<span><span class="va">Y</span>     <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">error</span>    <span class="co"># Y variable</span></span>
<span><span class="va">lm1</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>     <span class="co"># Correct OLS regression </span></span>
<span><span class="va">lm2</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">+</span><span class="va">X_ir</span><span class="op">)</span><span class="op">)</span><span class="co"># OLS regression with X_ir </span></span>
<span><span class="va">lm1</span><span class="op">$</span><span class="va">r.squared</span> <span class="op">&lt;</span> <span class="va">lm2</span><span class="op">$</span><span class="va">r.squared</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>So, <span class="math inline">\(R^2\)</span> increases here even though <code>X_ir</code> is a completely irrelevant explanatory variable.</p>
<p>Because of this, the <span class="math inline">\(R^2\)</span> cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called <strong>adjusted</strong> <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\overline{R}^2,\)</span> defined as <span class="math display">\[
\begin{eqnarray*}
  \overline{R}^2&amp;=&amp;1-\frac{\frac{1}{n-(p+1)}\sum_{i=1}^ne^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}\leq R^2%\\
\end{eqnarray*}
\]</span></p>
<p>The adjustment is in terms of the degrees of freedom <span class="math inline">\(n-(p+1)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lm1</span><span class="op">$</span><span class="va">adj.r.squared</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># model without X_ir</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.569</code></pre>
</div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lm2</span><span class="op">$</span><span class="va">adj.r.squared</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># model with X_ir</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.564</code></pre>
</div>
</div>
</section></section><section id="assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" class="level2" data-number="2.5"><h2 data-number="2.5" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-coefficient-estimators-hatbeta">
<span class="header-section-number">2.5</span> Assessing the Accuracy of the Coefficient Estimators <span class="math inline">\(\hat{\beta}\)</span>
</h2>
<section id="bias-of-hatbeta" class="level3" data-number="2.5.1"><h3 data-number="2.5.1" class="anchored" data-anchor-id="bias-of-hatbeta">
<span class="header-section-number">2.5.1</span> Bias of <span class="math inline">\(\hat{\beta}\)</span>
</h3>
<p>Under the Assumptions 1-4, one can show that the OLS estimator <span class="math display">\[
\hat\beta = (X^{\top}X)^{-1}X^{\top}Y
\]</span> is unbiased, i.e. <span id="eq-OLSUnbiased"><span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta)
&amp; = E(\hat\beta) - \beta = \underset{(p+1)\times 1}{0}.
\end{align*}
\qquad(2.3)\]</span></span> That is, in the mean of <span class="math inline">\(\hat\beta\)</span> equals <span class="math inline">\(\beta.\)</span></p>
<section id="showing-unbiasedness-of-hatbeta" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="showing-unbiasedness-of-hatbeta">Showing Unbiasedness of <span class="math inline">\(\hat{\beta}\)</span>
</h4>
<p><a href="#eq-OLSUnbiased" class="quarto-xref">Equation&nbsp;<span>2.3</span></a> can be shown as following:</p>
<p>Observe that <span class="math display">\[
\hat\beta=(X^{\top}X)^{-1}X^{\top}Y
\]</span> consists of two multivariate random variables; namely</p>
<ul>
<li>
<span class="math inline">\(X\in\mathbb{R}^{n\times(p+1)}\)</span> and</li>
<li><span class="math inline">\(Y\in\mathbb{R}^n.\)</span></li>
</ul>
<p>Thus we firstly need to derive the <strong>conditional mean</strong> <span class="math inline">\(E(\hat\beta|X),\)</span> which effectively allows us to focus on randomness due to <span class="math inline">\(\epsilon,\)</span><br><span class="math display">\[
\begin{align*}
E(\hat\beta|X)
&amp;= E((X^{\top}X)^{-1}X^{\top}Y|X) \\[2ex]
&amp;\text{Using that (by Assumption 1 Part (a)) $Y=X\beta + \epsilon$:}\\[2ex]
&amp;= E((X^{\top}X)^{-1}X^{\top}\underbrace{Y}_{=X\beta+\epsilon}|X) \\[2ex]
&amp;= E((X^{\top}X)^{-1}X^{\top}(X\beta+\epsilon)|X)                 \\[2ex]
&amp;= E(\underbrace{(X^{\top}X)^{-1}X^{\top}X}_{=I_{(p+1)}}\beta|X) + E((X^{\top}X)^{-1}X^{\top}\epsilon|X)\\[2ex]
&amp;= \underbrace{E(\beta|X)}_{=\beta} + \underbrace{E((X^{\top}X)^{-1}X^{\top}\epsilon|X)}_{=(X^{\top}X)^{-1}X^{\top}E(\epsilon|X)}  \\[2ex]
&amp;= \beta + (X^{\top}X)^{-1}X^{\top}\underbrace{E(\epsilon|X)}_{=0}\\[2ex]
&amp;= \underset{(p+1)\times 1}{\beta}  
\end{align*}
\]</span> Thus <span class="math inline">\(\hat\beta\)</span> is <strong>conditionally unbiased:</strong> <span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta|X)
&amp; = E(\hat\beta|X) - \beta\\[2ex]
&amp; = \beta - \beta \\[2ex]
&amp; = \underset{(p+1)\times 1}{0}   
\end{align*}
\]</span></p>
<p>From this if follows, by the iterated law of expectations, that the OLS estimator is also <strong>unconditionally unbiased:</strong><br><span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta)
&amp; = E(\operatorname{Bias}(\hat\beta|X))\\[2ex]
&amp; = E\left(E(\hat\beta|X) - \beta\right)\\[2ex]
&amp; = E\left(E(\hat\beta|X)\right) - E\left(\beta\right)\\[2ex]
&amp; = E\left(\beta\right) - E\left(\beta\right)\\[2ex]
&amp; = \beta - \beta = 0.
\end{align*}
\]</span></p>
</section></section><section id="standard-error-of-hatbeta_j" class="level3" data-number="2.5.2"><h3 data-number="2.5.2" class="anchored" data-anchor-id="standard-error-of-hatbeta_j">
<span class="header-section-number">2.5.2</span> Standard Error of <span class="math inline">\(\hat{\beta}_j\)</span>
</h3>
<p>The standard error of <span class="math inline">\(\hat{\beta}_j,\)</span> for each <span class="math inline">\(j=0,\dots,p,\)</span> is given by <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X)=\sqrt{Var(\hat\beta_j|X)},
\]</span> where <span class="math display">\[
Var(\hat\beta_j|X) = \left[Var(\hat\beta|X)\right]_{(j,j)}
\]</span> denotes the <span class="math inline">\(j\)</span>th diagonal element of the symmetric <span class="math inline">\((p+1)\times (p+1)\)</span> variance-covariance matrix <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=\begin{pmatrix}
Var(\hat\beta_0|X)&amp;Cov(\hat\beta_0,\hat\beta_1|X)&amp;\cdots&amp;Cov(\hat\beta_0,\hat\beta_{p}|X)\\
Cov(\hat\beta_1,\hat\beta_0|X)&amp;Var(\hat\beta_1|X)&amp;  &amp;Cov(\hat\beta_1,\hat\beta_{p}|X)\\
\vdots &amp;&amp;\ddots&amp;\\
Cov(\hat\beta_p,\hat\beta_0|X)&amp;Cov(\hat\beta_p,\hat\beta_1|X)&amp;\cdots&amp;Var(\hat\beta_{p}|X)\\
\end{pmatrix}
\end{align*}
\]</span></p>
<p>To get a useful, explicit expression for the standard error, <span class="math inline">\(\operatorname{SE}(\hat\beta_j|X),\)</span> we need to first compute an explicit expression for the symmetric <span class="math inline">\((p+1)\times(p+1)\)</span> variance-covariance matrix <span class="math inline">\(Var(\hat\beta|X).\)</span></p>
<section id="deriving-the-expression-for-varhatbetaxcolon" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="deriving-the-expression-for-varhatbetaxcolon">Deriving the Expression for <span class="math inline">\(Var(\hat\beta|X)\colon\)</span>
</h4>
<p>Note that <span class="math display">\[
\begin{align*}
\hat{\beta}
&amp;=(X^{\top}X)^{-1}X^{\top}Y\\[2ex]
&amp;\text{Using that (by Assumption 1 Part (a)) $Y=X\beta + \epsilon$:}\\[2ex]
&amp;=(X^{\top}X)^{-1}X^{\top}\; \underbrace{(X\beta + \epsilon)}_{=Y}\\[2ex]
&amp;=\underbrace{(X^{\top}X)^{-1}X^{\top}X}_{=I_{(p+1)}}\;\beta + (X^{\top}X)^{-1}X^{\top}\epsilon\\
&amp;=\beta + (X^{\top}X)^{-1}X^{\top}\epsilon
\end{align*}
\]</span> This leads to the so-called <strong>sampling error expression</strong> <span class="math display">\[
\hat{\beta} - \beta = (X^{\top}X)^{-1}X^{\top}\epsilon.
\]</span> With this, we can derive the general explicit expression for <span class="math inline">\(Var(\hat\beta|X).\)</span> <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;\text{Adding/subtracting constants does not change variance:}\\[2ex]
&amp;=Var(\hat\beta - \beta|X)\\[2ex]
&amp;\text{Using the sampling error expression $\hat{\beta} - \beta = (X^{\top}X)^{-1}X^{\top}\epsilon$:}\\[2ex]
&amp;=Var((X^{\top}X)^{-1}X^{\top}\epsilon|X)\\[2ex]
&amp;\text{Using that $Var(Z|X)=E[(Z-E(Z|X))(Z-E(Z|X))|X]$:}\\[2ex]
&amp;=E\Big[\big((X^{\top}X)^{-1}X^{\top}\epsilon-\underbrace{E((X^{\top}X)^{-1}X^{\top}\epsilon|X)}_{=0}\big)\times\\[2ex]
&amp;\phantom{=\Big(}\,\times\big((X^{\top}X)^{-1}X^{\top}\epsilon-\underbrace{E((X^{\top}X)^{-1}X^{\top}\epsilon|X)}_{=0}\big)^{\top}|X\Big]\\[2ex]
&amp;=E\left[((X^{\top}X)^{-1}X^{\top}\epsilon)((X^{\top}X)^{-1}X^{\top}\epsilon)^{\top}|X\right]\\[2ex]
&amp;=E\left[(X^{\top}X)^{-1}X^{\top}\epsilon\epsilon^{\top} X(X^{\top}X)^{-1}|X\right]\\[2ex]
&amp;=\;\;\;(X^{\top}X)^{-1}X^{\top}\underbrace{E\left(\epsilon\epsilon^{\top}|X\right)}_{=\highlight{Var(\epsilon|X)}}X(X^{\top}X)^{-1}
\end{align*}
\]</span> That is, the explicit expression for <span class="math inline">\(Var(\hat\beta|X)\)</span> depends on the explicit form of the symmetric <span class="math inline">\((n\times n)\)</span> matrix <span class="math inline">\(Var(\epsilon|X)\)</span> <span class="math display">\[
\begin{align*}
&amp;\highlight{Var(\epsilon|X)}=\\[2ex]
&amp;=\begin{pmatrix}
Var(\epsilon_1|X)&amp;Cov(\epsilon_1,\epsilon_2|X)&amp;\cdots&amp;Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&amp;Var(\epsilon_2|X)&amp;  &amp;Cov(\epsilon_2,\epsilon_n|X)\\
\vdots &amp;&amp;\ddots&amp;\\
Cov(\epsilon_n,\epsilon_1|X)&amp;Cov(\epsilon_n,\epsilon_2|X)&amp;\cdots&amp;Var(\epsilon_n|X)\\
\end{pmatrix}
\end{align*}
\]</span></p>
<p>The explicit form of the symmetric <span class="math inline">\((n\times n)\)</span> matrix <span class="math inline">\(Var(\epsilon|X)\)</span> depends on our (hopefully correct) assumption on the error-term variance (Assumption 4).</p>
<p>We consider the two most prominent types of assumptions for the symmetric <span class="math inline">\((n\times n)\)</span> matrix <span class="math inline">\(Var(\epsilon|X)\)</span>:</p>
<ul>
<li>Homoskedastic (and uncorrelated) errors</li>
<li>Heteroskedastic (and uncorrelated) errors</li>
</ul></section><section id="homoskedastic-and-uncorrelated-errors" class="level4" data-number="2.5.2.1"><h4 data-number="2.5.2.1" class="anchored" data-anchor-id="homoskedastic-and-uncorrelated-errors">
<span class="header-section-number">2.5.2.1</span> Homoskedastic and Uncorrelated Errors</h4>
<p>If <span class="math display">\[
\begin{align*}
Var(\epsilon|X)
&amp;=
\begin{pmatrix}
\sigma^2 &amp; 0        &amp; \cdots &amp; 0\\
0        &amp; \sigma^2 &amp; \cdots &amp; 0\\
\vdots   &amp; \vdots   &amp; \ddots &amp; 0\\
0        &amp; 0        &amp; \cdots &amp; \sigma^2\\
\end{pmatrix}
=\sigma^2 I_n,
\end{align*}
\]</span> then <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=(X^{\top}X)^{-1}X^{\top} \;\highlight{Var(\epsilon|X)}\; X(X^{\top}X)^{-1}\\[2ex]
&amp;=(X^{\top}X)^{-1}X^{\top} \;\highlight{\sigma^2 I_n}\; X(X^{\top}X)^{-1}\\[2ex]
&amp;=\sigma^2\;(X^{\top}X)^{-1}X^{\top} \left( I_n \right) X(X^{\top}X)^{-1}\\[2ex]
&amp;=\sigma^2\;\underbrace{(X^{\top}X)^{-1}X^{\top}X}_{I_{p+1}}\;(X^{\top}X)^{-1}\\[2ex]
&amp;=\sigma^2\;(X^{\top}X)^{-1},
\end{align*}
\]</span> where the only unknown component is <span class="math inline">\(\sigma^2=Var(\epsilon_i).\)</span></p>
<p>We can estimate the homoskedastik error term variance <span class="math inline">\(\sigma^2\)</span> using the <strong>R</strong>esidual <strong>S</strong>tandard <strong>E</strong>rror: <span class="math display">\[
\begin{align*}
\hat\sigma = \operatorname{RSE}
&amp;=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}\\[2ex]
&amp;=\sqrt{ \frac{1}{n-(p+1)} \sum_{i=1}^n e_i^2}.
\end{align*}
\]</span></p>
<p><strong>Summing up:</strong></p>
<p>In the case of homoskedastic (and uncorrelated) error terms the standard error of <span class="math inline">\(\hat\beta_j\)</span> is <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X) = \sqrt{\left[\sigma^2 \left(X^{\top}X\right)^{-1}\right]_{(j,j)}}.
\]</span> Typically, <span class="math inline">\(\sigma^2\)</span> is unknown, and has to be estimated using, for instance, the empirical residual standard error <span class="math display">\[
\widehat{\operatorname{SE}}(\hat\beta_j|X) = \sqrt{\left[\hat{\sigma}^2 \left(X^{\top}X\right)^{-1}\right]_{(j,j)}}.
\]</span></p>
<p>This is the default version for computing the standard error in statistical software packages such as <code>R</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>      <span class="op">&lt;-</span> <span class="fl">100</span>                           <span class="co"># Sample size</span></span>
<span><span class="va">X_1</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>               <span class="co"># Predictor variable X_1</span></span>
<span><span class="va">X_2</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span>               <span class="co"># Predictor variable X_2</span></span>
<span><span class="va">error</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">*</span><span class="fl">10</span>             <span class="co"># True (usually unknown) error</span></span>
<span><span class="va">Y</span>      <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X_1</span> <span class="op">-</span><span class="fl">5</span> <span class="op">*</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">error</span>  <span class="co"># Y variable</span></span>
<span><span class="va">lm_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span><span class="op">)</span>             <span class="co"># OLS regression </span></span>
<span></span>
<span><span class="co">## Standard OLS output table:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span>                         </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Y ~ X_1 + X_2)

Residuals:
    Min      1Q  Median      3Q     Max 
-39.071  -7.138  -0.575   9.570  33.368 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.0112     4.2440   0.238    0.812    
X_1           5.1954     0.4529  11.472  &lt; 2e-16 ***
X_2          -4.7001     0.6690  -7.026 2.95e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 12.84 on 97 degrees of freedom
Multiple R-squared:  0.6565,    Adjusted R-squared:  0.6494 
F-statistic: 92.68 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section><section id="heteroskedastic-and-uncorrelated-errors" class="level4" data-number="2.5.2.2"><h4 data-number="2.5.2.2" class="anchored" data-anchor-id="heteroskedastic-and-uncorrelated-errors">
<span class="header-section-number">2.5.2.2</span> Heteroskedastic and Uncorrelated Errors</h4>
<p>If <span class="math display">\[
\begin{align*}
Var(\epsilon|X)
&amp;=
\begin{pmatrix}
\sigma_1^2 &amp; 0          &amp; \cdots &amp; 0\\
0          &amp; \sigma_2^2 &amp; \cdots &amp; 0\\
\vdots     &amp; \vdots     &amp; \ddots &amp; 0\\
0          &amp; 0          &amp; \cdots &amp; \sigma_n^2\\
\end{pmatrix}
=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2),
\end{align*}
\]</span> then <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=(X^{\top}X)^{-1}X^{\top} \;\highlight{Var(\epsilon|X)}\; X(X^{\top}X)^{-1}\\[2ex]
&amp;=(X^{\top}X)^{-1}X^{\top} \;\highlight{\operatorname{diag}(\sigma_1^2,\dots,\sigma_n^2)}\; X(X^{\top}X)^{-1}\\[2ex]
&amp;=(X^{\top}X)^{-1} \left(\sum_{i=1}^n \sigma_i^2 X_i X_i^{\top}\right) (X^{\top}X)^{-1}\\[2ex]
\end{align*}
\]</span> Thus, the symmetric <span class="math inline">\((p+1)\times(p+1)\)</span> variance-covariance matrix <span class="math inline">\(Var(\hat\beta|X)\)</span> keeps its <strong>sandwich form</strong>, where the inner part of the sandwich <span class="math display">\[
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i^{\top}\right)
\]</span> is typically unknown, since <span class="math inline">\(\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2\)</span> are typically unknown.</p>
<p>There are different, so-called <strong>Heteroskedasticity Consistent (HC)</strong> estimators to estimate the unknown expression <span class="math display">\[
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i^{\top}\right).
\]</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 66%">
</colgroup>
<thead><tr class="header">
<th>HC-Type</th>
<th>Formular</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>HC0</td>
<td><span class="math inline">\(\sum_{i=1}^ne_i^2X_iX_i^{\top}\)</span></td>
</tr>
<tr class="even">
<td>HC1</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{n}{n-(p+1)}e_i^2X_iX_i^{\top}\)</span></td>
</tr>
<tr class="odd">
<td>HC2</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{e_{i}^{2}}{1-h_{i}}X_iX_i^{\top}\)</span></td>
</tr>
<tr class="even">
<td>HC3</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{e_{i}^{2}}{\left(1-h_{i}\right)^{2}}X_iX_i^{\top}\)</span></td>
</tr>
<tr class="odd">
<td>HC4</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{e_{i}^{2}}{\left(1-h_{i}\right)^{\delta_{i}}}X_iX_i^{\top}\)</span></td>
</tr>
</tbody>
</table>
<p>HC3 is the most often used HC-estimator.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The statistic <span class="math inline">\(h_i\)</span> is simply the <span class="math inline">\(i\)</span>th diagonal element of the projection matrix <span class="math inline">\(P_X\)</span> <span class="math display">\[
h_i=[P_X]_{ii}
\]</span> and is called the <span class="math inline">\(i\)</span>th <strong>leverage statistic</strong>, where</p>
<ul>
<li>
<span class="math inline">\(1/n\leq h_i\leq 1\)</span> and</li>
<li>
<span class="math inline">\(\bar{h}=n^{-1}\sum_{i=1}^nh_i=(p+1)/n\)</span>.</li>
</ul>
<p>Observations <span class="math inline">\(X_i\)</span> with leverage statistics <span class="math inline">\(h_i\)</span> that greatly exceed the average leverage value <span class="math inline">\((p+1)/n\)</span> are referred to as “high leverage” observations. High leverage observations <span class="math inline">\(X_i\)</span> are observations that are far away from the predictor values of all other subjects.</p>
<p>High leverage observations <span class="math inline">\(X_i\)</span> have the potential to distort the estimation results, <span class="math inline">\(\hat\beta_n\)</span>. Indeed, a high leverage observation <span class="math inline">\(X_i\)</span> will have an distorting effect on the estimation results if the absolute value of the corresponding residual <span class="math inline">\(|e_i|\)</span> is unusually large—such observations are called <strong>influential outliers</strong>. Such observations increase the estimation uncertainty.</p>
<p>General idea of the HC2-HC4 estimators is to increase the estimated variance in order to account for the effects of influential outliers. The residuals <span class="math inline">\(e_i\)</span> belonging to <span class="math inline">\(X_i\)</span> values that have a large leverage <span class="math inline">\(h_i\)</span> receive a higher weight and thus increase the value of <span class="math inline">\(\widehat{E}(\epsilon^2_iX_iX_i^{\top}).\)</span> This strategy takes into account increased estimation uncertainties due to single influential outliers.</p>
</div>
</div>
<p>The estimator HC0 was suggested in the econometrics literature by <span class="citation" data-cites="White1980">White (<a href="#ref-White1980" role="doc-biblioref">1980</a>)</span> and is justified by asymptotic (<span class="math inline">\(n\to\infty\)</span>) arguments. The estimators HC1, HC2 and HC3 were suggested by <span class="citation" data-cites="MacKinnon_White_1985">MacKinnon and White (<a href="#ref-MacKinnon_White_1985" role="doc-biblioref">1985</a>)</span> to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, <span class="citation" data-cites="Long_Ervin_2000">Long and Ervin (<a href="#ref-Long_Ervin_2000" role="doc-biblioref">2000</a>)</span> concludes that HC3 provides the best overall performance in finite samples. <span class="citation" data-cites="Cribari_2004">Cribari-Neto (<a href="#ref-Cribari_2004" role="doc-biblioref">2004</a>)</span> suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large <span class="math inline">\(h_i\)</span> values).</p>
<p><strong>Summing up:</strong></p>
<p>In the case of heteroskedastic and uncorrelated error terms the standard error of <span class="math inline">\(\hat\beta_j\)</span> is <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X) = \sqrt{\left[(X^{\top}X)^{-1} \left(\sum_{i=1}^n \sigma_i^2 X_i X_i^{\top}\right) (X^{\top}X)^{-1}\right]_{(j,j)}}.
\]</span> Typically, the variance terms <span class="math inline">\(\sigma_i^2\)</span> are unknown, but the expression <span class="math inline">\(\left(\sum_{i=1}^n \sigma_i^2 X_i X_i^{\top}\right)\)</span> can be estimated using the <strong>HC</strong> estimators above, such that we can work with <span class="math display">\[
\widehat{\operatorname{SE}}(\hat\beta_j|X) = \sqrt{\left[(X^{\top}X)^{-1} \left(\textsf{HC}\right) (X^{\top}X)^{-1}\right]_{(j,j)}},
\]</span> where <span class="math inline">\(\textsf{HC}\)</span> is a placeholder for one the <strong>H</strong>eteroskedasticity <strong>C</strong>onsistent estimators HC1-HC4 given above.</p>
<p>The <code>sandwich</code>-package allows to compute these standard errors in <code>R</code></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>      <span class="op">&lt;-</span> <span class="fl">100</span>                           <span class="co"># Sample size</span></span>
<span><span class="va">X_1</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>               <span class="co"># Predictor variable X_1</span></span>
<span><span class="va">X_2</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span>               <span class="co"># Predictor variable X_2</span></span>
<span><span class="va">error</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">X_2</span><span class="op">)</span>     <span class="co"># True (usually unknown) heteroskedastic error</span></span>
<span><span class="va">Y</span>      <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X_1</span> <span class="op">-</span><span class="fl">5</span> <span class="op">*</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">error</span>  <span class="co"># Y variable</span></span>
<span></span>
<span></span>
<span><span class="co">## Package for computing robust variance estimations</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://sandwich.R-Forge.R-project.org/">"sandwich"</a></span><span class="op">)</span> <span class="co"># vcovHC(), </span></span>
<span></span>
<span><span class="co">## Package for producing an OLS output table (etc.)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"lmtest"</span><span class="op">)</span><span class="op">)</span> <span class="co"># coeftest</span></span>
<span></span>
<span><span class="co">## Estimate the linear regression model parameters</span></span>
<span><span class="va">lm_obj</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">vcovHC3_mat</span> <span class="op">&lt;-</span> <span class="fu">sandwich</span><span class="fu">::</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_obj</span>, type<span class="op">=</span><span class="st">"HC3"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">lmtest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest</a></span><span class="op">(</span><span class="va">lm_obj</span>, vcov <span class="op">=</span> <span class="va">vcovHC3_mat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
t test of coefficients:

            Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept)  3.16955    2.17658   1.4562   0.1486    
X_1          4.92390    0.22302  22.0784   &lt;2e-16 ***
X_2         -4.57382    0.34716 -13.1748   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Note: The HC3-Robust SE estimates are: </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">vcovHC3_mat</span><span class="op">)</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)         X_1         X_2 
    2.17658     0.22302     0.34716 </code></pre>
</div>
</div>
</section></section></section><section id="inference" class="level2" data-number="2.6"><h2 data-number="2.6" class="anchored" data-anchor-id="inference">
<span class="header-section-number">2.6</span> Inference</h2>
<section id="confidence-intervals-for-beta_j" class="level3" data-number="2.6.1"><h3 data-number="2.6.1" class="anchored" data-anchor-id="confidence-intervals-for-beta_j">
<span class="header-section-number">2.6.1</span> Confidence Intervals for <span class="math inline">\(\beta_j\)</span>
</h3>
<p>Given the estimate <span class="math inline">\(\hat\beta_j\)</span> and the estimate of the standard error <span class="math display">\[
\widehat{\operatorname{SE}}(\hat\beta_j|X),
\]</span> we can construct the <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> confidence interval for the true (unknown) <span class="math inline">\(\beta_j\)</span>-parameter: <span class="math display">\[
\begin{align*}
&amp;\operatorname{CI}^{1-\alpha}_{\beta_j}=\\[2ex]
=&amp;\left[\hat\beta_j - q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j|X),\;
\hat\beta_j + q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j|X)\right]\\[2ex]
&amp;\text{More compact notation:}\\[2ex]
=&amp;\left[\hat\beta_j\; {\color{red}\pm} \; q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j|X)\right],
\end{align*}
\]</span> where <span class="math display">\[
q^{t,n-(p+1)}_{1-\alpha/2}
\]</span> denotes the <span class="math inline">\((1-\alpha/2)\)</span>-quantile of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(\operatorname{df}=n-(p+1)\)</span> degrees of freedom, and where <span class="math inline">\(\alpha\)</span> denotes the <strong>significance level</strong> with typical choices:</p>
<ul>
<li><span class="math inline">\(\alpha = 0.05\)</span></li>
<li><span class="math inline">\(\alpha = 0.01\)</span></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The confidence interval <span class="math display">\[
\operatorname{CI}^{1-\alpha}_{\beta_j}
\]</span> is a <strong>random confidence interval</strong>.</p>
<p>For a given, observed realization of the random sample <span class="math display">\[
((Y_{1,obs},X_{10,obs},\dots,X_{1(p+1),obs}),\dots,(Y_{n},X_{n0,obs},\dots,X_{n(p+1),obs}))
\]</span> we can compute a given, <strong>obs</strong>erved realization of the confidence interval <span class="math display">\[
\operatorname{CI}^{1-\alpha}_{\beta_j,obs}=\left[\hat\beta_{j,obs}\; \pm \; q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j|X)_{obs}\right],
\]</span> where <span class="math display">\[
\hat\beta_{j,obs}=\left[(X_{obs}^{\top}X_{obs})^{-1}X_{obs}^{\top}Y_{obs}\right]_{j}
\]</span> and (homoskedastic case) <span class="math display">\[
\widehat{\operatorname{SE}}(\hat\beta_j|X)_{obs}=\sqrt{\left[\hat{\sigma}^2_{obs}(X_{obs}^{\top}X_{obs})^{-1}\right]}
\]</span></p>
<p>There is a <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> chance (in resamplings from the training data random sample) that the <strong>random confidence interval</strong> <span class="math display">\[
\operatorname{CI}^{1-\alpha}_{\beta_j}
\]</span> contains the true (fix) parameter value <span class="math inline">\(\beta_j.\)</span></p>
<p>To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:</p>
<ul>
<li><a href="https://rpsychologist.com/d3/ci/">Interactive visualization for interpreting confidence intervals</a></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Only the above frequentist point of view can be nicely interpreted.</p>
<p>A given, observed confidence interval <span class="math display">\[
\operatorname{CI}^{1-\alpha}_{\beta_j,obs},
\]</span> either contains the true parameter value or not and usually we do not know it, since we do not know the value of <span class="math inline">\(\beta_j.\)</span></p>
<p><img src="images/CI_meme.jpg" class="img-fluid"></p>
</div>
</div>
</section><section id="confidence-intervals-for-statistical-hypothesis-testing" class="level3" data-number="2.6.2"><h3 data-number="2.6.2" class="anchored" data-anchor-id="confidence-intervals-for-statistical-hypothesis-testing">
<span class="header-section-number">2.6.2</span> Confidence Intervals for Statistical Hypothesis Testing</h3>
<p>We can use the <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> confidence interval to do statistical hypothesis testing at the significance level <span class="math inline">\(0&lt;\alpha&lt;1.\)</span> Typical significance levels:</p>
<ul>
<li><span class="math inline">\(\alpha=0.05\)</span></li>
<li><span class="math inline">\(\alpha=0.01\)</span></li>
</ul>
<p>Let us consider the following null-hypothesis <span class="math inline">\((H_0)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_j\)</span> equals the <strong>null-hypothetical value</strong> <span class="math inline">\(\beta^{(H_0)}_{j}\)</span> versus the two-sided alternative hypothesis <span class="math inline">\((H_1)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_j\)</span> does <strong>not equal</strong> the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{j}:\)</span> <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_j=\beta^{(H_0)}_{j}\\
H_1:&amp;\;\beta_j\neq \beta^{(H_0)}_{j}
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Classic No-Effect Null-Hypothesis
</div>
</div>
<div class="callout-body-container callout-body">
<p>For the special case, where the null-hypothetical value equals zero <span class="math display">\[
\beta^{(H_0)}_{j}=0
\]</span> we test the classic <strong>no-effect null-hypothesis</strong>.</p>
</div>
</div>
<p><strong>Testing-Procedure:</strong></p>
<ul>
<li><p>If the observed (obs) realization of the confidence interval, <span class="math inline">\(\operatorname{CI}^{1-\alpha}_{\beta_j,obs},\)</span> <strong>contains</strong> the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{j},\)</span> i.e. <span class="math display">\[
\begin{align*}
\beta^{(H_0)}_{j}&amp;\in\operatorname{CI}^{1-\alpha}_{\beta_j,obs}\\[2ex]
\Leftrightarrow\beta^{(H_0)}_{j}&amp;\in
\left[
\hat{\beta}_{j,obs} \;\pm\; q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j)_{obs}
\right],
\end{align*}
\]</span> then we <strong>cannot reject the null hypothesis</strong> <span class="math inline">\(\beta_j=\beta^{(H_0)}_{j}.\)</span></p></li>
<li><p>If, however, the observed (obs) realization of the confidence interval, <span class="math inline">\(\operatorname{CI}^{1-\alpha}_{\beta_j,obs},\)</span> does <strong>not contain</strong> the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{j},\)</span> i.e. <span class="math display">\[
\begin{align*}
\beta^{(H_0)}_{j}&amp;\not\in\operatorname{CI}^{1-\alpha}_{\beta_j,obs}\\[2ex]
\Leftrightarrow\beta^{(H_0)}_{j}&amp;\not\in
\left[
\hat{\beta}_{j,obs} \;\pm\; q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j)_{obs}
\right],
\end{align*}
\]</span> then we <strong>reject the null hypothesis</strong> and adopt the alternative <span class="math inline">\(\beta_j\neq\beta^{(H_0)}_{j}.\)</span></p></li>
</ul></section><section id="t-test" class="level3" data-number="2.6.3"><h3 data-number="2.6.3" class="anchored" data-anchor-id="t-test">
<span class="header-section-number">2.6.3</span> <span class="math inline">\(t\)</span>-Test</h3>
<p>Standard errors can also be used to construct test statistics for statistical hypothesis testing. In the following, we look at the <span class="math inline">\(t\)</span>-test statistic.</p>
<p>Choose a significance level <span class="math inline">\(0&lt;\alpha&lt;1\)</span> such as, for instance,</p>
<ul>
<li><span class="math inline">\(\alpha=0.05\)</span></li>
<li><span class="math inline">\(\alpha=0.01\)</span></li>
</ul>
<p>Let us (again) consider the null-hypothesis <span class="math inline">\((H_0)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_j\)</span> equals the <strong>null-hypothetical value</strong> <span class="math inline">\(\beta^{(H_0)}_{j}\)</span> versus the two-sided alternative hypothesis <span class="math inline">\((H_1)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_j\)</span> does not equal the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{j}:\)</span> <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_j=\beta^{(H_0)}_{j}\\[2ex]
H_1:&amp;\;\beta_j\neq \beta^{(H_0)}_{j}
\end{align*}
\]</span></p>
<p>The <strong>random</strong> <span class="math inline">\(t\)</span>-test statistic is given by <span class="math display">\[
T=\frac{\hat\beta_j - \beta^{(H_0)}_{j}}{\widehat{\operatorname{SE}}(\hat\beta_j)}
\]</span></p>
<p>Under the null-hypothesis, <span class="math inline">\(\beta_j=\beta^{(H_0)}_{j},\)</span> the <span class="math inline">\(t\)</span>-test statistic is <span class="math inline">\(t\)</span>-distributed with <span class="math inline">\(\operatorname{df}=n-(p+1)\)</span> degrees of freedom. <span class="math display">\[
T=\frac{\hat\beta_j - \beta^{(H_0)}_{j}}{\widehat{\operatorname{SE}}(\hat\beta_j)}\overset{H_0}{\sim} t_{n-(p+1)},
\]</span> where <span class="math inline">\(t_{n-(p+1)}\)</span> denotes the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(\operatorname{df}=n-(p+1)\)</span> degrees of freedom.</p>
<p>For a given realization of the training data random sample <span class="math display">\[
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{np}))
\]</span> we <strong>obs</strong>serve a specific realization of the <span class="math inline">\(t\)</span>-test statistic <span class="math display">\[
T_{obs}=\frac{\hat\beta_{j,obs} - \beta^{(H_0)}_{j}}{\widehat{\operatorname{SE}}(\hat\beta_j)_{obs}}
\]</span></p>
<section id="p-value" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="p-value"><strong><span class="math inline">\(p\)</span>-value:</strong></h4>
<p>The <span class="math inline">\(p\)</span>-value is the probability of seeing a realization of the <strong>random</strong> <span class="math inline">\(t\)</span>-test statistic, <span class="math inline">\(T,\)</span> which is more extreme than the observed value of the test-statistic, <span class="math inline">\(T_{obs},\)</span> given the null-hypothesis is true <span class="math display">\[
\begin{align*}
p_{obs}
&amp; = P\left(|T|\geq|T_{obs}|\;\;|\;\; \text{$H_0$ is true}\right)\\[2ex]
&amp; = 2\cdot\min\{P\left(T\geq T_{obs} \;\;|\;\; \text{$H_0$ is true}\right),\;\\
&amp; \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; P\left(T\leq T_{obs} \;\;|\;\; \text{$H_0$ is true}\right)\}.
\end{align*}
\]</span></p>
<p><strong>Testing-Procedure:</strong> <!-- To do the statistical hypothesis test, we need to select a significance level $\alpha$ (e.g.,  $\alpha=0.05$ or $\alpha=0.01$).  --></p>
<ul>
<li><p>If the observed realization of the <span class="math inline">\(p\)</span>-value is larger than or equal to the significance level <span class="math display">\[
p_{obs}\geq \alpha,
\]</span> then we <strong>cannot reject the null hypothesis</strong> <span class="math inline">\(\beta_j=\beta^{(H_0)}_{j}.\)</span></p></li>
<li><p>If, however, the observed realization of the <span class="math inline">\(p\)</span>-value is strictly smaller than the significance level <span class="math display">\[
p_{obs}&lt;\alpha,
\]</span> then we <strong>reject the null hypothesis</strong> and adopt the alternative hypothesis <span class="math inline">\(\beta_j\neq \beta^{(H_0)}_{j}.\)</span></p></li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Even very large <span class="math inline">\(p\)</span>-values <span class="math inline">\(p_{obs}\approx 1 \gg \alpha\)</span> can occur simply because a given violation of a null-hypothesis is smaller than the involved estimation errors.</p>
<center>
❗The <span class="math inline">\(p\)</span>-value is <strong>not</strong> the probability that the null-hypothesis is true❗
</center>
<p></p>
<p><img src="images/terminator.jpeg" class="img-fluid"></p>
</div>
</div>
<p>:::</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Equivalence of Confidence Intervals and <span class="math inline">\(t\)</span>-Test
</div>
</div>
<div class="callout-body-container callout-body">
<p>It can be shown that a statistical hypothesis test based on the above confidence interval <span class="math inline">\(\operatorname{CI}^{1-\alpha}_{\beta_j,obs}\)</span> leads to exactly the same test decisions as a statistical hypothesis test based on the above <span class="math inline">\(t\)</span>-test statistic <span class="math inline">\(T_{obs}.\)</span></p>
</div>
</div>
</section></section><section id="f-test" class="level3" data-number="2.6.4"><h3 data-number="2.6.4" class="anchored" data-anchor-id="f-test">
<span class="header-section-number">2.6.4</span> <span class="math inline">\(F\)</span>-Test</h3>
<p>The <span class="math inline">\(t\)</span>-test statistic (equivalently the confidence interval for <span class="math inline">\(\beta_j\)</span>) allows us to test a null-hypothesis about <strong>one</strong> parameter <span class="math inline">\(\beta_j.\)</span></p>
<p>To test whether there is a relationship between the response <span class="math inline">\(Y\)</span> and total <em>vector</em> predictors <span class="math inline">\((X_1,\dots,X_p),\)</span> we can use the <span class="math inline">\(F\)</span>-test statistic.</p>
<p>In this case, the <span class="math inline">\(F\)</span>-test tests the null-hypothesis <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta_2=\dots=\beta_p=0\\
\text{versus}\quad H_1:&amp;\;\text{at least one $\beta_j\neq 0$; $j=1,\dots,p$}
\end{align*}
\]</span></p>
<p><span class="math inline">\(F\)</span>-test statistic <span class="math display">\[
F=\frac{(\operatorname{TSS}-\operatorname{RSS})/p}{\operatorname{
  RSS}/(n-p-1)}\overset{H_0}{\sim} F_{p,n-p-1}
\]</span> Under <span class="math inline">\(H_0,\)</span> i.e.&nbsp;if <span class="math inline">\(H_0\)</span> is true, the <span class="math inline">\(F\)</span>-test statistic has a <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(p\)</span> numerator and <span class="math inline">\((n-p-1)\)</span> denominator degrees of freedom.</p>
<p>If <span class="math inline">\(H_0\)</span> is correct <span class="math display">\[
\begin{align*}
E((\operatorname{TSS}-\operatorname{RSS})/p)&amp;=\sigma^2\\[2ex]
E(\operatorname{RSS}/(n-p-1))&amp;=\sigma^2
\end{align*}
\]</span></p>
<p>Therefore:</p>
<ul>
<li>If <span class="math inline">\(H_0\)</span> is correct, we expect values of <span class="math inline">\(F\approx 1.\)</span>
</li>
<li>If <span class="math inline">\(H_1\)</span> is correct, we expect values of <span class="math inline">\(F\gg 1.\)</span>
</li>
</ul>
<p>For a given realization of the training data random sample <span class="math display">\[
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{np}))
\]</span> we <strong>obs</strong>serve a specific realization of the <span class="math inline">\(F\)</span>-test statistic <span class="math display">\[
F_{obs}=\frac{(\operatorname{TSS}_{obs}-\operatorname{RSS}_{obs})/p}{\operatorname{
  RSS}_{obs}/(n-p-1)}
\]</span></p>
<section id="p-value-1" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="p-value-1"><strong><span class="math inline">\(p\)</span>-value:</strong></h4>
<p>The <span class="math inline">\(p\)</span>-value is the probability of seeing a realization of the <strong>random</strong> <span class="math inline">\(F\)</span>-test statistic, <span class="math inline">\(F,\)</span> which is more extreme than the observed value of the test-statistic, <span class="math inline">\(F_{obs},\)</span> given the null-hypothesis is true <span class="math display">\[
\begin{align*}
p_{obs}
&amp;=P\left( F \geq F_{obs} \;\;|\;\; \text{$H_0$ is true}\right),
\end{align*}
\]</span> where <span class="math inline">\(F_{obs}\)</span> denotes the observed value of the <span class="math inline">\(F\)</span>-test statistic computed from the observed training data, and where <span class="math inline">\(F\)</span> is a random variable that has a <span class="math inline">\(F_{p,n-p-1}\)</span> distribution.</p>
<p><strong>Testing-Procedure:</strong> <!-- To do the statistical hypothesis test, we need to select a significance level $\alpha$ (e.g.,  $\alpha=0.05$ or $\alpha=0.01$).  --></p>
<ul>
<li><p>If the observed realization of the <span class="math inline">\(p\)</span>-value is larger than or equal to the significance level <span class="math display">\[
p_{obs}\geq \alpha,
\]</span> then we <strong>cannot reject the null hypothesis</strong>.</p></li>
<li><p>If, however, the observed realization of the <span class="math inline">\(p\)</span>-value is strictly smaller than the significance level <span class="math display">\[
p_{obs}&lt;\alpha,
\]</span> then we <strong>reject the null hypothesis</strong> and adopt the alternative hypothesis.</p></li>
</ul></section></section><section id="probability-of-a-type-i-error-power-and-consistency" class="level3" data-number="2.6.5"><h3 data-number="2.6.5" class="anchored" data-anchor-id="probability-of-a-type-i-error-power-and-consistency">
<span class="header-section-number">2.6.5</span> Probability of a Type I Error, Power and Consistency</h3>
<p>Every statistical testing procedure (conducted using confidence intervals or test-statistics) involves the decision of</p>
<ul>
<li>not rejecting <span class="math inline">\(H_0\)</span>
</li>
</ul>
<p>versus</p>
<ul>
<li>rejecting <span class="math inline">\(H_0\)</span>
</li>
</ul>
<p>In applied research, we “aim” for rejecting <span class="math inline">\(H_0.\)</span> (Caution: the desire for rejecting <span class="math inline">\(H_0\)</span> is the reason of a lot of scientific misconduct!) For instance, when we are able to reject the no-effect null-hypothesis <span class="math display">\[
H_0: \beta_j = 0
\]</span> and thus able to adopt the alternative <span class="math display">\[
H_1: \beta_j \neq 0,
\]</span> then we can state in a publication that variable <span class="math inline">\(X_j\)</span> has an effect.</p>
<p>Thus, if we reject the null-hypothesis even though the null-hypothesis is true, we conduct a <strong>Type I Error</strong> and thus may falsely claim that variable <span class="math inline">\(X_j\)</span> has an effect. Such a false claim can be fatal.</p>
<p>Therefore, a statistical hypothesis test is only valid if it is able control the probability of conducting a type I error from above by the chosen significance level; i.e.&nbsp;if <span class="math display">\[
\underbrace{P(\text{reject }H_0 \;|\; H_0\text{ is true})}_{\text{Probability of a type I error}}\leq \alpha,
\]</span> where <span class="math inline">\(\alpha\)</span> is some small value like <span class="math inline">\(\alpha = 0.05\)</span> or <span class="math inline">\(\alpha = 0.01.\)</span> I.e., in <span class="math inline">\(100\)</span> resamples we expect to see at most <span class="math inline">\(\alpha\cdot 100\)</span> false rejections of <span class="math inline">\(H_0.\)</span></p>
<p>By choosing a small significance level like <span class="math inline">\(\alpha = 0.05\)</span> or <span class="math inline">\(\alpha = 0.01,\)</span> we make sure that we can be quite “sure” that we do not falsely reject <span class="math inline">\(H_0.\)</span></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>A statistical testing procedure only controls the probability of a type I error (falsely rejecting <span class="math inline">\(H_0,\)</span> when <span class="math inline">\(H_0\)</span> is true), <span class="math display">\[
\underbrace{P(\text{reject }H_0 \;|\; H_0\text{ is true})}_{\text{Probability of a type I error}}\leq \alpha,
\]</span> but not the probability of a type II error (falsely not rejecting <span class="math inline">\(H_0,\)</span> when <span class="math inline">\(H_1\)</span> is true). In fact, we typically do not know the probability of a type II error <span class="math display">\[
\underbrace{P(\text{not reject }H_0 \;|\; H_1\text{ is true})}_{\text{Probability of a type II error}}\leq \;\; {\color{red}?}.
\]</span></p>
<p>Therefore, when we cannot reject the null-hypothesis we cannot claim that the null-hypothesis is probably true, simply because we cannot guarantee that the probability of a type II error (falsely not rejecting <span class="math inline">\(H_0,\)</span> when <span class="math inline">\(H_1\)</span> is true) is sufficiently small.</p>
</div>
</div>
<p>Under the alternative, i.e., if for instance<br><span class="math display">\[
H_1: \beta_j \neq 0
\]</span> is true, we want to be able to reject <span class="math inline">\(H_0\)</span> with a large as possible probability. The probability of rejecting a false null-hypothesis is called <strong>Power</strong> <span class="math display">\[
\underbrace{P(\text{reject }H_0 \;|\; H_1\text{ is true})}_{\text{Power}}
\]</span> We want that <span class="math display">\[
\underbrace{P(\text{reject }H_0 \;|\; H_1\text{ is true})}_{\text{Power}} \to 1\quad\text{as}\quad |\beta_j - 0| \to \infty
\]</span> for each given sample size <span class="math inline">\(n,\)</span> and that <span class="math display">\[
\underbrace{P(\text{reject }H_0 \;|\; H_1\text{ is true})}_{\text{Power}} \to 1\quad\text{as}\quad n \to \infty
\]</span> for each given violation of the null-hypothesis <span class="math inline">\(|\beta_j - 0|&gt;0.\)</span></p>
<p>Testing procedures that fulfill the latter property are called <strong>consistent.</strong></p>
<!-- 
## The Omitted Variable Bias 

Multiple linear regression is more than mere composition of single simple linear regression models. Take a look at the following two simple linear regression results: 

![](images/Tab_3_3.png)


Observations: 

* In the first simple linear regression, we see a statistical significant effect of  `radio` on `sales`; i.e. we can reject the null hypotheses 
$$
H_0:\beta_{radio}=0
$$
and adopt the alternative hypotheses 
$$
H_1:\beta_{radio}\neq 0.
$$

* In the second simple linear regression, we see a statistical significant effect of  `newspaper` on `sales`; i.e. we can reject the null hypotheses 
$$
H_0:\beta_{newspaper}=0
$$
and adopt the alternative hypotheses 
$$
H_1:\beta_{newspaper}\neq 0.
$$

By contrast, when looking at the **multiple linear regression** when regressing `sales` onto 

* `TV`, 
* `radio` and  
* `newspaper`,

then the effect of `newspaper` becomes statistically **insignificant**; see Table 3.4. 

![](images/Tab_3_4.png)


**Reason: Omitted Variable Bias** 

The reason for this change from a statistically significant effect of `newspaper` in the simple linear regression, to an insignificant effect in the multiple linear regression  is the so-called **Omitted Variable Bias**.

Explanation of the omitted variables bias: 

* `radio` has a **true positive effect** on `sales`
* `newspaper` has actually **no effect** on `sales`
* But, `newspaper` is correlated with `radio` $r_{\texttt{newspaper},\texttt{radio}}=0.3541$; see Table 3.5

![](images/Tab_3_5.png)

* Thus, when **omitting** `radio` from the regression model, `newspaper` becomes a surrogate for `radio` and we see a spurious effect.  

::: {.callout-caution}
Interpreting statistically significant results as true effects ("a change of $X_j$ by one unit causes on average a change in $Y$ by $\beta_{j}$") is a delectate thing. 

Even if the $f(X)$ is really so simple that we can write it as a simple or multiple linear regression model, we may miss to include all relevant predictor variables and thus statistically significant results may only be spurious effects due to omitted variables. 
:::


**Interpretation of the Coefficients in Table 3.4** 

For fixed values of `TV` and `newspaper`, spending additionally 1000 USD for `radio`, increases on average `sales` by approximately 189 units.  
-->
</section></section><section id="other-considerations-in-the-regression-model" class="level2" data-number="2.7"><h2 data-number="2.7" class="anchored" data-anchor-id="other-considerations-in-the-regression-model">
<span class="header-section-number">2.7</span> Other Considerations in the Regression Model</h2>
<section id="qualitative-predictors" class="level3" data-number="2.7.1"><h3 data-number="2.7.1" class="anchored" data-anchor-id="qualitative-predictors">
<span class="header-section-number">2.7.1</span> Qualitative Predictors</h3>
<p>Often some predictors are <em>qualitative</em> variables (also known as a <em>factor</em> variables). For instance, the <code>Credit</code> dataset contains the following qualitative predictors:</p>
<ul>
<li>
<code>own</code> (house ownership: yes/no)</li>
<li>
<code>student</code> (student status: yes/no)</li>
<li>
<code>status</code> (marital status: yes/no)</li>
<li>
<code>region</code> (regions: east, west or south)</li>
</ul>
<section id="predictors-with-only-two-levels" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="predictors-with-only-two-levels">Predictors with Only Two Levels</h4>
<p>If a <strong>qualitative predictor (factor)</strong> only has two levels (i.e.&nbsp;possible values), then incorporating it into a regression model is very simple: We simply create an indicator or <strong>dummy variable</strong> that takes on two possible numerical values; for instance, <span class="math display">\[
x_{i} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person owns a house}\\
  0&amp;\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
\]</span> Using this dummy variable as a predictor in the regression equation results in the following regression model: <span class="math display">\[\begin{align*}
y_{i}
&amp;=\beta_0 + \beta_1 x_i + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i &amp;\quad \text{if the $i$th person owns a house}\\
  \beta_0 + \epsilon_i           &amp;\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span>: The average credit card balance among those who do not own a house</li>
<li>
<span class="math inline">\(\beta_0+\beta_1\)</span>: The average credit card balance among those who do own a house</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The average difference in credit card balance between owners and non-owners</li>
</ul>
<p><img src="images/Tab_3_7.png" class="img-fluid"></p>
<p>Alternatively, instead of a 0/1 coding scheme, we could create a dummy variable <span class="math display">\[
x_{i} = \left\{
  \begin{array}{ll}
  1 &amp;\quad \text{if the $i$th person owns a house}\\
-1 &amp;\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
\]</span> <span class="math display">\[\begin{align*}
y_{i}
&amp;=\beta_0 + \beta_1 x_i + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i&amp;\quad \text{if the $i$th person owns a house}\\
  \beta_0 - \beta_1 + \epsilon_i&amp;\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span>: The overall average credit card balance (ignoring the house ownership effect)</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The average amount by which house owners and non-owners have credit card balances that are above and below the overall average, respectively.</li>
</ul></section><section id="qualitative-predictors-with-more-than-two-levels" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</h4>
<p>When a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the</p>
<p><code>region</code> <span class="math inline">\(\in\{\)</span><code>South</code>, <code>West</code>, <code>East</code><span class="math inline">\(\}\)</span></p>
<p>variable, we create <strong>two</strong> dummy variables. The first could be <span class="math display">\[
x_{i1} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person is from the South}\\
  0&amp;\quad \text{if the $i$th person is not from the South,}
  \end{array}\right.
\]</span> and the second could be <span class="math display">\[
x_{i2} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person is from the West}\\
  0&amp;\quad \text{if the $i$th person is not from the West.}
  \end{array}\right.
\]</span> Using both of these dummy variables results in the following regression model: order to obtain the model <span class="math display">\[\begin{align*}
y_{i}&amp;=\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1  + \epsilon_i&amp; \quad \text{if the $i$th person is from the South}\\
  \beta_0 + \beta_2  + \epsilon_i&amp; \quad \text{if the $i$th person is from the West}\\
  \beta_0            + \epsilon_i&amp; \quad \text{if the $i$th person is from the East.}\\
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span>: The average credit card balance for individuals from the East</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The difference in the average balance between people from the South versus the East</li>
<li>
<span class="math inline">\(\beta_2\)</span>: The difference in the average balance between people from the West versus the East</li>
</ul>
<p><img src="images/Tab_3_8.png" class="img-fluid"></p>
<p>There are many different ways of coding qualitative variables besides the dummy variable approach taken here. All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular <strong>contrasts</strong>. (A detailed discussion of <em>contrasts</em> is beyond the scope of this lecture.)</p>
</section></section><section id="extensions-of-the-linear-model" class="level3" data-number="2.7.2"><h3 data-number="2.7.2" class="anchored" data-anchor-id="extensions-of-the-linear-model">
<span class="header-section-number">2.7.2</span> Extensions of the Linear Model</h3>
<section id="interaction-effects" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="interaction-effects">Interaction Effects</h4>
<p>Consider the following model</p>
<center>
<code>sales</code> <span class="math inline">\(= \beta_0 + \beta_1\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\)</span> <code>radio</code> <span class="math inline">\(+ \beta_3\)</span> <code>newspaper</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<p>which states, for instance, that the average increase in <code>sales</code> associated with a one-unit increase in <code>TV</code> is <span class="math inline">\(\beta_1,\)</span> regardless of the amount spent on <code>radio</code>.</p>
<p>However, this simple model may be incorrect. Suppose that there is a synergy effect, such that spending money on <code>radio</code> advertising actually increases the effectiveness of <code>TV</code> advertising.</p>
<p>Figure 3.5 suggests that such an effect may be present in the advertising data:</p>
<ul>
<li>When levels of either <code>TV</code> or <code>radio</code> are low, then the true <code>sales</code> are lower than predicted by the linear model.</li>
<li>But when advertising is split between the two media, then the model tends to <strong>underestimate</strong> sales. <img src="images/Fig_3_5.png" class="img-fluid">
</li>
</ul>
<p><strong>Solution: Interaction Effects:</strong></p>
<p>Consider the standard linear regression model with two variables, <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon.
\]</span> Here each predictor <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> has a given effect, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, on <span class="math inline">\(Y\)</span> and this effect does not depend on the value of the other predictor. <strong>(Additive Assumption)</strong></p>
<p>One way of extending this model is to include a third predictor, called an <strong>interaction term</strong>, which is constructed by computing the product of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2.\)</span> This results in the model <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 \overbrace{\color{red}X_1X_2}^{=X_3} + \epsilon.
\]</span> This is a powerful extension relaxing the additive assumption. Notice that the model can now be written as <span class="math display">\[
\begin{align*}
Y &amp;= \beta_0 + \underbrace{(\beta_1 + \beta_3 X_2)}_{=\tilde{\beta}_1(X_2)} X_1 + \beta_2 X_2 + \epsilon,
\end{align*}
\]</span> where the new slope parameter <span class="math inline">\(\tilde{\beta}_2(X_2)\)</span> is a linear function of <span class="math inline">\(X_2,\)</span> i.e. <span class="math display">\[
\tilde{\beta}_1(X_2)=\beta_1 + \beta_3 X_2.
\]</span></p>
<p>Thus, a change in the value of <span class="math inline">\(X_2\)</span> will change the association between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y.\)</span></p>
<p>A similar argument shows that a change in the value of <span class="math inline">\(X_1\)</span> changes the association between <span class="math inline">\(X_2\)</span> and <span class="math inline">\(Y.\)</span></p>
<p>Let us return to the <code>Advertising</code> example: A linear model that predicts <code>sales</code> using</p>
<ul>
<li>the predictor <code>radio</code>,</li>
<li>the predictor <code>TV</code>, and</li>
<li>the interaction <code>radio</code><span class="math inline">\(\times\)</span><code>TV</code>
</li>
</ul>
<p>takes the form</p>
<center>
<code>sales</code> <span class="math inline">\(= \beta_0 + \beta_1\times\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\times\)</span> <code>radio</code> <span class="math inline">\(+ \beta_3\times(\)</span> <code>radio</code><span class="math inline">\(\times\)</span> <code>TV</code><span class="math inline">\()+\epsilon\)</span>
</center>
<p> which can be rewritten as</p>
<center>
<code>sales</code> <span class="math inline">\(=\beta_0 + (\beta_1+ \beta_3\times\)</span> <code>radio</code> <span class="math inline">\()\times\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\times\)</span> <code>radio</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<p><br></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_3\)</span> denotes the increase in the effectiveness of TV advertising associated with a one-unit increase in radio advertising.</li>
</ul>
<p><img src="images/Tab_3_9.png" class="img-fluid"></p>
<p><strong>Interpretation of Table 3.9:</strong></p>
<ul>
<li>Both separate main effects, <code>TV</code> and <code>radio</code>, are statistically significant (<span class="math inline">\(p\)</span>-values smaller than 0.01).</li>
<li>Additionally, the <span class="math inline">\(p\)</span>-value for the interaction term, <code>TV</code><span class="math inline">\(\times\)</span><code>radio</code>, is extremely low, indicating that there is strong evidence for <span class="math inline">\(H_1: \beta_3\neq 0.\)</span> In other words, it is clear that the true relationship is not additive.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hierarchical Principle of Interaction Terms
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we include an interaction in a model, we should also include the main effects, even if the <span class="math inline">\(p\)</span>-values associated with their coefficients are not significant.</p>
</div>
</div>
<p><strong>Interactions with Qualitative Variables:</strong></p>
<p>An interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation.</p>
<p>Consider the <code>Credit</code> data set and suppose that we wish to predict <code>balance</code> using the predictors:</p>
<ul>
<li>
<code>income</code> (quantitative) and</li>
<li>
<code>student</code> (qualitative) using a dummy variable with <span class="math display">\[
x_{i2}=\left\{
\begin{array}{ll}
1&amp;\text{if person $i$ is a student}\\
0&amp;\text{if not}\\
\end{array}
\right.
\]</span>
</li>
</ul>
<p>In the absence of an interaction term, the model takes the form <img src="images/Eq_3_34.png" class="img-fluid"></p>
<p>Thus, the regression lines for students and non-students have different intercepts, <span class="math inline">\(\beta_0+\beta_2\)</span> versus <span class="math inline">\(\beta_0\)</span>, <strong>but the same slope</strong> <span class="math inline">\(\beta_1\)</span>.</p>
<p>This represents a potentially serious limitation of the model, since a change in <code>income</code> may have a very different effect on the credit card <code>balance</code> of a student versus a non-student.</p>
<p>This limitation can be addressed by adding an interaction variable, created by multiplying <code>income</code> with the dummy variable for student. Our model now becomes <img src="images/Eq_3_35.png" class="img-fluid"></p>
<p>Now we have different intercepts for students and non-students but also different slopes for these groups. <img src="images/Fig_3_7.png" class="img-fluid"></p>
</section><section id="polynomial-regression-non-linear-relationships" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="polynomial-regression-non-linear-relationships">Polynomial Regression: Non-linear Relationships</h4>
<p>Polynomial regression allows to accommodate non-linear relationships between the predictors <span class="math inline">\(X\)</span> and the outcome <span class="math inline">\(Y.\)</span> <img src="images/Fig_3_8.png" class="img-fluid"></p>
<p>For example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that a model of the form</p>
<center>
<code>mpg</code> <span class="math inline">\(=\beta_0 + \beta_1\times\)</span> <code>horsepower</code> <span class="math inline">\(+ \beta_2\times(\)</span><code>horsepower</code><span class="math inline">\()^2+\epsilon\)</span>
</center>
<p></p>
<p>This regression model involves predicting <code>mpg</code> using a non-linear function of <code>horsepower</code>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>But it is still a linear model!</strong> It’s simply a multiple linear regression model <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
\]</span> with</p>
<ul>
<li>
<span class="math inline">\(X_1=\)</span><code>horsepower</code> and</li>
<li>
<span class="math inline">\(X_2 =(\)</span><code>horsepower</code><span class="math inline">\()^2\)</span>
</li>
</ul>
<p>as the predictor variables.</p>
</div>
</div>
<p>Since this is nothing but a multiple linear regression model, we can use standard linear regression software to estimate <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> in order to fit the (quadratic) non-linear regression function.</p>
<p><img src="images/Tab_3_10.png" class="img-fluid"></p>
</section></section><section id="detecting-potential-problems" class="level3" data-number="2.7.3"><h3 data-number="2.7.3" class="anchored" data-anchor-id="detecting-potential-problems">
<span class="header-section-number">2.7.3</span> Detecting Potential Problems</h3>
<p>In this chapter, we discuss the use of <strong>diagnostic plots</strong> to detect potential problems.</p>
<p><strong>1. Non-linearity of the response-predictor relationships.</strong></p>
<p><strong>Diagnostic residual plots</strong> are most useful to detect possible non-linear response-predictor relationships.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span> </span>
<span></span>
<span><span class="co">## Gives the variable names in the Auto dataset</span></span>
<span><span class="co"># names(Auto)</span></span>
<span></span>
<span><span class="co">## Simple linear regression</span></span>
<span><span class="va">lmobj_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Quadratic regression </span></span>
<span><span class="va">lmobj_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Diagnostic Plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_1</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_2</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Plotting <strong>residuals versus fitted values</strong>, i.e.&nbsp;plotting the data pairs <span class="math display">\[
(\underbrace{y_i - \hat{y}_i}_{=e_i}, \hat{y}_i),\quad\text{for all}\quad i=1,\dots,n
\]</span> is a useful graphical tool for identifying non-linearity which works for the</p>
<ul>
<li>
<strong>simple linear regression model</strong> with <span class="math inline">\(\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_{i1}\)</span> and the</li>
<li>
<strong>multiple linear regression model</strong> with <span class="math inline">\(\hat{y}_i=\hat{\beta}_0+\sum_{j=1}^p\hat{\beta}_jx_{ij}\)</span>
</li>
</ul>
<p>If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as <span class="math display">\[
\log(X),\; \sqrt{X},\; \text{or}\; X^2
\]</span> in the regression model.</p>
<!-- In the later chapters, we will discuss other more advanced non-linear approaches for addressing this issue. -->
<p><strong>2. Correlation of Error Terms</strong></p>
<p>An important assumption of the linear regression model is that the error terms, <span class="math display">\[
\epsilon_1, \epsilon_2, \dots , \epsilon_n,
\]</span> are independent and thus uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that <span class="math inline">\(\epsilon_i\)</span> is positive provides little or no information about the sign of <span class="math inline">\(\epsilon_{i+1}.\)</span></p>
<p>Auto-correlations among the error terms typically occur in time series data. Figure 3.10 shows time-series of residuals with</p>
<ul>
<li>no auto-correlation (<span class="math inline">\(\rho=0\)</span>)</li>
<li>intermediate auto-correlation (<span class="math inline">\(\rho=0.5\)</span>)</li>
<li>strong auto-correlation (<span class="math inline">\(\rho=0.9\)</span>)</li>
</ul>
<p><img src="images/Fig_3_10.png" class="img-fluid"></p>
<p><strong>3. Non-Constant Variance of Error Terms (Heteroskedasticity)</strong></p>
<p>Another important issue is to check whether the errors can be assumed homoskedastic <span class="math display">\[
Var(\epsilon_i|X_i) = \sigma^2,\quad\text{for all}\quad i=1,\dots,n.
\]</span> or heteroskedastic <span class="math display">\[
Var(\epsilon_i|X_i) = \sigma^2_i,\quad i=1,\dots,n.
\]</span> One can identify <strong>non-constant variances (“heteroskedasticity”)</strong> in the errors, using diagnostic residual plots. Often one observes that the magnitude of the scattering of the residuals tends to increase with the fitted values. When faced with this problem, one possible solution is to transform the response <span class="math inline">\(Y\)</span> using a concave function such as <span class="math display">\[
\log(Y)\;\text{ or }\; \sqrt{Y}.
\]</span> Such a transformation results in a greater amount of shrinkage of the larger responses.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Quadratic regression </span></span>
<span><span class="va">lmobj_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Quadratic regression with transformed response log(Y)</span></span>
<span><span class="va">lmobj_3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">)</span><span class="op">)</span> <span class="op">~</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Diagnostic Plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_2</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_3</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<!-- 
::: {.callout-caution}
The standard formulas for 

* standard errors, 
* confidence intervals, and 
* hypothesis tests 

in this chapter are based on the assumption of 

* uncorrelated error terms $Cor(\epsilon_i,\epsilon_j)=0$ for all $i\neq j$  
* with equal variances $Var(\epsilon_i)=\sigma^2$ for all $i=1,\dots,n.$  

If in fact there is correlation and/or heteroskedasticity among the error terms, then the estimated standard errors will be wrong leading to invalid inferences. 

Thus, if the error terms are auto-correlated and/or heteroskedastic, we need to take this into account by using so-called auto-correlation and/or heteroskedasticity robust standard errors. 

The `R` package [sandwich](https://cran.r-project.org/web/packages/sandwich/index.html) contains such robust standard error estimators. 
::: 
-->
<p><strong>Note:</strong> In case of doubt (homoskedastic vs heteroskedastic error terms), one should use HC-robust inference, which allows for valid inference under both homoskedastic and heterskedastic errors.</p>
<p><strong>4. Outliers</strong></p>
<p>An outlier is a point <span class="math inline">\(i\)</span> for which <span class="math inline">\(y_i\)</span> is far from the value <span class="math inline">\(\hat{y}_i\)</span> predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p>
<p>Outliers typically have a strong effect on the <span class="math inline">\(R^2\)</span> value since they add a <strong>very large residual</strong> to its computation.</p>
<p><strong>Harmless Outlier:</strong> Figure 3.12 shows a clear outlier (observation 20) which, however, has a typical predictor value <span class="math inline">\(x_i\)</span>; i.e.&nbsp;the <span class="math inline">\(x_i\)</span>-value is right in the center of all predicor values. Such outliers have little effect on the regression fit. <img src="images/Fig_3_12.png" class="img-fluid"></p>
<p><strong>Harmful Outlier:</strong> Figure 3.13 shows again a clear outlier (observation 41) which, however, has a predictor value <span class="math inline">\(x_i\)</span> that is <strong>very atypical</strong>. Such outliers are said to have <strong>large leverage</strong> giving them power to affect the regression fit considerably. <img src="images/Fig_3_13.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Critical outliers have both,</p>
<ul>
<li><strong>large residuals</strong></li>
</ul>
<p><em>and</em></p>
<ul>
<li>
<strong>large leverage</strong>.</li>
</ul>
</div>
</div>
<p><strong>5. High Leverage Points</strong></p>
<p>In order to quantify an observation’s leverage, we compute the <strong>leverage statistic</strong> <span class="math inline">\(h_i\)</span> for each observation <span class="math inline">\(i=1,\dots,n.\)</span> A large value of this statistic indicates an observation with high leverage.</p>
<p>In the case of the <strong>simple linear regression model</strong> <span class="math display">\[
h_i = \frac{1}{n} + \frac{(x_i-\bar{x})^2}{\sum_{j=1}^n(x_{j}-\bar{x})^2}.
\]</span></p>
<p>In the case of the <strong>multiple linear regression model</strong>, <span class="math inline">\(h_i\)</span> is the <span class="math inline">\(i\)</span>th diagonal value of the <span class="math inline">\((n\times n)\)</span>-dimensional “hat-matrix” <span class="math display">\[
H=X(X^{\top}X)^{-1}X^{\top}.
\]</span></p>
<ul>
<li>The leverage statistic <span class="math inline">\(h_i\)</span> is always between <span class="math inline">\(1/n\)</span> and <span class="math inline">\(1\)</span>
</li>
<li>The average leverage for all the observations is equal to <span class="math display">\[
\bar{h}=\frac{1}{n}\sum_{i=1}^n h_i=(p + 1)/n.
\]</span>
</li>
<li>If a given observation has a leverage statistic <span class="math inline">\(h_i\)</span> that greatly exceeds the average leverage value, <span class="math inline">\((p+1)/n,\)</span> then we may suspect that the corresponding point has high leverage.</li>
</ul>
<p><strong>6. Collinearity</strong></p>
<p>Collinearity refers to the situation in which two or more predictor variables are closely related to one another.</p>
<p>In the following example, the variables <code>Age</code> and <code>Limit</code> are essentially unrelated, but the variables <code>Rating</code> and <code>Limit</code> are closely related to one another.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Credit</span><span class="op">)</span> <span class="co"># names(Credit)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Age</span>,    x <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Limit</span>, main <span class="op">=</span> <span class="st">"No Collinearity"</span>, ylab <span class="op">=</span> <span class="st">"Age"</span>, xlab <span class="op">=</span> <span class="st">"Limit"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Rating</span>, x <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Limit</span>, main <span class="op">=</span> <span class="st">"Strong Collinearity"</span>, ylab <span class="op">=</span> <span class="st">"Rating"</span>, xlab <span class="op">=</span> <span class="st">"Limit"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The left panel of Figure 3.15 shows that, in the case of unrelated predictors (<code>Age</code> and <code>Limit</code>), the least squares problem has a minimum <span class="math inline">\((\hat\beta_{Age},\hat\beta_{Limit})\)</span> that is well identified since the minimum is well defined.</p>
<p>The right panel of Figure 3.15 shows that, in the case of collinear predictors (<code>Rating</code> and <code>Limit</code>), the least squares problem has a minimum <span class="math inline">\((\hat\beta_{Rating},\hat\beta_{Limit})\)</span> that is not well identified: One can substitute values of <span class="math inline">\(\hat\beta_{Limit}\)</span> for <span class="math inline">\(\hat\beta_{Rating}\)</span> ending up in new pairs <span class="math inline">\((\hat\beta_{Rating},\hat\beta_{Limit})\)</span> with basically the same RSS-value than the original value than it is achieved by the minimizer <span class="math inline">\((\hat\beta_{Rating},\hat\beta_{Limit})\)</span>.</p>
<p><img src="images/Fig_3_15.png" class="img-fluid"></p>
<p>Table 3.11 demonstrates that this identification problem between the collinear predictors (<code>Rating</code> and <code>Limit</code>) causes a variance inflation in the variance (square of standard error) of the estimators <span class="math inline">\(\hat\beta_{Rating}\)</span> and <span class="math inline">\(\hat\beta_{Limit}.\)</span></p>
<ul>
<li>In Model 1: <span class="math inline">\(\hat\beta_{Limit} = 0.005^2=0.000025\)</span>
</li>
<li>In Model 2: <span class="math inline">\(\hat\beta_{Rating} = 0.064^2=0.004096\)</span>
</li>
</ul>
<p><img src="images/Tab_3_11.png" class="img-fluid"></p>
<p>We call this situation <strong>multicollinearity</strong>.</p>
<p>To detect multicollinearity issues, one can use the variance inflation factor (VIF) <span class="math display">\[
\operatorname{VIF}(\hat{\beta}_j)=\frac{1}{1-R^2_{X_j|X_-j}},
\]</span> where <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors.</p>
<ul>
<li>If <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is close to one, then multicollinearity is present, and <span class="math inline">\(\operatorname{VIF}(\hat{\beta}_j)\)</span> will be large.</li>
</ul>
<p>In the <code>Credit</code> data, one gets for the predictors <code>age</code>, <code>rating</code>, and <code>limit</code> the following VIF values:</p>
<ul>
<li>1.01 (<code>age</code>)</li>
<li>160.67 (<code>rating</code>)</li>
<li>160.59 (<code>limit</code>)</li>
</ul>
<p>Thus, as we suspected, there is considerable collinearity in the data!</p>
<p>Possible solutions:</p>
<ol type="1">
<li><p>Drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. <br><strong>Caution:</strong> In econometrics, dropping control variables is generally not a good idea since control variables are there to rule out possible issues with omitted variables biases.</p></li>
<li><p>Combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.</p></li>
<li><p>Use a different estimation procedure like ridge regression.</p></li>
<li><p>Live with it. At least you know where the large stand errors are coming from.</p></li>
</ol></section></section><section id="comparison-linear-regression-vs.-k-nn-regression" class="level2" data-number="2.8"><h2 data-number="2.8" class="anchored" data-anchor-id="comparison-linear-regression-vs.-k-nn-regression">
<span class="header-section-number">2.8</span> Comparison: Linear Regression vs.&nbsp;K-NN Regression</h2>
<p>Linear regression is an example of a parametric approach because it assumes a linear model form for <span class="math display">\[
f(X_i)=\beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}
\]</span></p>
<p><strong>Advantages of parametric approaches:</strong></p>
<ul>
<li>Typically easy to fit</li>
<li>Simple interpretation</li>
<li>Simple inference</li>
</ul>
<p><strong>Disadvantages of parametric approaches:</strong></p>
<ul>
<li>The parametric model assumption can be far from true; i.e. <span class="math display">\[
f(X_i) \neq \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}
\]</span>
</li>
</ul>
<p><strong>Alternative:</strong></p>
<p><strong>Non-parametric methods</strong> such as <strong>K-nearest neighbors regression</strong> since non-parametric approaches do not explicitly assume a parametric form for <span class="math inline">\(f(X).\)</span></p>
<section id="k-nearest-neighbors-k-nn-regression" class="level3" data-number="2.8.1"><h3 data-number="2.8.1" class="anchored" data-anchor-id="k-nearest-neighbors-k-nn-regression">
<span class="header-section-number">2.8.1</span> K-Nearest Neighbors (K-NN) Regression</h3>
<p>Let <span class="math inline">\(x_0\in\mathbb{R}^p\)</span> denote a certain (multivariate) predictor value at which we want to estimate <span class="math display">\[
f(x_0)
\]</span> and let <span class="math inline">\(K\)</span> denote the number of closest predictior value neighbors of <span class="math inline">\(x_0.\)</span></p>
<p>KNN regression regression then computes the estimate <span class="math display">\[
\hat{f}_K(x_0)
\]</span> in two steps:</p>
<ol type="1">
<li>Compute the distances between <span class="math inline">\(x_0\)</span> and all training data predictior values <span class="math inline">\(X_1,\dots,X_{n_{Train}}\)</span> <span class="math display">\[
d(x_0,X_1),d(x_0,X_2)\dots,d(x_0,X_{n_{Train}}).
\]</span> Use these distances to identify the <span class="math inline">\(K\)</span> training data predictior values <span class="math inline">\(X_1,\dots,X_{n_{Train}}\)</span> that are closest to <span class="math inline">\(x_0\)</span> and collect their indices the index set <span class="math inline">\(\mathcal{N}_0,\)</span> where <span class="math display">\[
\begin{align*}
\mathcal{N}_0
&amp; =\{i\in\{1,2,\dots,n_{Train}\} \; |\; d(x_0,X_i)\text{ is one of the $K$ smallest distances}\}
\end{align*}
\]</span> such that <span class="math inline">\(\mathcal{N}_0\subset\{1,2,\dots,n_{Train}\}\)</span> with <span class="math inline">\(|\mathcal{N}_0|=K.\)</span>
</li>
<li>Estimate <span class="math inline">\(f(x_0)\)</span> using the sample average of all the training responses <span class="math inline">\(y_i\)</span> with <span class="math inline">\(i\in\mathcal{N}_0,\)</span> i.e.&nbsp; <span class="math display">\[
\hat{f}_K(x_0)=\frac{1}{K}\sum_{i\in\mathcal{N}_0}y_i.
\]</span>
</li>
</ol>
<p>The above two steps are then repeated for all predictior values <span class="math inline">\(x_0\in\mathbb{R}^p\)</span> of interest.</p>
<p>The performance of the estimator <span class="math inline">\(\hat{f}_K(x_0)\)</span> depends on</p>
<ul>
<li>the choice of <span class="math inline">\(K\)</span> and</li>
<li>the choice of distance <span class="math inline">\(d\)</span>
</li>
</ul>
<p>For real valued predictors, <span class="math inline">\(x_0,X_i\in\mathbb{R}^p\)</span> a usual choice is the Euclidian distance <span class="math display">\[
d_E(x_0, X_i) = ||x_0 - X_i||^2 = \sum_{j=1}^p (x_{0j} - X_{ij})^2.
\]</span></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Use Standardized Predictors!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Typically, it is important to compute the distances with respect to the standardized (centering, and scaling to unit variance) predictor variables; i.e. <span class="math display">\[
d^*_E(x_0, X_i) = ||x^*_0 - X^*_i||^2 = \sum_{j=1}^p (x^*_{0j} - X^*_{ij})^2,
\]</span> where <span class="math display">\[
x^*_{0j} = \frac{x_{0j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
\]</span> and <span class="math display">\[
X^*_{ij} = \frac{X_{ij} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
\]</span> with <span class="math inline">\(\bar{X}_{j} = \frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}X_{ij}.\)</span></p>
<p>Otherwise, the distance values could be dominated by one of the <span class="math inline">\(p\)</span> predictors.</p>
<p>E.g. when one predictor is age (values between <span class="math inline">\(0\)</span> and <span class="math inline">\(99\)</span>) and another predictor is yearly income (values between <span class="math inline">\(0\)</span> and <span class="math inline">\(12,000,000\)</span>), then the differences in income will dominate the differences in age only because of the different scales. <!-- 
$$
\frac{X_{1j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}},\dots,\frac{X_{n_{Train}j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
$$  --></p>
</div>
</div>
<p>The left panel of Figure 3.16 shows the estimation result for <span class="math inline">\(K=1\)</span> and the right panel for <span class="math inline">\(K=9.\)</span></p>
<p><img src="images/Fig_3_16.png" class="img-fluid"></p>
<p>In general, the optimal value for <span class="math inline">\(K\)</span> will depend on the <em>bias-variance tradeoff</em>, which we introduced in <span class="quarto-unresolved-ref">?sec-SL</span>:</p>
<p><strong>A small value for <span class="math inline">\(K\)</span></strong> provides the most flexible fit, which will have</p>
<ul>
<li>low bias <span class="math display">\[
    |\operatorname{Bias}(\hat{f}_K(x_0))| = |E(\hat{f}_K(x_0)) - f(x_0)| \;\text{ is small}
    \]</span>
</li>
<li>high variance <span class="math display">\[
    Var(\hat{f}_K(x_0))  \;\text{ is large}
    \]</span>
</li>
</ul>
<p>The low bias is due to the fact that the prediction <span class="math inline">\(\hat{f}_K(x_0)\)</span> at a given <span class="math inline">\(x_0\)</span> only uses a few (e.g.&nbsp;<span class="math inline">\(K=1\)</span>) close neighbors for which we can expect that they are “good neighbors:” Close neighbors are <strong>good neighbors</strong> since <span class="math inline">\(|f(x_0) - f(X_i)|\approx 0.\)</span></p>
<p>The high variance is due to the fact that the prediction <span class="math inline">\(\hat{f}_K(x_0)\)</span> at a given <span class="math inline">\(x_0\)</span> only depends on a small number of <span class="math inline">\(K\)</span> observations (e.g.&nbsp;<span class="math inline">\(K=1\)</span>) such that the law of larger numbers had no chance to reduce variance.</p>
<p><strong>A large value of <span class="math inline">\(K\)</span></strong> provides a less flexible fit, which will have</p>
<ul>
<li>large bias <span class="math display">\[
    |\operatorname{Bias}(\hat{f}_K(x_0))| = |E(\hat{f}_K(x_0)) - f(x_0)| \;\text{ is large}
    \]</span>
</li>
<li>low variance <span class="math display">\[
    Var(\hat{f}_K(x_0))  \;\text{ is small}
    \]</span>
</li>
</ul>
<p>The large bias is due to the fact that the prediction <span class="math inline">\(\hat{f}_K(x_0)\)</span> at a given <span class="math inline">\(x_0\)</span> uses observations from a larger neighborhood (e.g.&nbsp;<span class="math inline">\(K=30\)</span>) which increases the chance of considering rather distant <span class="math inline">\(||x_0-X_i||\gg 0\)</span> and thus “bad” neigboors. Distant neighbors are <strong>bad neighbors</strong> since <span class="math inline">\(|f(x_0) - f(X_i)|\gg 0.\)</span></p>
<p>The low variance is due to the fact that the prediction <span class="math inline">\(\hat{f}_K(x_0)\)</span> at a given <span class="math inline">\(x_0\)</span> depends on a larger number of <span class="math inline">\(K\)</span> observations (e.g.&nbsp;<span class="math inline">\(K=30\)</span>) such that the law of larger numbers has a chance to reduce variance.</p>
<p><strong>An optimal value of <span class="math inline">\(K\)</span></strong> can be chosen using, e.g., cross-validation; see <a href="Ch6_ResamplingMethods.html" class="quarto-xref"><span>Chapter 6</span></a>.</p>
<p>Generally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of <span class="math inline">\(f\)</span> and vice versa.</p>
<p><strong>Figure 3.17</strong> provides an example with data generated from a one-dimensional linear regression model:</p>
<ul>
<li>black solid lines: true <span class="math inline">\(f(x)\)</span>
</li>
<li>blue curves: KNN fits <span class="math inline">\(\hat{f}_K(x)\)</span> using <span class="math inline">\(K = 1\)</span> (left plot) and <span class="math inline">\(K = 9\)</span> (right plot).</li>
</ul>
<p>Observations:</p>
<ul>
<li>The KNN fit <span class="math inline">\(\hat{f}_K(x)\)</span> using <span class="math inline">\(K = 1\)</span> is far too wiggly</li>
<li>The KNN fit <span class="math inline">\(\hat{f}_K(x)\)</span> using <span class="math inline">\(K = 9\)</span> is much closer to the true <span class="math inline">\(f(X).\)</span>
</li>
</ul>
<p>However, since the true regression function is here linear, it is hard for a non-parametric approach to compete with simple linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. <img src="images/Fig_3_17.png" class="img-fluid"></p>
<p>The blue dashed line in the left-hand panel of <strong>Figure 3.18</strong> represents the simple linear regression fit to the same data. It is almost perfect.</p>
<p>The right-hand panel of <strong>Figure 3.18</strong> reveals that linear regression outperforms KNN for this data across different choices of <span class="math inline">\(K=1,2,\dots,10.\)</span> <img src="images/Fig_3_18.png" class="img-fluid"></p>
<p><strong>Figure 3.19</strong> displays a non-linear situations in which KNN performs much better than simple linear regression. <img src="images/Fig_3_19.png" class="img-fluid"></p>
<section id="curse-of-dimensionality" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="curse-of-dimensionality">Curse of Dimensionality</h4>
<p>Unfortunately, in higher dimensions, KNN often performs worse than simple/multiple linear regression, since non-parametric approaches suffer from the <strong>curse of dimensionality</strong>.</p>
<p><strong>Figure 3.20</strong> considers the same strongly non-linear situation as in the second row of <strong>Figure 3.19</strong>, except that we have added additional noise (i.e.&nbsp;redundant) predictors that are not associated with the response.</p>
<ul>
<li>When <span class="math inline">\(p = 1\)</span> or <span class="math inline">\(p = 2,\)</span> KNN outperforms linear regression.</li>
<li>But for <span class="math inline">\(p = 3\)</span> the results are mixed, and for <span class="math inline">\(p\geq 4\)</span> linear regression is superior to KNN. <img src="images/Fig_3_20.png" class="img-fluid">
</li>
</ul>
<p>Observations:</p>
<ul>
<li>When <span class="math inline">\(p=1\)</span>, a sample size of <span class="math inline">\(n=50\)</span> can provide enough information to estimate <span class="math inline">\(f(X)\)</span> accurately using non-parametric methods since the <span class="math inline">\(K\)</span> nearest neighbors can actually be close to a given test observation <span class="math inline">\(x_0.\)</span>
</li>
<li>However, when spreading the <span class="math inline">\(n=50\)</span> data points over a large number of, for instance, <span class="math inline">\(p=20\)</span> dimensions, the <span class="math inline">\(K\)</span> nearest neighbors tend to become far away from <span class="math inline">\(x_0\)</span> causing a large bias.</li>
</ul>
<!-- 
## Assignment

Consider the following data generating process: 





::: {.cell layout-align="center"}

```{.r .cell-code}
## A Function to simulate data from a simple linear regression model
myDataGenerator <- function(){
  
  n      <- 50                                    # sample size
  beta_0 <- 0.1                                   # intercept parameter
  beta_1 <- 5                                     # slope parameter
  X      <- runif(n, min = -2, max = 2)           # predictor
  error  <- rnorm(n, mean = 0, sd = (5 + abs(X))) # heteroskedastic error term
  Y      <- beta_0 + beta_1 * X + error           # generate Y 
  
  ## return realization of the random sample
  return(data.frame("Y" = Y, "X" = X))
}
```
:::





Each time you call the function `myDataGenerator()`, you generate a new realization of the random sample
$$
((Y_1,X_1^{\top}),\dots,(Y_n,X_n^{\top}))
$$
as defined by the data generating process specified in the function-body of `myDataGenerator()`





::: {.cell layout-align="center"}

```{.r .cell-code}
data_sim    <- myDataGenerator()

head(data_sim)
```

::: {.cell-output .cell-output-stdout}

```
           Y           X
1  -2.083072 -1.05464044
2   7.949106  0.30174268
3  -5.751286 -0.07096835
4  -4.556137  0.27741075
5  -6.019922 -1.42339074
6 -10.144683 -1.41716495
```


:::
:::






For a given data generating process (as, for instance, defined in `myDataGenerator()`), we can approximate the bias of the OLS estimator by 
$$
\widehat{\operatorname{Bias}}(\hat\beta) = \widehat{E}(\hat\beta) - \beta, 
$$
where 
$$
\begin{align*}
\widehat{E}(\hat\beta) 
& = \frac{1}{B}\sum_{j=1}^B \hat\beta_{b}\\[2ex] 
& = \frac{1}{B}\sum_{j=1}^B \begin{pmatrix}\hat\beta_{0,b}\\\hat\beta_{1,b}\end{pmatrix}\\[2ex]
& = \begin{pmatrix}\frac{1}{B}\sum_{j=1}^B \hat\beta_{0,b}\\ \frac{1}{B}\sum_{j=1}^B \hat\beta_{1,b}\end{pmatrix}
\end{align*} 
$$
denotes the sample mean across $B$-many simulated realizations 
$$
\hat\beta_1,\hat\beta_2,\dots,\hat\beta_B 
$$
of the OLS estimator $\hat\beta,$ and where $\beta$ denotes the true parameter vector; i.e., here 
$$
\beta
= \begin{pmatrix}\beta_{0}\\ \beta_{1}\end{pmatrix} 
= \begin{pmatrix}0.1\\ 5\end{pmatrix}. 
$$

By the law of large numbers, the approximation 
$$
\widehat{\operatorname{Bias}}(\hat\beta) \approx \operatorname{Bias}(\hat\beta) 
$$
becomes arbitrarily precise as $B\to\infty.$ Setting, for instance, $B=100000$ typically guarantees a very good approximation. 


#### **Assignment Task Nr. 1 (Bias of the OLS Estimator):** {-}

1. Use the above given data generating process to generate $B=100,000$ OLS estimates of the true (usually unknown) parameter vector $\beta^{\top}=(\beta_0,\beta_1).$ Use two boxplot (one for the estimates of $\beta_0$ and another for those of $\beta_1$) to visualize the estimation results.  
2. Check the bias of the OLS estimator for the above given data generating process using the above described (Monte Carlo) simulation approach. 
3. Based on your simulation results, can you state that the OLS estimatior is *generally* biased/unbiased? 


#### **Assignment Task Nr. 2 (Coverage Probability of Confidence Intervals):** {-}

1. Use the above given data generating process to generate $B=100,000$ realizations 
$$
\operatorname{CI}^{1-\alpha}_{\beta_1,1},\operatorname{CI}^{1-\alpha}_{\beta_1,2},\dots,\operatorname{CI}^{1-\alpha}_{\beta_1,B}
$$
of the $(1-\alpha)\cdot 100 \%$ confidence interval $\operatorname{CI}^{1-\alpha}_{\beta_1}$ for $\beta_1.$ Set $\alpha=0.05.$ 
2. Approximate the true coverage probability using the empirical coverage probability across all $B$ simulated confidence interals: 
$$
P(\beta_1^{(H_0)} \in \operatorname{CI}^{1-\alpha}_{\beta_1})\approx \frac{1}{B}\sum_{j=1}^B 1_{\left(\beta_1^{(H_0)} \in \operatorname{CI}^{1-\alpha}_{\beta_1,j}\right)},
$$
where $1_{(\text{TRUE})}=1$ and $1_{(\text{FALSE})}=0.$  
Consider the following two cases: 
      a. The null-hypothesis is true, i.e. $\beta_1^{(H_0)} = 5 = \beta_1.$
      b. The null-hypothesis is false with $\beta_1^{(H_0)} = 0 \neq \beta_1.$

Note: By the law of large numbers, the above approximation becomes arbitrarily precise as $B\to\infty.$ Thus, setting, for instance, $B=100,000$ typically guarantees a very good approximation. 


**Link to your personal assignment repository:**</br>
[https://classroom.github.com/a/GYoOx4tD](https://classroom.github.com/a/GYoOx4tD) 

--></section></section></section><section id="self-study-exercises" class="level2" data-number="2.9"><h2 data-number="2.9" class="anchored" data-anchor-id="self-study-exercises">
<span class="header-section-number">2.9</span> Self-Study: Exercises</h2>
<p>Prepare the following exercises of Chapter 3 in our course textbook <code>ISLR</code>:</p>
<ul>
<li>Exercise 1</li>
<li>Exercise 2</li>
<li>Exercise 3</li>
<li>Exercise 8</li>
<li>Exercise 9</li>
</ul>
<!-- {{< include Ch4_LinearRegression_Solutions.qmd >}} --><section id="solutions" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="solutions">Solutions</h3>
<section id="exercise-1" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="exercise-1">Exercise 1</h4>
<p><strong>1 a)</strong> Describe the null hypotheses to which the <span class="math inline">\(p\)</span>-values given in Table 3.4 correspond.</p>
<p><img src="DATA/Table.PNG" class="img-fluid"></p>
<p><strong>1 b)</strong> Explain what conclusions you can draw based on these <span class="math inline">\(p\)</span>-values. Your explanation should be phrased in terms of <code>sales</code>, <code>TV</code>, <code>radio</code>, and <code>newspaper</code>, rather than in terms of the coefficients of the linear model.</p>
<p><strong>Answers:</strong></p>
<p><strong>1 a)</strong> In Table 3.4, the null hypothesis for <code>TV</code> is that in the presence of <code>radio</code> ads and <code>newspaper</code> ads, <code>TV</code> ads have no effect on sales. Similarly, the null hypothesis for <code>radio</code> is that in the presence of <code>TV</code> ads and <code>newspaper</code> ads, <code>radio</code> ads have no effect on sales.</p>
<p><strong>1 b)</strong> The low p-values of <code>TV</code> and <code>radio</code> allow us to reject the “no effect” null hypotheses for <code>TV</code> and <code>radio</code>. Hence, we believe that</p>
<ul>
<li>
<code>TV</code> ads have an effect on <code>sales</code> in the presence of <code>radio</code> and <code>newspaper</code> ads.</li>
<li>
<code>radio</code> ads have an effect on <code>sales</code> in the presence of <code>TV</code> and <code>newspaper</code> ads.</li>
</ul>
<p>The high p-value of <code>newspaper</code> does <em>not</em> allow us to reject the “no effect” null-hypothesis. This constitutes an <strong>inconclusive result</strong> and only says that the possible effects of <code>newspaper</code> ads are not large enough to stand out from the estimation errors.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remember
</div>
</div>
<div class="callout-body-container callout-body">
<p>An insignificant hypothesis test result is never informative about whether the tested null hypothesis is true. We do not have an error-control for falsely accepting the null-hypothesis, i.e.&nbsp;for type-II-errors. We only have an error-control (by the significance level) for falsely rejecting the null-hypothesis, i.e.&nbsp;for type-I-errors.</p>
</div>
</div>
</section><section id="exercise-2" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="exercise-2">Exercise 2</h4>
<p>Carefully explain the main difference between the KNN classifier and KNN regression methods.</p>
<p><strong>Answer:</strong></p>
<p>KNN classifier and KNN regression methods are closely related in formula. However, the final result of KNN classifier is the classification output for <span class="math inline">\(Y\)</span> (qualitative), given a certain predictor <span class="math inline">\(x_0\)</span>, where as the output for a KNN regression predicts the quantitative value for <span class="math inline">\(f(x_0)\)</span>, given a certain predictor <span class="math inline">\(x_0\)</span>. <!-- 
Finally, KNN classifies the test observation $x_0$ to the class $j$ with the largest probability from @eq-DefKNN.  
--></p>
</section><section id="exercise-3" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="exercise-3">Exercise 3</h4>
<p>Suppose we have a data set with five predictors:</p>
<p><span class="math inline">\(X_1 =GPA\)</span></p>
<p><span class="math inline">\(X_2 = IQ\)</span></p>
<p><span class="math inline">\(X_3 = Gender\)</span> (<span class="math inline">\(1\)</span> for Female and <span class="math inline">\(0\)</span> for Male)</p>
<p><span class="math inline">\(X_4 =\)</span> Interaction between <span class="math inline">\(GPA\)</span> and <span class="math inline">\(IQ\)</span></p>
<p><span class="math inline">\(X_5 =\)</span> Interaction between <span class="math inline">\(GPA\)</span> and <span class="math inline">\(Gender\)</span></p>
<p>The response variable (in thousands of dollars) is defined as:</p>
<p><span class="math inline">\(Y =\)</span> starting salary after graduation</p>
<p>Suppose we use least squares to fit the model, and get:</p>
<p><span class="math inline">\(\hat{\beta}_0 = 50\)</span>, <span class="math inline">\(\hat{\beta}_1 = 20\)</span>, <span class="math inline">\(\hat{\beta}_2 = 0.07\)</span>, <span class="math inline">\(\hat{\beta}_3 = 35\)</span>, <span class="math inline">\(\hat{\beta}_4 = 0.01\)</span>, and <span class="math inline">\(\hat{\beta}_5 = −10\)</span>.</p>
<p>Thus we have:</p>
<p><span class="math display">\[
\begin{align*}
&amp;E[Y|X] = \\
&amp; 50 + 20\,\overbrace{GPA}^{X_1} + 0.07\,\overbrace{IQ}^{X_2} + 35\,\overbrace{Gender}^{X_3} +
0.01\,\overbrace{GPA\cdot IQ}^{X_4=X_1\cdot X_2} - 10\,\overbrace{GPA\cdot Gender}^{X_5=X_1\cdot X_3}
\end{align*}
\]</span></p>
<p><strong>3 a)</strong> Which answer is correct, and why?</p>
<ol type="i">
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, males earn more on average than females.</li>
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, females earn more on average than males.</li>
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, males earn more on average than females provided that the <span class="math inline">\(GPA\)</span> is high enough.</li>
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, females earn more on average than males provided that the <span class="math inline">\(GPA\)</span> is high enough.</li>
</ol>
<p><strong>Answer:</strong> Observe that: <span class="math display">\[
\begin{align*}
\text{Male\; $(X_3 = 0)$:}\quad   &amp; 50 + 20 X_1 + 0.07 X_2 + \phantom{3}0 + 0.01\,(X_1 \cdot X_2) -0     \\[1.5ex]
\text{Female\; $(X_3 = 1)$:}\quad &amp; 50 + 20 X_1 + 0.07 X_2 + 35 + 0.01(X_1 \cdot X_2) - 10\,X_1
\end{align*}
\]</span></p>
<p>Thus 3 a) iii. is correct, since once the <span class="math inline">\(X_1=\)</span><code>GPA</code> is high enough (<span class="math inline">\(35-10\,X_1&lt;0 \Leftrightarrow X_1&gt;3.5\)</span>), males earn more on average.</p>
<p><strong>3 b)</strong> Predict the salary of a female with <code>IQ</code> of 110 and a <code>GPA</code> of 4.0.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">GPA</span>    <span class="op">&lt;-</span>   <span class="fl">4</span></span>
<span><span class="va">IQ</span>     <span class="op">&lt;-</span> <span class="fl">110</span></span>
<span><span class="va">Gender</span> <span class="op">&lt;-</span>   <span class="fl">1</span> <span class="co"># female = 1</span></span>
<span><span class="co">## Prediction</span></span>
<span><span class="va">Y_hat</span>  <span class="op">&lt;-</span> <span class="fl">50</span> <span class="op">+</span> <span class="fl">20</span><span class="op">*</span><span class="va">GPA</span> <span class="op">+</span> <span class="fl">0.07</span><span class="op">*</span><span class="va">IQ</span> <span class="op">+</span> <span class="fl">35</span><span class="op">*</span><span class="va">Gender</span> <span class="op">+</span> <span class="fl">0.01</span><span class="op">*</span><span class="va">GPA</span><span class="op">*</span><span class="va">IQ</span> <span class="op">-</span> <span class="fl">10</span><span class="op">*</span><span class="va">GPA</span></span>
<span><span class="va">Y_hat</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 137.1</code></pre>
</div>
</div>
<p><strong>3 c)</strong> True or false: Since the coefficient for the <code>GPA</code><span class="math inline">\(\times\)</span><code>IQ</code> interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.</p>
<p><strong>Answer:</strong></p>
<p>False. We must examine the <span class="math inline">\(p\)</span>-value (or the <span class="math inline">\(t\)</span>-statistic) of the regression coefficient to determine if the interaction term is statistically significant or not.</p>
</section><section id="exercise-8" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="exercise-8">Exercise 8</h4>
<p>This question involves the use of simple linear regression on the <code>Auto</code> data set.</p>
<p><strong>8 a)</strong> Use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to perform a simple linear regression with <code>mpg</code> as the response and <code>horsepower</code> as the predictor. Use the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function to print the results.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Auto"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform linear regression</span></span>
<span><span class="va">lmObj_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Use summary function to print the results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ horsepower, data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5710  -3.2592  -0.3435   2.7630  16.9240 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***
horsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.906 on 390 degrees of freedom
Multiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 
F-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Comment on the output. For example:</p>
<p><strong>i)</strong> Is there a relationship between the predictor and the response?</p>
<p><strong>Answer:</strong></p>
<p>Yes, there is. The predictor horsepower has a statistically significant (<span class="math inline">\(p&lt;0.001\)</span>) linear relationship with the response.</p>
<p><strong>ii)</strong> How strong is the relationship between the predictor and the response?</p>
<p><strong>Answer:</strong></p>
<p>Statistical significance does not necessarily mean a practically strong or important relationship.</p>
<p>To quantify the strength of the relationship between the predictor and the response, we can look at the following quantities:</p>
<ul>
<li>Residual Standard Error (RSE) (estimate of the standard deviation of <span class="math inline">\(\epsilon\)</span>) in comparison to the RSE of the trivial linear regression model with only an intercept.</li>
<li>The <span class="math inline">\(R^2\)</span> Statistic (the proportion of variance explained by the model)</li>
<li>The <span class="math inline">\(F\)</span>-Statistic</li>
</ul>
<p>The Residual Standard Error (RSE) of the regression model with <code>intercept</code> and <code>horsepower</code> as predictors is given by:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## RSE of lm(mpg ~ horsepower):</span></span>
<span><span class="va">RSS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">n</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">RSE</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">RSS</span><span class="op">/</span><span class="op">(</span><span class="va">n</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">RSE</span>, <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.906</code></pre>
</div>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Alternatively: </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">$</span><span class="va">sigma</span>, <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.906</code></pre>
</div>
</div>
<p>This RSE value is considerable smaller than the RSE of a model with only an intercept:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lmObj_onlyIntercept</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="op">+</span><span class="fl">1</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="va">RSS_onlyIntercept</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lmObj_onlyIntercept</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">n</span>                   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lmObj_onlyIntercept</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">RSE_onlyIntercept</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">RSS_onlyIntercept</span><span class="op">/</span><span class="op">(</span><span class="va">n</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">RSE_onlyIntercept</span>, <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7.805</code></pre>
</div>
</div>
<p>Thus, the larger model with <code>horsepower</code> included explains more of the variances in the response variable <code>mpg</code>. Including <code>horsepower</code> as a predictor reduces the RSE by <code>((RSE_onlyIntercept - RSE)/RSE_onlyIntercept)*100</code> %; i.e.&nbsp;by 37.15%.</p>
<p>The <span class="math inline">\(R^2\)</span> value:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">$</span><span class="va">r.squared</span>, <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.61</code></pre>
</div>
</div>
<p>shows that <span class="math inline">\(60\%\)</span> of variability in <span class="math inline">\(Y\)</span> can be explained using an intercept and <code>horsepower</code> as predictors.</p>
<p>The value of the <span class="math inline">\(F\)</span> statistic</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">$</span><span class="va">fstatistic</span>, <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> value  numdf  dendf 
599.72   1.00 390.00 </code></pre>
</div>
</div>
<p>is much larger than <span class="math inline">\(1\)</span> which means that the linear regression model with intercept and <code>horsepower</code> fits the data significantly better than the trivial regression model with only an intercept.</p>
<p><strong>iii)</strong> Is the relationship between the predictor and the response positive or negative?</p>
<p><strong>Answer:</strong></p>
<p>The relationship is negative, as we can see from the parameter estimate for <code>horsepower</code></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>horsepower 
-0.1578447 </code></pre>
</div>
</div>
<p><strong>iv)</strong> What is the predicted <code>mpg</code> associated with a <code>horsepower</code> of <span class="math inline">\(98\)</span>? What is the associated <span class="math inline">\(95\%\)</span> confidence interval? <!-- and prediction intervals? --></p>
<p><strong>Answer:</strong></p>
<p>The predicted value plus confidence interval:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Horsepower of 98</span></span>
<span><span class="va">new_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>horsepower <span class="op">=</span> <span class="fl">98</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># confidence interval </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">lmObj_1</span>, newdata <span class="op">=</span> <span class="va">new_df</span>, interval <span class="op">=</span> <span class="st">"confidence"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit      lwr      upr
1 24.46708 23.97308 24.96108</code></pre>
</div>
</div>
<!-- 
The predicted value plus prediction interval:




::: {.cell layout-align="center"}

```{.r .cell-code}
# Horsepower of 98
new_df <- data.frame(horsepower = 98)

# prediction interval
predict(object = lmObj_1, newdata = new_df, interval = "prediction")
```

::: {.cell-output .cell-output-stdout}

```
       fit     lwr      upr
1 24.46708 14.8094 34.12476
```


:::
:::




-->
<p><strong>8 b)</strong> Plot the response and the predictor. Use the <code><a href="https://rdrr.io/r/graphics/abline.html">abline()</a></code> function to display the least squares regression line.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Auto</span><span class="op">$</span><span class="va">horsepower</span>, y <span class="op">=</span> <span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, ylab <span class="op">=</span> <span class="st">"MPG"</span>, xlab <span class="op">=</span> <span class="st">"Horsepower"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lmObj_1</span>, col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, </span>
<span>       legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"(y,x)"</span>, <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"("</span>,<span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>,<span class="st">",x)"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>       pch<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="cn">NA</span><span class="op">)</span>, lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>,<span class="fl">1</span><span class="op">)</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"blue"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>8 c)</strong> Use the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmObj_1</span>, col<span class="op">=</span><span class="st">'blue'</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Looking at the smoothing line of the residuals (<span class="math inline">\(e_i=y_i−\hat{y}_i\)</span>) vs.&nbsp;the fitted values (<span class="math inline">\(\hat{y}_i\)</span>), there is a strong pattern in the residuals, indicating non-linearity. You can see evidence of this also in the scatter plot in the answer for question 8 b).</p>
<p>There also appears to be non-constant variance in the error terms (heteroscedasticity), but this may be corrected to an extent when trying a quadratic fit. If not, transformations such as <span class="math inline">\(log(y)\)</span> or <span class="math inline">\(\sqrt{y}\)</span> can shrink larger responses by a greater amount and reduce this issue.</p>
<p>There are some observations with large standardized residuals &amp; high leverage (hence, high Cook’s Distance) that we need to review.</p>
</section><section id="exercise-9" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="exercise-9">Exercise 9</h4>
<p>This question involves the use of multiple linear regression on the <code>Auto</code> data set.</p>
<p><strong>9 a)</strong> Produce a scatterplot matrix which includes all of the variables in the data set.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Auto"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Produce scatterplot matrix</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/pairs.html">pairs</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>9 b)</strong> Compute the matrix of correlations between the variables using the function <code><a href="https://rdrr.io/r/stats/cor.html">cor()</a></code>. You will need to exclude the <code>name</code> variable, which is qualitative.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">Auto</span>, select <span class="op">=</span> <span class="op">-</span><span class="va">name</span><span class="op">)</span><span class="op">)</span>, <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              mpg cylinders displacement horsepower weight acceleration year
mpg           1.0      -0.8         -0.8       -0.8   -0.8          0.4  0.6
cylinders    -0.8       1.0          1.0        0.8    0.9         -0.5 -0.3
displacement -0.8       1.0          1.0        0.9    0.9         -0.5 -0.4
horsepower   -0.8       0.8          0.9        1.0    0.9         -0.7 -0.4
weight       -0.8       0.9          0.9        0.9    1.0         -0.4 -0.3
acceleration  0.4      -0.5         -0.5       -0.7   -0.4          1.0  0.3
year          0.6      -0.3         -0.4       -0.4   -0.3          0.3  1.0
origin        0.6      -0.6         -0.6       -0.5   -0.6          0.2  0.2
             origin
mpg             0.6
cylinders      -0.6
displacement   -0.6
horsepower     -0.5
weight         -0.6
acceleration    0.2
year            0.2
origin          1.0</code></pre>
</div>
</div>
<p><strong>9 c)</strong> Use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to perform a multiple linear regression with <code>mpg</code> as the response and all other variables except <code>name</code> as the predictors. Use the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function to print the results. Comment on the output by answering the below questions 9 c i) to 9 c iii).</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center" data-scrolled="true">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Perform multiplie linear regression</span></span>
<span><span class="va">fit.lm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span><span class="va">name</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Print results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.lm</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ . - name, data = Auto)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.5903 -2.1565 -0.1169  1.8690 13.0604 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***
cylinders     -0.493376   0.323282  -1.526  0.12780    
displacement   0.019896   0.007515   2.647  0.00844 ** 
horsepower    -0.016951   0.013787  -1.230  0.21963    
weight        -0.006474   0.000652  -9.929  &lt; 2e-16 ***
acceleration   0.080576   0.098845   0.815  0.41548    
year           0.750773   0.050973  14.729  &lt; 2e-16 ***
origin         1.426141   0.278136   5.127 4.67e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.328 on 384 degrees of freedom
Multiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 
F-statistic: 252.4 on 7 and 384 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p><strong>9 c i)</strong> Is there a relationship between the predictors and the response?</p>
<p><strong>Answer:</strong></p>
<p>Yes, there is a relationship between the predictors and the response. By testing the null hypothesis of whether all (except intercept) the regression coefficients are zero (i.e.&nbsp;H<span class="math inline">\(_0\)</span>: <span class="math inline">\(\beta_1=\dots=\beta_7=0\)</span>), we can see that the <span class="math inline">\(F\)</span>-statistic is big and its <span class="math inline">\(p\)</span>-value is close to zero, indicating evidence against the null hypothesis.</p>
<p><strong>9 c ii)</strong> Which predictors appear to have a statistically significant relationship to the response?</p>
<p><strong>Answer:</strong></p>
<p>Looking at the <span class="math inline">\(p\)</span>-values associated with each predictor’s <span class="math inline">\(t\)</span>-statistic, we see that <code>displacement</code>, <code>weight</code>, <code>year</code>, and <code>origin</code> have a statistically significant relationship, while <code>cylinders</code>, <code>horsepower</code>, and <code>acceleration</code> do not.</p>
<p><strong>Caution:</strong> This consideration neglects issues due to multiple testing. When testing at the significance level <span class="math inline">\(\alpha=0.05\)</span>, then each single test has a type I error (false H<span class="math inline">\(_0\)</span> rejections) rate of up to <span class="math inline">\(5\%\)</span>. These type I error rates accumulate since we consider seven hypothesis tests simultaneously, and thus the probability of seeing one type I error among the seven tests is up to <span class="math inline">\(7\cdot 5\%=35\%\)</span>. So is quite likely to see one type I error.</p>
<p><strong>Bonferroni correction for multiple testing:</strong> To determine if any of the seven predictors is statistically significant, the corresponding <span class="math inline">\(p\)</span>-value must be smaller than <span class="math inline">\(\alpha/7\)</span>. For instance, with <span class="math inline">\(\alpha/7=0.05/7\approx 0.007\)</span>, only <code>weight</code>, <code>year</code>, and <code>origin</code> have a statistically significant relationships to the response.</p>
<p><strong>9 c iii)</strong> What does the coefficient for the <code>year</code> variable suggest?</p>
<p><strong>Answer:</strong></p>
<p>The regression coefficient for <code>year</code> suggests that, on average, one <code>year</code> later year-of-construction is associated with an increased <code>mpg</code> by <span class="math inline">\(0.75\)</span>, when holding every other predictor value constant.</p>
<p><strong>9 d)</strong> Use the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">fit.lm</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>The “Residuals vs Fitted” plot (1st plot) shows some systematic deviations of the residuals from <span class="math inline">\(0\)</span>. The reason is that we are imposing a straight “line” (better hyper plane) fit for the conditional mean function <span class="math inline">\(E[Y|X]=f(X)\)</span> which appears non-linear here. This results in a systematic underestimation of the true conditional mean function for large and small fitted values <span class="math inline">\(\hat{y}=\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p\)</span>.</p></li>
<li><p>The “Normal Q-Q” plot (2nd plot) suggests non-normally distributed residuals–particularly the upper tail deviates from that of a normal distribution.</p></li>
<li><p>The “Residuals vs Leverage” plot (3rd plot) shows that there are some potential outliers that we can see when: standardized residuals are below <span class="math inline">\(-2\)</span> or above <span class="math inline">\(+2\)</span>. Moreover, the plot shows also potentially problematic “high-leverage” points with leverage values heavily exceeding the rule-of-thumb threshold <span class="math inline">\((p+1)/n=8/392=0.02\)</span>. All points with simultaneously high-leverages and large absolute standardized residuals should be handled with care since these may distort the estimation.</p></li>
<li><p>The “Scale-Location” plot (4th plot) shows is rather inconclusive about heteroscedasticity. However the “Residuals vs Fitted” plot (1st plot)shows some clear sign of heteroscedastic residuals.</p></li>
</ul>
<p><strong>9 e)</strong> Use the <code>*</code> and <code>:</code> symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?</p>
<p><strong>Answer:</strong></p>
<p>Violating the hierarchy principle:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fit.lm0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span><span class="op">+</span><span class="va">cylinders</span><span class="op">+</span><span class="va">year</span><span class="op">+</span><span class="va">weight</span><span class="op">:</span><span class="va">displacement</span>, </span>
<span>              data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.lm0</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ horsepower + cylinders + year + weight:displacement, 
    data = Auto)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.1046 -2.8861 -0.2415  2.3967 15.3221 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         -1.343e+01  5.043e+00  -2.663  0.00807 ** 
horsepower          -3.914e-02  1.278e-02  -3.063  0.00234 ** 
cylinders           -1.358e+00  3.233e-01  -4.201 3.31e-05 ***
year                 6.661e-01  6.019e-02  11.067  &lt; 2e-16 ***
weight:displacement -3.354e-06  1.352e-06  -2.480  0.01355 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.985 on 387 degrees of freedom
Multiple R-squared:  0.7419,    Adjusted R-squared:  0.7393 
F-statistic: 278.2 on 4 and 387 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Following the hierarchical principle:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fit.lm1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">~</span><span class="va">horsepower</span><span class="op">+</span><span class="va">cylinders</span><span class="op">+</span><span class="va">year</span><span class="op">+</span><span class="va">weight</span><span class="op">*</span><span class="va">displacement</span>, </span>
<span>              data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.lm1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ horsepower + cylinders + year + weight * displacement, 
    data = Auto)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.7530 -1.8228 -0.0602  1.5780 12.6133 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         -2.210e+00  3.819e+00  -0.579  0.56316    
horsepower          -3.396e-02  9.560e-03  -3.552  0.00043 ***
cylinders            2.072e-01  2.914e-01   0.711  0.47756    
year                 7.858e-01  4.555e-02  17.250  &lt; 2e-16 ***
weight              -1.084e-02  6.346e-04 -17.076  &lt; 2e-16 ***
displacement        -7.947e-02  9.905e-03  -8.023 1.26e-14 ***
weight:displacement  2.431e-05  2.141e-06  11.355  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.976 on 385 degrees of freedom
Multiple R-squared:  0.8568,    Adjusted R-squared:  0.8546 
F-statistic: 384.1 on 6 and 385 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Note that there is a difference between using <code>A:B</code> and <code>A*B</code> when running a regression. While the first includes only the interaction term between the variable <code>A</code> and <code>B</code>, the second one also includes the stand-alone variables <code>A</code> and <code>B</code>.</p>
<p>Generally, you should follow the hierarchical principle for interaction effects: If we include an interaction in a model, we should also include the main effects, even if the <span class="math inline">\(p\)</span>-values associated with their coefficients are not significant.</p>
<p><strong>9 f)</strong></p>
<p>Try a few different transformations of the variables, such as <span class="math inline">\(\log(X)\)</span>, <span class="math inline">\(\sqrt{X}\)</span>, <span class="math inline">\(X^2\)</span>. Comment on your findings.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fit.lm2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">weight</span><span class="op">)</span><span class="op">+</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">)</span><span class="op">+</span></span>
<span>                <span class="va">acceleration</span><span class="op">+</span><span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">acceleration</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>,</span>
<span>              data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.lm2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ log(weight) + sqrt(horsepower) + acceleration + 
    I(acceleration^2), data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-11.2932  -2.5082  -0.2237   2.0237  15.7650 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       178.30303   10.80451  16.503  &lt; 2e-16 ***
log(weight)       -14.74259    1.73994  -8.473 5.06e-16 ***
sqrt(horsepower)   -1.85192    0.36005  -5.144 4.29e-07 ***
acceleration       -2.19890    0.63903  -3.441 0.000643 ***
I(acceleration^2)   0.06139    0.01857   3.305 0.001037 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.99 on 387 degrees of freedom
Multiple R-squared:  0.7414,    Adjusted R-squared:  0.7387 
F-statistic: 277.3 on 4 and 387 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">##</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">fit.lm2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-30-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This try suffers basically from the same issues as the model considered in 9 d)</p>
<p>Let’s consider again the model with all predictors (except <code>name</code>), but with transforming the outcome variable <code>mpg</code> by a <span class="math inline">\(\log\)</span>-transformation.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fit.lm3</span> <span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">)</span><span class="op">~</span> <span class="va">.</span> <span class="op">-</span><span class="va">name</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.lm3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log(mpg) ~ . - name, data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.40955 -0.06533  0.00079  0.06785  0.33925 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.751e+00  1.662e-01  10.533  &lt; 2e-16 ***
cylinders    -2.795e-02  1.157e-02  -2.415  0.01619 *  
displacement  6.362e-04  2.690e-04   2.365  0.01852 *  
horsepower   -1.475e-03  4.935e-04  -2.989  0.00298 ** 
weight       -2.551e-04  2.334e-05 -10.931  &lt; 2e-16 ***
acceleration -1.348e-03  3.538e-03  -0.381  0.70339    
year          2.958e-02  1.824e-03  16.211  &lt; 2e-16 ***
origin        4.071e-02  9.955e-03   4.089 5.28e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1191 on 384 degrees of freedom
Multiple R-squared:  0.8795,    Adjusted R-squared:  0.8773 
F-statistic: 400.4 on 7 and 384 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">##</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">fit.lm3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-31-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This model specification is much better!</p>
<ul>
<li>No clear issues of systematic under/over estimations for given fitted values.</li>
<li>No clear issues of heteroscedastic residuals.</li>
<li>Normality assumption may be wrong, but this isn’t problematic since we have a large dataset, such that a central limit theorem will make the estimators asymptotically normal distributed.</li>
<li>One large leverage point which, however, has a small residual.</li>
</ul></section></section></section><section id="self-study-r-lab-linear-regression" class="level2" data-number="2.10"><h2 data-number="2.10" class="anchored" data-anchor-id="self-study-r-lab-linear-regression">
<span class="header-section-number">2.10</span> Self-Study: <code>R</code>-Lab Linear Regression</h2>
<section id="libraries" class="level3" data-number="2.10.1"><h3 data-number="2.10.1" class="anchored" data-anchor-id="libraries">
<span class="header-section-number">2.10.1</span> Libraries</h3>
<p>The <code><a href="https://rdrr.io/r/base/library.html">library()</a></code> function is used to load <em>libraries</em>, or groups of functions and data sets that are not included in the base <code>R</code> distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries.</p>
<p>Here we load the <code>MASS</code> package, which is a very large collection of data sets and functions. We also load the <code>ISLR2</code> package, which includes the data sets associated with this book.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as <code>MASS</code>, come with <code>R</code> and do not need to be separately installed on your computer. However, other packages, such as <code>ISLR2</code>, must be downloaded the first time they are used. <!-- 
This can be done directly from within `R`. For example, on a Windows system,  select the `Install package` option under the `Packages` tab.  After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and `R` will automatically download the package. Alternatively,  --></p>
<p>This can be done, for instance, at the <code>R</code> command line via <code>install.packages("ISLR2")</code> function. This installation only needs to be done the first time you use a package. However, the <code><a href="https://rdrr.io/r/base/library.html">library()</a></code> function must be called within each <code>R</code> session.</p>
</section><section id="simple-linear-regression" class="level3" data-number="2.10.2"><h3 data-number="2.10.2" class="anchored" data-anchor-id="simple-linear-regression">
<span class="header-section-number">2.10.2</span> Simple Linear Regression</h3>
<p>The <code>ISLR2</code> library contains the <code>Boston</code> data set, which records <code>medv</code> (median house value) for <span class="math inline">\(506\)</span> census tracts in Boston. We will seek to predict <code>medv</code> using <span class="math inline">\(12\)</span> predictors such as <code>rmvar</code> (average number of rooms per house), <code>age</code> (average age of houses), and <code>lstat</code> (percent of households with low socioeconomic status).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat
1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21
  medv
1 24.0
2 21.6
3 34.7
4 33.4
5 36.2
6 28.7</code></pre>
</div>
</div>
<p>To find out more about the data set, we can type <code><a href="https://rdrr.io/pkg/ISLR2/man/Boston.html">?Boston</a></code>.</p>
<p>We will start by using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to fit a simple linear regression model, with <code>medv</code> as the response and <code>lstat</code> as the predictor. The basic syntax is <code>lm(y ~ x, data)</code>, where <code>y</code> is the response, <code>x</code> is the predictor, and <code>data</code> is the data set in which these two variables are kept.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(predvars, data, env): object 'medv' not found</code></pre>
</div>
</div>
<p>The command causes an error because <code>R</code> does not know where to find the variables <code>medv</code> and <code>lstat</code>.</p>
<p>The next line tells <code>R</code> that the variables are in <code>Boston</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Alternatively, we can attach the <code>Boston</code> object:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/attach.html">attach</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">)</span></span>
<span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we type <code>lm.fit</code>, some basic information about the model is output. For more detailed information, we use <code>summary(lm.fit)</code>. This gives us <span class="math inline">\(p\)</span>-values and standard errors for the coefficients, as well as the <span class="math inline">\(R^2\)</span> statistic and <span class="math inline">\(F\)</span>-statistic for the model.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat)

Coefficients:
(Intercept)        lstat  
      34.55        -0.95  </code></pre>
</div>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.168  -3.990  -1.318   2.034  24.500 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***
lstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.216 on 504 degrees of freedom
Multiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 
F-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can use the <code><a href="https://rdrr.io/r/base/names.html">names()</a></code> or the <code><a href="https://rdrr.io/r/utils/str.html">str()</a></code> function in order to find out what other pieces of information are stored in <code>lm.fit</code>.</p>
<p>We can extract these quantities by name—e.g.&nbsp;<code>lm.fit$coefficients</code>.</p>
<p>For some objects, there are also specific extractor functions like <code><a href="https://rdrr.io/r/stats/coef.html">coef()</a></code> to access them.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"        </code></pre>
</div>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)       lstat 
 34.5538409  -0.9500494 </code></pre>
</div>
</div>
<p>In order to obtain a confidence interval for the coefficient estimates, we can use the <code><a href="https://rdrr.io/r/stats/confint.html">confint()</a></code> command.</p>
<p>Type <code>confint(lm.fit)</code> at the command line to obtain the confidence intervals for the linear regression coefficients.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                2.5 %     97.5 %
(Intercept) 33.448457 35.6592247
lstat       -1.026148 -0.8739505</code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function can be used to produce confidence intervals and prediction intervals for the prediction of <code>medv</code> for a given value of <code>lstat</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lm.fit</span>, <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>lstat <span class="op">=</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">10</span>, <span class="fl">15</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>        interval <span class="op">=</span> <span class="st">"confidence"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit      lwr      upr
1 29.80359 29.00741 30.59978
2 25.05335 24.47413 25.63256
3 20.30310 19.73159 20.87461</code></pre>
</div>
</div>
<p>For instance, the observed value of the 95% confidence interval associated with a <code>lstat</code> value of <span class="math inline">\(10,\)</span> i.e.&nbsp;associated with <span class="math display">\[
\beta_0 + \beta_1 \cdot 10
\]</span> is <span class="math display">\[
\operatorname{CI}_{\beta_0 + \beta_1 \cdot 10,obs}=[24.47, 25.63]
\]</span> with observed prediction value <span class="math display">\[
25.05335 = \hat\beta_0 + \hat\beta_1 \cdot 10.
\]</span></p>
<!-- predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), 
        interval = "prediction")
The 95\% prediction interval associated with a `lstat` value of 10 is $(12.828, 37.28)$. 

As expected, the confidence and prediction intervals are centered around the same point (a predicted value of $\hat{y}_i=25.05$ for `medv` when $x_{i2}=$`lstat` equals 10), but the latter are substantially wider.

::: {.callout-caution}
The `predict()` function with the option `interval = "prediction"` assumes **Gaussian** error terms; i.e. 
$$
\epsilon_i\sim\mathcal{N}(0,\sigma^2),\quad\text{for all}\quad i=1,\dots,n.
$$
If this distributional assumption is not fulfilled, you should not use the prediction interval.
::: -->
<p>We will now plot <code>medv</code> and <code>lstat</code> along with the least squares regression line using the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> and <code><a href="https://rdrr.io/r/graphics/abline.html">abline()</a></code> functions.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>There is some evidence for non-linearity in the relationship between <code>lstat</code> and <code>medv</code>. We will explore this issue later in this lab.</p>
<p>The <code><a href="https://rdrr.io/r/graphics/abline.html">abline()</a></code> function can be used to draw any line, not just the least squares regression line. To draw a line with intercept <code>a</code> and slope <code>b</code>, we type <code>abline(a, b)</code>. Below we experiment with some additional settings for plotting lines and points. The <code>lwd = 3</code> command causes the width of the regression line to be increased by a factor of 3; this works for the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> and <code><a href="https://rdrr.io/r/graphics/lines.html">lines()</a></code> functions also. We can also use the <code>pch</code> option to create different plotting symbols.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lm.fit</span>, lwd <span class="op">=</span> <span class="fl">3</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk10-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span>, pch <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk10-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span>, pch <span class="op">=</span> <span class="st">"+"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk10-4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, pch <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk10-5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Next we examine some diagnostic plots. Four diagnostic plots are automatically produced by applying the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> function directly to the output from <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>. In general, this command will produce one plot at a time, and hitting <em>Enter</em> will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the <code><a href="https://rdrr.io/r/graphics/par.html">par()</a></code> and <code>mfrow()</code> functions, which tell <code>R</code> to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, <code>par(mfrow = c(2, 2))</code> divides the plotting region into a <span class="math inline">\(2 \times 2\)</span> grid of panels.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk11-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Alternatively, we can compute the residuals from a linear regression fit using the <code><a href="https://rdrr.io/r/stats/residuals.html">residuals()</a></code> function. The function <code><a href="https://rdrr.io/r/stats/influence.measures.html">rstudent()</a></code> will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk12-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">rstudent</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk12-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>On the basis of the residual plots, there is some evidence of non-linearity.</p>
<p>Leverage statistics can be computed using the <code><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues()</a></code> function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk13-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.max</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>375 
375 </code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/base/which.min.html">which.max()</a></code> function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span>, decreasing <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       375        415        374 
0.02686517 0.02495670 0.02097101 </code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/base/sort.html">sort()</a></code> function can be used to sort and print values of a vector like <code>hatvalues(lm.fit)</code>.</p>
</section><section id="multiple-linear-regression" class="level3" data-number="2.10.3"><h3 data-number="2.10.3" class="anchored" data-anchor-id="multiple-linear-regression">
<span class="header-section-number">2.10.3</span> Multiple Linear Regression</h3>
<p>In order to fit a multiple linear regression model using least squares, we again use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function. The syntax <code>lm(y ~ x1 + x2 + x3)</code> is used to fit a model with three predictors, <code>x1</code>, <code>x2</code>, and <code>x3</code>. The <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function now outputs the regression coefficients for all the predictors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span> <span class="op">+</span> <span class="va">age</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + age, data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.981  -3.978  -1.283   1.968  23.158 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***
lstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***
age          0.03454    0.01223   2.826  0.00491 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.173 on 503 degrees of freedom
Multiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 
F-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code>Boston</code> data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ ., data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.595  -2.730  -0.518   1.777  26.199 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
zn           4.642e-02  1.373e-02   3.382 0.000778 ***
indus        2.056e-02  6.150e-02   0.334 0.738288    
chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
rm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***
age          6.922e-04  1.321e-02   0.052 0.958229    
dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
black        9.312e-03  2.686e-03   3.467 0.000573 ***
lstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.745 on 492 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 
F-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can access the individual components of a summary object by name (type <code><a href="https://rdrr.io/r/stats/summary.lm.html">?summary.lm</a></code> to see what is available). Hence <code>summary(lm.fit)$r.sq</code> gives us the <span class="math inline">\(R^2\)</span>, and <code>summary(lm.fit)$sigma</code> gives us the RSE. The <code><a href="https://rdrr.io/pkg/car/man/vif.html">vif()</a></code> function, part of the <code>car</code> package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The <code>car</code> package is not part of the base <code>R</code> installation so it must be downloaded the first time you use it via the <code><a href="https://rdrr.io/r/utils/install.packages.html">install.packages()</a></code> function in <code>R</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span><span class="op">)</span> <span class="co"># contains the vif() function</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span> <span class="co"># computes the VIF statistics and sorts them</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    chas    black     crim  ptratio       rm       zn    lstat      age 
1.073995 1.348521 1.792192 1.799084 1.933744 2.298758 2.941491 3.100826 
     dis    indus      nox      rad      tax 
3.955945 3.991596 4.393720 7.484496 9.008554 </code></pre>
</div>
</div>
<p>What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, <code>age</code> has a high <span class="math inline">\(p\)</span>-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except <code>age</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="va">age</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ . - age, data = Boston)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.6054  -2.7313  -0.5188   1.7601  26.2243 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***
crim         -0.108006   0.032832  -3.290 0.001075 ** 
zn            0.046334   0.013613   3.404 0.000719 ***
indus         0.020562   0.061433   0.335 0.737989    
chas          2.689026   0.859598   3.128 0.001863 ** 
nox         -17.713540   3.679308  -4.814 1.97e-06 ***
rm            3.814394   0.408480   9.338  &lt; 2e-16 ***
dis          -1.478612   0.190611  -7.757 5.03e-14 ***
rad           0.305786   0.066089   4.627 4.75e-06 ***
tax          -0.012329   0.003755  -3.283 0.001099 ** 
ptratio      -0.952211   0.130294  -7.308 1.10e-12 ***
black         0.009321   0.002678   3.481 0.000544 ***
lstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.74 on 493 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 
F-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Alternatively, the <code><a href="https://rdrr.io/r/stats/update.html">update()</a></code> function can be used.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span><span class="op">(</span><span class="va">lm.fit</span>, <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="va">age</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="interaction-terms" class="level3" data-number="2.10.4"><h3 data-number="2.10.4" class="anchored" data-anchor-id="interaction-terms">
<span class="header-section-number">2.10.4</span> Interaction Terms</h3>
<p>It is easy to include interaction terms in a linear model using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function. The syntax <code>lstat:black</code> tells <code>R</code> to include an interaction term between <code>lstat</code> and <code>black</code>. The syntax <code>lstat * age</code> simultaneously includes <code>lstat</code>, <code>age</code>, and the interaction term <code>lstat</code><span class="math inline">\(\times\)</span><code>age</code> as predictors; it is a shorthand for <code>lstat + age + lstat:age</code>. %We can also pass in transformed versions of the predictors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span> <span class="op">*</span> <span class="va">age</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat * age, data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.806  -4.045  -1.333   2.085  27.552 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***
lstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***
age         -0.0007209  0.0198792  -0.036   0.9711    
lstat:age    0.0041560  0.0018518   2.244   0.0252 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.149 on 502 degrees of freedom
Multiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 
F-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section><section id="non-linear-transformations-of-the-predictors" class="level3" data-number="2.10.5"><h3 data-number="2.10.5" class="anchored" data-anchor-id="non-linear-transformations-of-the-predictors">
<span class="header-section-number">2.10.5</span> Non-linear Transformations of the Predictors</h3>
<p>The <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function can also accommodate non-linear transformations of the predictors. For instance, given a predictor <span class="math inline">\(X\)</span>, we can create a predictor <span class="math inline">\(X^2\)</span> using <code>I(X^2)</code>. The function <code><a href="https://rdrr.io/r/base/AsIs.html">I()</a></code> is needed since the <code>^</code> has a special meaning in a formula object; wrapping as we do allows the standard usage in <code>R</code>, which is to raise <code>X</code> to the power <code>2</code>. We now perform a regression of <code>medv</code> onto <code>lstat</code> and <code>lstat^2</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb99"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">lstat</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + I(lstat^2))

Residuals:
     Min       1Q   Median       3Q      Max 
-15.2834  -3.8313  -0.5295   2.3095  25.4148 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***
lstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***
I(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.524 on 503 degrees of freedom
Multiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 
F-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The near-zero <span class="math inline">\(p\)</span>-value associated with the quadratic term suggests that it leads to an improved model. We use the <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> function to further quantify the extent to which the quadratic fit is superior to the linear fit.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb101"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">lm.fit</span>, <span class="va">lm.fit2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: medv ~ lstat
Model 2: medv ~ lstat + I(lstat^2)
  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
1    504 19472                                 
2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Here Model 1 represents the linear submodel containing only one predictor, <code>lstat</code>, while Model 2 corresponds to the larger quadratic model that has two predictors, <code>lstat</code> and <code>lstat^2</code>. The <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior.</p>
<p>Here the <span class="math inline">\(F\)</span>-statistic is <span class="math inline">\(135\)</span> and the associated <span class="math inline">\(p\)</span>-value is virtually zero. This provides very clear evidence that the model containing the predictors <code>lstat</code> and <code>lstat^2</code> is far superior to the model that only contains the predictor <code>lstat</code>.</p>
<p>This is not surprising, since earlier we saw evidence for non-linearity in the relationship between <code>medv</code> and <code>lstat</code>.</p>
<p>If we type</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb103"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lm.fit2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk22-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>then we see that when the <code>lstat^2</code> term is included in the model, there is little discernible pattern in the residuals.</p>
<p>In order to create a <strong>cubic fit</strong>, we can include a predictor of the form <code>I(X^3)</code>. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function to create the polynomial within <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>. For example, the following command produces a fifth-order polynomial fit:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit5</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ poly(lstat, 5))

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5433  -3.1039  -0.7052   2.0844  27.1153 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***
poly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***
poly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***
poly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***
poly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***
poly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.215 on 500 degrees of freedom
Multiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 
F-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>This suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant <span class="math inline">\(p\)</span>-values in a regression fit.</p>
<p>By default, the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function, the argument <code>raw = TRUE</code> must be used.</p>
<p>Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb106"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">rm</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ log(rm), data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-19.487  -2.875  -0.104   2.837  39.816 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***
log(rm)       54.055      2.739   19.73   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.915 on 504 degrees of freedom
Multiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 
F-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section><section id="qualitative-predictors-1" class="level3" data-number="2.10.6"><h3 data-number="2.10.6" class="anchored" data-anchor-id="qualitative-predictors-1">
<span class="header-section-number">2.10.6</span> Qualitative Predictors</h3>
<p>We will now examine the <code>Carseats</code> data, which is part of the <code>ISLR2</code> library. We will attempt to predict <code>Sales</code> (child car seat sales) in <span class="math inline">\(400\)</span> locations based on a number of predictors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb108"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Carseats</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education
1  9.50       138     73          11        276   120       Bad  42        17
2 11.22       111     48          16        260    83      Good  65        10
3 10.06       113     35          10        269    80    Medium  59        12
4  7.40       117    100           4        466    97    Medium  55        14
5  4.15       141     64           3        340   128       Bad  38        13
6 10.81       124    113          13        501    72       Bad  78        16
  Urban  US
1   Yes Yes
2   Yes Yes
3   Yes Yes
4   Yes Yes
5   Yes  No
6    No Yes</code></pre>
</div>
</div>
<p>The <code>Carseats</code> data includes qualitative predictors such as <code>shelveloc</code>, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor <code>shelveloc</code> takes on three possible values: <em>Bad</em>, <em>Medium</em>, and <em>Good</em>. Given a qualitative variable such as <code>shelveloc</code>, <code>R</code> generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb110"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Sales</span> <span class="op">~</span> <span class="va">.</span> <span class="op">+</span> <span class="va">Income</span><span class="op">:</span><span class="va">Advertising</span> <span class="op">+</span> <span class="va">Price</span><span class="op">:</span><span class="va">Age</span>, </span>
<span>    data <span class="op">=</span> <span class="va">Carseats</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9208 -0.7503  0.0177  0.6754  3.3413 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***
CompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***
Income              0.0108940  0.0026044   4.183 3.57e-05 ***
Advertising         0.0702462  0.0226091   3.107 0.002030 ** 
Population          0.0001592  0.0003679   0.433 0.665330    
Price              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***
ShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***
ShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***
Age                -0.0579466  0.0159506  -3.633 0.000318 ***
Education          -0.0208525  0.0196131  -1.063 0.288361    
UrbanYes            0.1401597  0.1124019   1.247 0.213171    
USYes              -0.1575571  0.1489234  -1.058 0.290729    
Income:Advertising  0.0007510  0.0002784   2.698 0.007290 ** 
Price:Age           0.0001068  0.0001333   0.801 0.423812    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.011 on 386 degrees of freedom
Multiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 
F-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/contrasts.html">contrasts()</a></code> function returns the coding that <code>R</code> uses for the dummy variables.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb112"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/attach.html">attach</a></span><span class="op">(</span><span class="va">Carseats</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/contrasts.html">contrasts</a></span><span class="op">(</span><span class="va">ShelveLoc</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Good Medium
Bad       0      0
Good      1      0
Medium    0      1</code></pre>
</div>
</div>
<p>Use <code><a href="https://rdrr.io/r/stats/contrasts.html">?contrasts</a></code> to learn about other contrasts, and how to set them.</p>
<p><code>R</code> has created a <code>ShelveLocGood</code> dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a <code>ShelveLocMedium</code> dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for <code>ShelveLocGood</code> in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And <code>ShelveLocMedium</code> has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.</p>
</section><section id="writing-functions" class="level3" data-number="2.10.7"><h3 data-number="2.10.7" class="anchored" data-anchor-id="writing-functions">
<span class="header-section-number">2.10.7</span> Writing Functions</h3>
<p>As we have seen, <code>R</code> comes with many useful functions, and still more functions are available by way of <code>R</code> libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the <code>ISLR2</code> and <code>MASS</code> libraries, called <code>LoadLibraries()</code>. Before we have created the function, <code>R</code> returns an error if we try to call it.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb114"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LoadLibraries</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(expr, envir, enclos): object 'LoadLibraries' not found</code></pre>
</div>
<div class="sourceCode" id="cb116"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">LoadLibraries</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in LoadLibraries(): could not find function "LoadLibraries"</code></pre>
</div>
</div>
<p>We now create the function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb118"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LoadLibraries</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span> <span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span></span>
<span> <span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="st">"The libraries have been loaded."</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now if we type in <code>LoadLibraries</code>, <code>R</code> will tell us what is in the function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb119"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LoadLibraries</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>function() {
 library(ISLR2)
 library(MASS)
 print("The libraries have been loaded.")
}</code></pre>
</div>
</div>
<p>If we call the function, the libraries are loaded in and the print statement is output.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb121"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">LoadLibraries</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "The libraries have been loaded."</code></pre>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Cribari_2004" class="csl-entry" role="listitem">
Cribari-Neto, Francisco. 2004. <span>“Asymptotic Inference Under Heteroskedasticity of Unknown Form.”</span> <em>Computational Statistics &amp; Data Analysis</em> 45 (2): 215–33.
</div>
<div id="ref-Long_Ervin_2000" class="csl-entry" role="listitem">
Long, J Scott, and Laurie H Ervin. 2000. <span>“Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.”</span> <em>The American Statistician</em> 54 (3): 217–24.
</div>
<div id="ref-MacKinnon_White_1985" class="csl-entry" role="listitem">
MacKinnon, James G, and Halbert White. 1985. <span>“Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.”</span> <em>Journal of Econometrics</em> 29 (3): 305–25.
</div>
<div id="ref-White1980" class="csl-entry" role="listitem">
White, Halbert. 1980. <span>“A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.”</span> <em>Econometrica</em>, 817–38.
</div>
</div>
</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./Ch1_Intro2R.html" class="pagination-link" aria-label="`R`-Lab: Introduction to `R`">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title"><code>R</code>-Lab: Introduction to <code>R</code></span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch3_MatrixAlgebra.html" class="pagination-link" aria-label="Matrix Algebra">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>