<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>4&nbsp; Linear Regression – Computer-Aided Statistical Analysis (B.Sc.)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch5_Classification.html" rel="next">
<link href="./Ch3_MatrixAlgebra.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch4_LinearRegression.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer-Aided Statistical Analysis (B.Sc.)</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Intro2R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title"><code>R</code>-Lab: Introduction to <code>R</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_StatLearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_MatrixAlgebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_LinearRegression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_Classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6_ResamplingMethods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#sec-LinModAssumptions" id="toc-sec-LinModAssumptions" class="nav-link active" data-scroll-target="#sec-LinModAssumptions"><span class="header-section-number">4.1</span> Assumptions</a></li>
  <li><a href="#deriving-the-expression-of-the-ols-estimator" id="toc-deriving-the-expression-of-the-ols-estimator" class="nav-link" data-scroll-target="#deriving-the-expression-of-the-ols-estimator"><span class="header-section-number">4.2</span> Deriving the Expression of the OLS Estimator</a></li>
  <li><a href="#assessing-the-accuracy-of-the-model-fit-hatf" id="toc-assessing-the-accuracy-of-the-model-fit-hatf" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-model-fit-hatf"><span class="header-section-number">4.3</span> Assessing the Accuracy of the Model Fit <span class="math inline">\(\hat{f}\)</span></a></li>
  <li>
<a href="#assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" id="toc-assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-coefficient-estimators-hatbeta"><span class="header-section-number">4.4</span> Assessing the Accuracy of the Coefficient Estimators <span class="math inline">\(\hat{\beta}\)</span></a>
  <ul class="collapse">
<li><a href="#bias-of-hatbeta" id="toc-bias-of-hatbeta" class="nav-link" data-scroll-target="#bias-of-hatbeta"><span class="header-section-number">4.4.1</span> Bias of <span class="math inline">\(\hat{\beta}\)</span></a></li>
  <li><a href="#standard-error-of-hatbeta_j" id="toc-standard-error-of-hatbeta_j" class="nav-link" data-scroll-target="#standard-error-of-hatbeta_j"><span class="header-section-number">4.4.2</span> Standard Error of <span class="math inline">\(\hat{\beta}_j\)</span></a></li>
  </ul>
</li>
  <li>
<a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference"><span class="header-section-number">4.5</span> Inference</a>
  <ul class="collapse">
<li><a href="#confidence-intervals-for-beta_j" id="toc-confidence-intervals-for-beta_j" class="nav-link" data-scroll-target="#confidence-intervals-for-beta_j"><span class="header-section-number">4.5.1</span> Confidence Intervals for <span class="math inline">\(\beta_j\)</span></a></li>
  <li><a href="#confidence-intervals-for-statistical-hypothesis-testing" id="toc-confidence-intervals-for-statistical-hypothesis-testing" class="nav-link" data-scroll-target="#confidence-intervals-for-statistical-hypothesis-testing"><span class="header-section-number">4.5.2</span> Confidence Intervals for Statistical Hypothesis Testing</a></li>
  <li><a href="#t-test" id="toc-t-test" class="nav-link" data-scroll-target="#t-test"><span class="header-section-number">4.5.3</span> <span class="math inline">\(t\)</span>-Test</a></li>
  <li><a href="#f-test" id="toc-f-test" class="nav-link" data-scroll-target="#f-test"><span class="header-section-number">4.5.4</span> <span class="math inline">\(F\)</span>-Test</a></li>
  <li><a href="#probability-of-a-type-i-error-power-and-consistency" id="toc-probability-of-a-type-i-error-power-and-consistency" class="nav-link" data-scroll-target="#probability-of-a-type-i-error-power-and-consistency"><span class="header-section-number">4.5.5</span> Probability of a Type I Error, Power and Consistency</a></li>
  </ul>
</li>
  <li><a href="#the-omitted-variable-bias" id="toc-the-omitted-variable-bias" class="nav-link" data-scroll-target="#the-omitted-variable-bias"><span class="header-section-number">4.6</span> The Omitted Variable Bias</a></li>
  <li>
<a href="#other-considerations-in-the-regression-model" id="toc-other-considerations-in-the-regression-model" class="nav-link" data-scroll-target="#other-considerations-in-the-regression-model"><span class="header-section-number">4.7</span> Other Considerations in the Regression Model</a>
  <ul class="collapse">
<li><a href="#qualitative-predictors" id="toc-qualitative-predictors" class="nav-link" data-scroll-target="#qualitative-predictors"><span class="header-section-number">4.7.1</span> Qualitative Predictors</a></li>
  <li><a href="#extensions-of-the-linear-model" id="toc-extensions-of-the-linear-model" class="nav-link" data-scroll-target="#extensions-of-the-linear-model"><span class="header-section-number">4.7.2</span> Extensions of the Linear Model</a></li>
  <li><a href="#detecting-potential-problems" id="toc-detecting-potential-problems" class="nav-link" data-scroll-target="#detecting-potential-problems"><span class="header-section-number">4.7.3</span> Detecting Potential Problems</a></li>
  </ul>
</li>
  <li>
<a href="#comparison-linear-regression-vs.-k-nn-regression" id="toc-comparison-linear-regression-vs.-k-nn-regression" class="nav-link" data-scroll-target="#comparison-linear-regression-vs.-k-nn-regression"><span class="header-section-number">4.8</span> Comparison: Linear Regression vs.&nbsp;K-NN Regression</a>
  <ul class="collapse">
<li><a href="#k-nearest-neighbors-k-nn-regression" id="toc-k-nearest-neighbors-k-nn-regression" class="nav-link" data-scroll-target="#k-nearest-neighbors-k-nn-regression"><span class="header-section-number">4.8.1</span> K-Nearest Neighbors (K-NN) Regression</a></li>
  </ul>
</li>
  <li><a href="#assignment" id="toc-assignment" class="nav-link" data-scroll-target="#assignment"><span class="header-section-number">4.9</span> Assignment</a></li>
  <li>
<a href="#self-study-exercises" id="toc-self-study-exercises" class="nav-link" data-scroll-target="#self-study-exercises"><span class="header-section-number">4.10</span> Self-Study: Exercises</a>
  <ul class="collapse">
<li><a href="#solutions" id="toc-solutions" class="nav-link" data-scroll-target="#solutions">Solutions</a></li>
  </ul>
</li>
  <li>
<a href="#self-study-r-lab-linear-regression" id="toc-self-study-r-lab-linear-regression" class="nav-link" data-scroll-target="#self-study-r-lab-linear-regression"><span class="header-section-number">4.11</span> Self-Study: <code>R</code>-Lab Linear Regression</a>
  <ul class="collapse">
<li><a href="#libraries" id="toc-libraries" class="nav-link" data-scroll-target="#libraries"><span class="header-section-number">4.11.1</span> Libraries</a></li>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression"><span class="header-section-number">4.11.2</span> Simple Linear Regression</a></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">4.11.3</span> Multiple Linear Regression</a></li>
  <li><a href="#interaction-terms" id="toc-interaction-terms" class="nav-link" data-scroll-target="#interaction-terms"><span class="header-section-number">4.11.4</span> Interaction Terms</a></li>
  <li><a href="#non-linear-transformations-of-the-predictors" id="toc-non-linear-transformations-of-the-predictors" class="nav-link" data-scroll-target="#non-linear-transformations-of-the-predictors"><span class="header-section-number">4.11.5</span> Non-linear Transformations of the Predictors</a></li>
  <li><a href="#qualitative-predictors-1" id="toc-qualitative-predictors-1" class="nav-link" data-scroll-target="#qualitative-predictors-1"><span class="header-section-number">4.11.6</span> Qualitative Predictors</a></li>
  <li><a href="#writing-functions" id="toc-writing-functions" class="nav-link" data-scroll-target="#writing-functions"><span class="header-section-number">4.11.7</span> Writing Functions</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-linearRegCh" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>This chapter is based on the following references:</p>
<ul>
<li>Chapter 3 of <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a>
</li>
<li>Chapter 1 of Econometrics by Fumio Hayashi</li>
</ul>
<section id="sec-LinModAssumptions" class="level2" data-number="4.1"><h2 data-number="4.1" class="anchored" data-anchor-id="sec-LinModAssumptions">
<span class="header-section-number">4.1</span> Assumptions</h2>
<p>The (multiple) linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:</p>
<p><strong>Assumption 1: Model and Sampling</strong></p>
<p><strong>Part (a): Linear Model</strong></p>
<p><span id="eq-LinMod"><span class="math display">\[
\begin{align}
  Y_i= \underbrace{\sum_{k=0}^p\beta_k X_{ik}}_{=f(X_i)}+\epsilon_i, \quad i=1,\dots,n,
\end{align}
\qquad(4.1)\]</span></span> where <span class="math display">\[
X_{i0}=1
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span> is called “dependent variable” or “outcome variable” or “regressand”</li>
<li>
<span class="math inline">\(X_{ik}\)</span> is called the <span class="math inline">\(k\)</span>th “independent variable” or “predictor variable” or “regressor” or “explanatory variable” or “control variable.”</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> is the error term.</li>
</ul>
<p>It is convenient to write <a href="#eq-LinMod" class="quarto-xref">Equation&nbsp;<span>4.1</span></a> using matrix notation <span class="math display">\[
\begin{eqnarray*}
  Y_i&amp;=&amp;\underset{(1\times (p+1))}{X_i'}\underset{((p+1)\times 1)}{\beta} +\epsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}
\]</span> where <span class="math display">\[
X_i=\left(\begin{matrix}X_{i0}\\ \vdots\\  X_{ip}\end{matrix}\right)
  \quad\text{and}\quad
\beta=\left(\begin{matrix}\beta_0\\ \vdots\\ \beta_p\end{matrix}\right).
\]</span> Stacking all individual rows, <span class="math inline">\(X_i',\)</span> <span class="math inline">\(i=1,\dots,n\)</span> leads to <span class="math display">\[
\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&amp;=&amp;\underset{(n\times (p+1))}{X}\underset{((p+1)\times 1)}{\beta} + \underset{(n\times 1)}{\epsilon},
\end{eqnarray*}
\]</span> where <span class="math display">\[
\begin{equation*}
Y=\left(\begin{matrix}Y_1
\\ \vdots\\
Y_n
\end{matrix}\right),
\quad
X=\left(\begin{matrix}
X_{10}&amp;\dots&amp;X_{1p}\\
\vdots&amp;\ddots&amp;\vdots\\
X_{n0}&amp;\dots&amp;X_{np}\\
\end{matrix}\right),
\quad
\text{and}
\quad
\epsilon=
\left(\begin{matrix}
\epsilon_1\\ \vdots\\
\epsilon_n
\end{matrix}\right).
\end{equation*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Simple Linear Regression and Polynomial Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>The special case of <span class="math inline">\(p=1\)</span> <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i
\]</span> is called the <strong><em>simple</em> linear regression model</strong>. With the simple linear regression model, only straight line fits are possible.</p>
<p>By contrast, with the multiple linear regression model, we can also fit polynomials. For instance, we can define <span class="math display">\[
X_{i2} := X_{i1}^2
\]</span> which leads to a quadratic regression model (often used for life-cycle analyses that include the predictor <code>Age</code><span class="math inline">\(_i=X_{i1}\)</span>) <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i1}^2 + \epsilon_i.
\]</span> Of course, further predictor variables <span class="math inline">\(X_{i2},\dots,X_{ip}\)</span> can (and should) be added to this model.</p>
<p>The same logic applies to polynomials with higher polynomial degrees <span class="math inline">\((\geq 2).\)</span> Large polynomial degrees, however, can lead to unstable estimation results.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The assumption <span class="math inline">\(f(X_i) = X_i'\beta\)</span> may be a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true (unknown) relationship is that simple.</p>
</div>
</div>
<p><strong>Part (b): Random Sample</strong></p>
<p>We assume that the <span class="math inline">\(n\)</span> observed data points <span class="math display">\[
((y_{1},x_{10},\dots,x_{1p}),\dots,(y_{n},x_{n0},\dots,x_{np}))
\]</span> are a realization of the <strong>training data random sample</strong> <span class="math display">\[
((Y_{1},X_{10},\dots,X_{1p}),\dots,(Y_{n},X_{n0},\dots,X_{np})).
\]</span></p>
<p>That is, the <span class="math inline">\(i\)</span>th observed <span class="math inline">\(p+2\)</span> dimensional data point <span class="math display">\[
(y_{i},x_{i0},\dots,x_{ip})\in\mathbb{R}^{p+2}
\]</span> is a realization of a <span class="math inline">\(p+2\)</span> dimensional random variable <span class="math display">\[
(Y_{i},X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+2},
\]</span> where</p>
<ol type="1">
<li>
<span class="math inline">\((Y_{i},X_{i0},\dots,X_{ip})\)</span> has the identical <span class="math inline">\(p+2\)</span> dimensional distribution for all <span class="math inline">\(i=1,\dots,n.\)</span>
</li>
<li>
<span class="math inline">\((Y_{i},X_{i0},\dots,X_{ip})\)</span> is independent of <span class="math inline">\((Y_{j},X_{j0},\dots,X_{jp})\)</span> for all <span class="math inline">\(i\neq j=1,\dots,n.\)</span>
</li>
</ol>
<!-- 
::: {.callout-note}
Due to @eq-LinMod, this i.i.d. assumption is equivalent to assuming that the multivariate random variables 
$$
(\epsilon_i,X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+2}
$$ 
are i.i.d. across $i=1,\dots,n$. 
::: 
--><div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Remark:</strong> Often, we do not use a different notation for observed realizations <span class="math inline">\((y_{i},x_{i0},\dots,x_{ip})\in\mathbb{R}^{p+2}\)</span> and for the corresponding random variable <span class="math inline">\((Y_{i},X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+2}\)</span> since often both interpretations (random variable and its realizations) can make sense in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.</p>
</div>
</div>
<p><strong>Assumption 2: Exogeneity</strong> <span id="eq-assExogen"><span class="math display">\[
E(\epsilon_i|X_i)=0,\quad i=1,\dots,n
\qquad(4.2)\]</span></span></p>
<p>This assumption demands that the mean of the random error term <span class="math inline">\(\epsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_i\)</span>. This exogeneity assumption is also called</p>
<ul>
<li>“orthogonality assumption” or</li>
<li>“mean independence assumption.”</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Together with the random sample assumption (Assumption 1, Part (b)) <a href="#eq-assExogen" class="quarto-xref">Equation&nbsp;<span>4.2</span></a> even implies <strong>strict exogeneity</strong> <span class="math display">\[
E(\epsilon|X) = \underset{(n\times 1)}{0},
\]</span> since we have independence across <span class="math inline">\(i=1,\dots,n\)</span>. Under strict exogeneity, the mean of the random error <strong>vector</strong> <span class="math inline">\(\epsilon\in\mathbb{R}^n\)</span> is zero irrespective of the realizations of the <span class="math inline">\((n\times (p+1))\)</span>-dimensional random predictor matrix <span class="math inline">\(X.\)</span></p>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Independence between error term and predictors
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let</p>
<ul>
<li>
<span class="math inline">\(E(\epsilon_i)=0\)</span> and</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> be independent of <span class="math inline">\(X_i\)</span>
</li>
</ul>
<p>Here the assumption of exogeneity is fulfilled since by the independence between <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_i\)</span> we have that <span class="math display">\[
E(\epsilon_i|X_i) = E(\epsilon_i)
\]</span> and by assumption <span class="math inline">\(E(\epsilon_i)=0\)</span> such that <span class="math display">\[
E(\epsilon_i|X_i) = 0.
\]</span></p>
<p>Note: The assumption <span class="math inline">\(E(\epsilon_i)=0\)</span> is not critical (i.e.&nbsp;not restrictive) due to the intercept term in <a href="#eq-assExogen" class="quarto-xref">Equation&nbsp;<span>4.2</span></a>.</p>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Examples: Heteroskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let</p>
<ul>
<li>
<span class="math inline">\(\epsilon_i\sim\mathcal{N}(0,\sigma_i^2),\)</span> where</li>
<li><span class="math inline">\(\sigma_i = |X_{i1}|\)</span></li>
</ul>
<p>Here the assumption of exogeneity is fulfilled since realizations of <span class="math inline">\(X_i\)</span> do not affect the mean of <span class="math inline">\(\epsilon_i,\)</span> thus <span class="math display">\[
E(\epsilon_i|X_i) = 0.
\]</span></p>
<p>However, <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_i\)</span> are not independent of each other, since the conditional variance of <span class="math inline">\(\epsilon_i\)</span> is a function of <span class="math inline">\(X_{i1}\)</span> <span class="math display">\[
Var(\epsilon_i|X_i) = |X_{i1}|^2.
\]</span></p>
</div>
</div>
<p><strong>Assumption 3: Rank Condition (no perfect multicollinearity)</strong></p>
<p><span class="math display">\[
\begin{align*}
\operatorname{rank}(X)&amp;=p+1\quad\text{a.s.}\\[2ex]
\Leftrightarrow P\big(\operatorname{rank}(X)&amp;= p+1 \big)=1
\end{align*}
\]</span> This assumption demands that, with probability one, no predictor variable <span class="math inline">\(X_{k}\in\mathbb{R}^n\)</span> is linearly dependent of the others. (This is the literal translation of the “almost surely (a.s.)” concept.)</p>
<p><strong>Note:</strong> The assumption implies that <span class="math inline">\(n\geq (p+1),\)</span> since <span class="math display">\[
\operatorname{rank}(X)\leq \min\{n,(p+1)\}\quad(a.s.)
\]</span></p>
<p>This rank assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation). The violation of this assumption harms any economic interpretation since we cannot disentangle the explanatory variables’ individual effects on <span class="math inline">\(Y\)</span>. Therefore, this assumption is also often called an <strong>identification assumption</strong>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Under Assumption 3, we have that <span class="math display">\[
\operatorname{rank}(X)=(p+1)\quad\text{(a.s.)}
\]</span></p></li>
<li><p>This implies that the <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix <span class="math inline">\(X'X\)</span> has full rank, i.e.&nbsp;that <span class="math display">\[
\operatorname{rank}(X'X)=(p+1)\quad\text{(a.s.)}
\]</span></p></li>
<li><p>Thus <span class="math inline">\((X'X)\)</span> is invertible; i.e.&nbsp;there exists a <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix <span class="math inline">\((X'X)^{-1}\)</span> such that <span class="math display">\[
(X'X)(X'X)^{-1} = (X'X)^{-1}(X'X) = I_{(p+1)}.
\]</span></p></li>
</ul>
</div>
</div>
<p><strong>Assumption 4: Error distribution</strong></p>
<p>There are different more or less restrictive assumptions. Some of the most common ones are the following:</p>
<ul>
<li>
<strong>Conditional distribution with sufficiently many moments:</strong> <span class="math display">\[
\epsilon_i|X_i \sim f_{\epsilon|X}
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for any distribution <span class="math inline">\(f_{\epsilon|X}\)</span> with two (or more) finite moments.
<ul>
<li>
<strong>Example: Conditional normal distribution</strong> <span class="math display">\[
\epsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span>.</li>
</ul>
</li>
<li>
<strong>Independence between error and predictors:</strong> <span class="math inline">\(\epsilon_i\sim f_\epsilon\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> such that <span class="math inline">\(f_\epsilon=f_{\epsilon|X}\)</span> and such that <span class="math inline">\(f_\epsilon\)</span> has two (or more) finite moments.
<ul>
<li>
<strong>Example: Independence between a Gaussian error and the predictors</strong> <span class="math display">\[
f_\epsilon=\mathcal{N}(0,\sigma^2),
\]</span> where <span class="math inline">\(\sigma^2\)</span> does not depend on <span class="math inline">\(X.\)</span>
</li>
</ul>
</li>
<li>
<strong>Spherical errors:</strong> The conditional distributions of <span class="math inline">\(\epsilon_i|X_i\)</span> may generally depend on <span class="math inline">\(X_i\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> but only such that <span class="math display">\[
E(\epsilon|X)=\underset{(n\times 1)}{0}
\]</span> and <span class="math display">\[
\begin{align*}
&amp;\underset{(n\times n)}{Var\left(\epsilon|X\right)}=\\[2ex]
&amp; = \left(\begin{matrix}
Var(\epsilon_1|X)&amp;Cov(\epsilon_1,\epsilon_2|X)&amp;\dots&amp;Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&amp;Var(\epsilon_2|X)&amp;\dots&amp;Cov(\epsilon_2,\epsilon_n|X)\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
Cov(\epsilon_n,\epsilon_1|X)&amp;Cov(\epsilon_n,\epsilon_2|X)&amp;\dots&amp;Var(\epsilon_n|X)
\end{matrix}\right)\\[2ex]
&amp; = \left(\begin{matrix}
\sigma^2&amp;0&amp;\dots&amp;0\\
0&amp;\sigma^2&amp;\dots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\dots&amp;\sigma^2
\end{matrix}\right)
= \sigma^2 I_n,
\end{align*}
\]</span> where <span class="math inline">\(I_n\)</span> denotes the <span class="math inline">\((n\times n)\)</span> identity matrix with ones on the diagonal and zeros else. Thus, under the spherical errors assumption, one has, for all possible realizations of <span class="math inline">\(X\)</span>, that:
<ul>
<li>
<strong>uncorrelated:</strong> <span class="math inline">\(Cov(\epsilon_i,\epsilon_j|X)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and all <span class="math inline">\(j=1,\dots,n\)</span> such that <span class="math inline">\(i\neq j\)</span>
</li>
<li>
<strong>homoskedastic:</strong> <span class="math inline">\(Var(\epsilon_i|X)=\sigma^2\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>
</li>
</ul>
</li>
</ul>
<p>All four Assumptions 1-4 must hold for doing inference using the (multiple) linear regression model.</p>
<section id="homoskedastic-versus-heteroskedastic-error-terms" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="homoskedastic-versus-heteroskedastic-error-terms">Homoskedastic versus Heteroskedastic Error Terms</h4>
<p>The i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\((X_{i0},\dots,X_{ip})\in\mathbb{R}^{p+1}\)</span>. That is, the error term <span class="math inline">\(\epsilon_i\)</span> can have a conditional distribution which depends on <span class="math inline">\((X_{i0},\dots,X_{ip}).\)</span></p>
<p>The exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of <span class="math inline">\(\epsilon_i\)</span> is independent of <span class="math inline">\(X_i\)</span>. Besides this, dependencies between <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(X_{i0},\dots,X_{ip}\)</span> are allowed. For instance, the variance of <span class="math inline">\(\epsilon_i\)</span> can be a function of <span class="math inline">\(X_{i0},\dots,X_{ip}.\)</span> If this is the case, <span class="math inline">\(\epsilon_i\)</span> is said to be <strong>“heteroskedastic.”</strong></p>
<ul>
<li><p><strong>Heteroskedastic error terms:</strong> The conditional variances <span class="math display">\[
Var(\epsilon_i|X_i=x_i)=\sigma^2(x_i)
\]</span> are a non-constant function <span class="math inline">\(\sigma^2(x_i)&gt;0\)</span> of the realizations <span class="math inline">\(X_i=x_i.\)</span></p></li>
<li><p><strong>Homoskedastic error terms:</strong> The conditional variances <span class="math display">\[
Var(\epsilon_i|X_i=x_i)=\sigma^2
\]</span> are constant <span class="math inline">\(\sigma^2&gt;0\)</span> for every possible realization <span class="math inline">\(X_i=x_i.\)</span></p></li>
</ul>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Heteroskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\epsilon_i|X_i\sim \mathcal{U}[-0.5|X_{i2}|, 0.5|X_{i2}|],
\]</span> with <span class="math display">\[
X_{i2}\sim \mathcal{U}[-4,4]
\]</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> where <span class="math inline">\(\mathcal{U}[a,b]\)</span> denotes the uniform distribution over <span class="math inline">\([a,b].\)</span></p>
<p>This error term is mean independent of <span class="math inline">\(X_i\)</span> since <span class="math inline">\(E(\epsilon_i|X_i)=0\)</span>, but it has a heteroskedastic conditional variance since <span class="math display">\[
Var(\epsilon_i|X_i)=\frac{1}{12}X_{i2}^2
\]</span> depends on <span class="math inline">\(X_{i2}.\)</span></p>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Homoskedastic Error
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\epsilon_i\sim{\mathcal N} (0, \sigma^2)
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span> Here, the conditional variance of the error terms <span class="math inline">\(\epsilon_i\)</span> given <span class="math inline">\(X_i\)</span> <span class="math display">\[
Var(\epsilon_i|X_i)=Var(\epsilon_i)=\sigma^2
\]</span> are equal to the constant <span class="math inline">\(\sigma^2&gt;0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for every possible realization of <span class="math inline">\(X_i.\)</span></p>
</div>
</div>
</section></section><section id="deriving-the-expression-of-the-ols-estimator" class="level2" data-number="4.2"><h2 data-number="4.2" class="anchored" data-anchor-id="deriving-the-expression-of-the-ols-estimator">
<span class="header-section-number">4.2</span> Deriving the Expression of the OLS Estimator</h2>
<p>We derive the expression for the OLS estimator <span class="math display">\[
\hat\beta=(\hat\beta_0,\dots,\hat\beta_p)'\in\mathbb{R}^{p+1}
\]</span> as the vector-valued minimizing argument of the sum of squared residuals, <span class="math display">\[
\operatorname{RSS}(b)=\sum_{i=1}^n\big(\underbrace{Y_i-X_i'b}_{\text{$i$th residual}}\big)^2
\]</span> with <span class="math inline">\(b\in\mathbb{R}^(p+1)\)</span>, for a given sample <span class="math display">\[
((Y_1,X_1),\dots,(Y_n,X_n)).
\]</span></p>
<p>Using matrix/vector notation we can write <span class="math inline">\(S_n(b)\)</span> as <span class="math display">\[
\begin{align*}
\operatorname{RSS}(b)
&amp;=\sum_{i=1}^n(Y_i-X_i'b)^2\\[2ex]
&amp;=(Y-X b)^{\prime}(Y-X b)\\[2ex]
&amp;=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}
\]</span> To find the minimizing argument <span class="math display">\[
\hat\beta = \arg\min_{b\in\mathbb{R}^{p+1}}\operatorname{RSS}(b)
\]</span> we compute the vector containing all partial derivatives <span class="math display">\[
\begin{align*}
\underset{((p+1)\times 1)}{\frac{\partial \operatorname{RSS}(b)}{\partial b}} &amp;=-2\left(X^{\prime}Y -X^{\prime} Xb\right).
\end{align*}
\]</span> Setting each partial derivative to zero leads to <span class="math inline">\((p+1)\)</span> linear equations (“normal equations”) in <span class="math inline">\((p+1)\)</span> unknowns. This linear system of equations defines the OLS estimates, <span class="math inline">\(\hat{\beta}\)</span>, for a given dataset: <span class="math display">\[
\begin{align*}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)
&amp;=\underset{((p+1)\times 1)}{0}\\[2ex]
X^{\prime} X\hat{\beta}
&amp;=\underset{((p+1)\times 1)}{X^{\prime}Y}.
\end{align*}
\]</span> From our full rank assumption (Assumption 3) it follows that <span class="math inline">\(X^{\prime}X\)</span> is an invertible <span class="math inline">\(((p+1)\times (p+1))\)</span>-dimensional matrix which allows us to solve the equation system by <span class="math display">\[
\begin{align*}
\underset{((p+1)\times 1)}{\hat{\beta}} &amp;=\left(X^{\prime} X\right)^{-1} X^{\prime} Y.
\end{align*}
\]</span></p>
<p>The following codes computes the estimate <span class="math inline">\(\hat{\beta}\)</span> for a given dataset with <span class="math inline">\(X_i\in\mathbb{R}^{p+1}\)</span>, <span class="math inline">\(p=2\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Some given data</span></span>
<span><span class="va">X_1</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.9</span>,<span class="fl">0.8</span>,<span class="fl">1.1</span>,<span class="fl">0.1</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">4.4</span>,<span class="fl">4.6</span>,<span class="fl">1.6</span>,<span class="fl">5.5</span>,<span class="fl">3.4</span><span class="op">)</span></span>
<span><span class="va">X_2</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">66</span>, <span class="fl">62</span>, <span class="fl">64</span>, <span class="fl">61</span>, <span class="fl">63</span>, <span class="fl">70</span>, <span class="fl">68</span>, <span class="fl">62</span>, <span class="fl">68</span>, <span class="fl">66</span><span class="op">)</span></span>
<span><span class="va">Y</span>       <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.7</span>,<span class="op">-</span><span class="fl">1.0</span>,<span class="op">-</span><span class="fl">0.2</span>,<span class="op">-</span><span class="fl">1.2</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">3.4</span>,<span class="fl">0.0</span>,<span class="fl">0.8</span>,<span class="fl">3.7</span>,<span class="fl">2.0</span><span class="op">)</span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="st">"X_1"</span> <span class="op">=</span> <span class="va">X_1</span>, <span class="st">"X_2"</span> <span class="op">=</span> <span class="va">X_2</span>, <span class="st">"Y"</span> <span class="op">=</span> <span class="va">Y</span><span class="op">)</span></span>
<span><span class="co">## Compute the OLS estimation</span></span>
<span><span class="va">lmobj</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span>, data <span class="op">=</span> <span class="va">dataset</span><span class="op">)</span></span>
<span><span class="co">## Plot sample regression surface</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"scatterplot3d"</span><span class="op">)</span> <span class="co"># library for 3d plots</span></span>
<span><span class="va">plot3d</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/scatterplot3d/man/scatterplot3d.html">scatterplot3d</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">X_1</span>, y <span class="op">=</span> <span class="va">X_2</span>, z <span class="op">=</span> <span class="va">Y</span>,</span>
<span>            angle <span class="op">=</span> <span class="fl">33</span>, scale.y <span class="op">=</span> <span class="fl">0.8</span>, pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>            color <span class="op">=</span><span class="st">"red"</span>, </span>
<span>            xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,</span>
<span>            ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>,</span>
<span>            main <span class="op">=</span><span class="st">"OLS Regression Surface"</span><span class="op">)</span></span>
<span><span class="va">plot3d</span><span class="op">$</span><span class="fu">plane3d</span><span class="op">(</span><span class="va">lmobj</span>, lty.box <span class="op">=</span> <span class="st">"solid"</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.5</span><span class="op">)</span>, draw_polygon<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="special-case-simple-linear-regression-model" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="special-case-simple-linear-regression-model">Special Case: Simple Linear Regression Model</h4>
<p>Let’s consider the simple linear regression model <span class="math inline">\((p=1)\)</span> <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i,\qquad i=1,\dots,n.
\]</span> For this case, the expression of the OLS-estimators simplifies. For a given observed realization of the training data random sample <span class="math display">\[
(x_1,y_1),\dots,(x_n,y_n)
\]</span> the values of the OLS estimator <span class="math display">\[
\hat\beta =
\begin{pmatrix}
\hat\beta_0\\
\hat\beta_1
\end{pmatrix}
\]</span> are given by <span class="math display">\[
\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
\]</span> and <span class="math display">\[
\hat\beta_0=\bar{y} - \hat\beta_1\bar{x},
\]</span> where <span class="math inline">\(\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i\)</span> and <span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i\)</span>.</p>
<p><img src="images/Fig_3_1.png" class="img-fluid"></p>
<!-- 
![](images/Fig_3_2.png) 
-->
</section><section id="some-quantities-of-interest" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="some-quantities-of-interest">Some Quantities of Interest</h4>
<p><strong>Predicted values and residuals.</strong></p>
<ul>
<li><p>The (OLS) <strong>predicted values</strong>: <span class="math display">\[
\hat{Y}_i=X_i'\hat\beta, \quad i=1,\dots,n
\]</span> The <span class="math inline">\((n\times 1)\)</span> vector of predicted values <span class="math display">\[
\begin{align*}
\hat{Y} = \left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)
&amp;=X\hat{\beta}\\[-2ex]
&amp;=\underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&amp;=P_X Y
\end{align*}
\]</span></p></li>
<li><p>The (OLS) <strong>residuals</strong>: <span class="math display">\[
e_i=Y_i-\hat{Y}_i, \quad i=1,\dots,n
\]</span> The <span class="math inline">\((n\times 1)\)</span> vector of residuals <span class="math display">\[
\begin{align*}
e =
\left(\begin{matrix}e_1\\e_2\\ \vdots\\ e_n\end{matrix}\right)
&amp;=
\left(\begin{matrix}Y_1\\[.5ex]Y_2\\[.5ex] \vdots\\[.5ex] Y_n\end{matrix}\right)-
\left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)\\[2ex]
&amp;=Y - \hat{Y}\\[2ex]
%&amp;=Y - X\hat{\beta}\\[-2ex]
%&amp;=Y - \underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&amp;=Y - P_X Y\\[2ex]
&amp;=\underbrace{(I_n - P_X)}_{=M_X} Y\\[2ex]
&amp;=M_XY
\end{align*}
\]</span></p></li>
</ul>
<p><strong>Projection matrices.</strong></p>
<p>The matrix <span class="math display">\[
P_X=X(X'X)^{-1}X'
\]</span> is the <span class="math inline">\((n\times n)\)</span> <strong>projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the column space spanned by the column vectors of <span class="math inline">\(X\)</span> and <span class="math display">\[
M_X=I_n-X(X'X)^{-1}X'=I_n-P_X
\]</span> is the associated <span class="math inline">\((n\times n)\)</span> <strong>orthogonal projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the vector space that is orthogonal to that spanned by the column vectors of <span class="math inline">\(X.\)</span></p>
</section></section><section id="assessing-the-accuracy-of-the-model-fit-hatf" class="level2" data-number="4.3"><h2 data-number="4.3" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-model-fit-hatf">
<span class="header-section-number">4.3</span> Assessing the Accuracy of the Model Fit <span class="math inline">\(\hat{f}\)</span>
</h2>
<p>The larger the proportion of the explained variance, the better is the fit of the estimated model <span class="math inline">\(\hat{f}\)</span> to the training data. This motivates the definition of the so-called <span class="math inline">\(R^2\)</span> coefficient of determination: <span class="math display">\[
\begin{eqnarray*}
R^2
%&amp;=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&amp;=1-\frac{\sum_{i=1}^ne_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\\[2ex]
&amp;=1-\frac{\operatorname{RSS}}{\operatorname{TSS}}
\end{eqnarray*}
\]</span> with <span class="math display">\[
\begin{align*}
\operatorname{RSS}\equiv \operatorname{RSS}(\hat\beta)=\sum_{i=1}^n\left(y_i-x_i'\hat\beta\right)^2=\sum_{i=1}^ne_i^2
\end{align*}
\]</span> and <span class="math display">\[
\begin{align*}
\operatorname{TSS}=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2.
\end{align*}
\]</span></p>
<p><span class="math inline">\(\operatorname{TSS}\)</span> “Total Sum of Squares”</p>
<p><span class="math inline">\(\operatorname{RSS}\)</span> “Residual Sum of Squares”</p>
<ul>
<li><p>Obviously, we have that <span class="math inline">\(0\leq R^2\leq 1\)</span>.</p></li>
<li><p>The closer <span class="math inline">\(R^2\)</span> lies to <span class="math inline">\(1\)</span>, the better is the fit of the model to the observed training data.</p></li>
</ul>
<p>In tendency an accurate model has …</p>
<ul>
<li><p>a low residual standard error <span class="math inline">\(\operatorname{RSE}\)</span> <span class="math display">\[
\operatorname{RSE}=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}
\]</span></p></li>
<li><p>a high <span class="math inline">\(R^2\)</span></p></li>
</ul>
<p><span class="math display">\[
R^2=\frac{\operatorname{TSS}-\operatorname{RSS}}{\operatorname{TSS}}=1-\frac{\operatorname{RSS}}{\operatorname{TSS}},
\]</span> where <span class="math inline">\(0\leq R^2\leq 1.\)</span></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Cautionary Note Nr 1:</strong></p>
<p>Do not forget that there is a <strong>irreducible error</strong> <span class="math inline">\(Var(\epsilon)=\sigma^2&gt;0\)</span>. Thus</p>
<ul>
<li>very low <span class="math inline">\(\operatorname{RSE}\)</span> values <span class="math inline">\((\operatorname{RSE}\approx 0)\)</span> and</li>
<li>very high <span class="math inline">\(R^2\)</span> values <span class="math inline">\((R^2\approx 1)\)</span>
</li>
</ul>
<p>can be warning signals indicating overfitting. While overfitting typically does not happen with a simple linear regression model, it can happen with multiple linear regression models containing a lot of parameters (large <span class="math inline">\(p\)</span>).</p>
<p><strong>Cautionary Note Nr 2:</strong></p>
<p>The <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\operatorname{RSE}\)</span> are only based on <strong>training data</strong>. In <a href="Ch2_StatLearning.html" class="quarto-xref"><span>Chapter 2</span></a>, we have seen that a proper assessment of the model accuracy needs to take into account <strong>test data</strong>.</p>
</div>
</div>
<section id="r2-and-correlation-coefficient" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="r2-and-correlation-coefficient">
<span class="math inline">\(R^2\)</span> and correlation coefficient</h4>
<p>In the case of the simple linear regression model, <span class="math inline">\(R^2\)</span> equals the squared sample correlation coefficient between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, <span class="math display">\[
R^2 = r_{yx_1}^2,
\]</span> where <span class="math display">\[
r_{yx_1}=\frac{\sum_{i=1}^n(x_{i1}-\bar{x}_1)(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_{i1}-\bar{x}_1)^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}},
\]</span> where <span class="math inline">\(\bar{x}_1=n^{-1}\sum_{i=1}^nx_{i1}.\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the multiple linear regression model <span class="math inline">\(Y_i=\beta_0+\sum_{j=1}^p\beta_jX_{ij}+\epsilon_i,\)</span> the <span class="math inline">\(R^2\)</span> equals the squared correlation between response and the fitted values: <span class="math display">\[
R^2=r^2_{y\hat{y}}
\]</span> with <span class="math display">\[
r_{y\hat{y}}=\frac{\sum_{i=1}^n(y_i-\bar{y})(\hat{y}_i-\bar{\hat{y}})}{\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}\sqrt{\sum_{i=1}^n(\hat{y}_i-\bar{\hat{y}})^2}},
\]</span> where <span class="math inline">\(\bar{y}=n^{-1}\sum_{i=1}^ny_{i}.\)</span></p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>A high/low <span class="math inline">\(R^2\)</span> value only means that the predictors have high/low <em>predictive power</em> with respect to the training data.</p></li>
<li><p>A high/low <span class="math inline">\(R^2\)</span> does not mean a validation/falsification of the estimated model. Any econometric model needs a plausible explanation from relevant economic theory.<br></p></li>
</ul>
</div>
</div>
<p>The most often criticized disadvantage of the <span class="math inline">\(R^2\)</span> is that additional regressors (relevant or not) will increase the <span class="math inline">\(R^2\)</span>. The below <code>R</code>-codes demonstrates this problem.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>     <span class="op">&lt;-</span> <span class="fl">100</span>                  <span class="co"># Sample size</span></span>
<span><span class="va">X</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>      <span class="co"># Relevant X variable</span></span>
<span><span class="va">X_ir</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">5</span>, <span class="fl">20</span><span class="op">)</span>      <span class="co"># Irrelevant X variable</span></span>
<span><span class="va">error</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">*</span><span class="fl">10</span>    <span class="co"># True (usually unknown) error</span></span>
<span><span class="va">Y</span>     <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">error</span>    <span class="co"># Y variable</span></span>
<span><span class="va">lm1</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>     <span class="co"># Correct OLS regression </span></span>
<span><span class="va">lm2</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">+</span><span class="va">X_ir</span><span class="op">)</span><span class="op">)</span><span class="co"># OLS regression with X_ir </span></span>
<span><span class="va">lm1</span><span class="op">$</span><span class="va">r.squared</span> <span class="op">&lt;</span> <span class="va">lm2</span><span class="op">$</span><span class="va">r.squared</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>So, <span class="math inline">\(R^2\)</span> increases here even though <code>X_ir</code> is a completely irrelevant explanatory variable.</p>
<p>Because of this, the <span class="math inline">\(R^2\)</span> cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called <strong>adjusted</strong> <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\overline{R}^2,\)</span> defined as <span class="math display">\[
\begin{eqnarray*}
  \overline{R}^2&amp;=&amp;1-\frac{\frac{1}{n-(p+1)}\sum_{i=1}^ne^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}\leq R^2%\\
\end{eqnarray*}
\]</span> The adjustment is in terms of the degrees of freedom <span class="math inline">\(n-(p+1)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lm1</span><span class="op">$</span><span class="va">adj.r.squared</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># model without X_ir</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.569</code></pre>
</div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lm2</span><span class="op">$</span><span class="va">adj.r.squared</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># model with X_ir</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.564</code></pre>
</div>
</div>
</section></section><section id="assessing-the-accuracy-of-the-coefficient-estimators-hatbeta" class="level2" data-number="4.4"><h2 data-number="4.4" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-coefficient-estimators-hatbeta">
<span class="header-section-number">4.4</span> Assessing the Accuracy of the Coefficient Estimators <span class="math inline">\(\hat{\beta}\)</span>
</h2>
<section id="bias-of-hatbeta" class="level3" data-number="4.4.1"><h3 data-number="4.4.1" class="anchored" data-anchor-id="bias-of-hatbeta">
<span class="header-section-number">4.4.1</span> Bias of <span class="math inline">\(\hat{\beta}\)</span>
</h3>
<p>Under the Assumptions 1-4, one can show that the OLS estimator <span class="math display">\[
\hat\beta = (X'X)^{-1}X'Y
\]</span> is unbiased, i.e. <span id="eq-OLSUnbiased"><span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta)
&amp; = E(\hat\beta) - \beta \\[2ex]
&amp; = \beta  - \beta \\[2ex]
&amp; = \underset{(p+1)\times 1}{0}.
\end{align*}
\qquad(4.3)\]</span></span> That is, in the mean <span class="math inline">\(\hat\beta\)</span> equals <span class="math inline">\(\beta.\)</span></p>
<p><a href="#eq-OLSUnbiased" class="quarto-xref">Equation&nbsp;<span>4.3</span></a> can be shown as following:</p>
<p>Observe that <span class="math display">\[
\hat\beta=(X'X)^{-1}X'Y
\]</span> consists of two multivariate random variables; namely</p>
<ul>
<li>
<span class="math inline">\(X\in\mathbb{R}^{n\times(p+1)}\)</span> and</li>
<li><span class="math inline">\(Y\in\mathbb{R}^n.\)</span></li>
</ul>
<p>Thus we firstly need to derive the <strong>conditional mean</strong> <span class="math inline">\(E(\hat\beta|X),\)</span> which effectively allows us to focus on randomness due to <span class="math inline">\(\epsilon,\)</span><br><span class="math display">\[
\begin{align*}
E(\hat\beta|X)
&amp;= E((X'X)^{-1}X'Y|X) \\[2ex]
&amp;\text{Using that (by Assumption 1 Part (a)) $Y=X\beta + \epsilon$:}\\[2ex]
&amp;= E((X'X)^{-1}X'\underbrace{Y}_{=X\beta+\epsilon}|X) \\[2ex]
&amp;= E((X'X)^{-1}X'(X\beta+\epsilon)|X)                 \\[2ex]
&amp;= E(\underbrace{(X'X)^{-1}X'X}_{=I_{(p+1)}}\beta|X) + E((X'X)^{-1}X'\epsilon|X)\\[2ex]
&amp;= \underbrace{E(\beta|X)}_{=\beta} + \underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=(X'X)^{-1}X'E(\epsilon|X)}  \\[2ex]
&amp;= \beta + (X'X)^{-1}X'\underbrace{E(\epsilon|X)}_{=0}\\[2ex]
&amp;= \underset{(p+1)\times 1}{\beta}  
\end{align*}
\]</span> Thus <span class="math inline">\(\hat\beta\)</span> is <strong>conditionally unbiased:</strong> <span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta|X)
&amp; = E(\hat\beta|X) - \beta\\[2ex]
&amp; = \beta - \beta \\[2ex]
&amp; = \underset{(p+1)\times 1}{0}   
\end{align*}
\]</span></p>
<p>From this if follows, by the iterated law of expectations, that the OLS estimator is also <strong>unconditionally unbiased:</strong><br><span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta)
&amp; = E(\operatorname{Bias}(\hat\beta|X))\\[2ex]
&amp; = E\left(E(\hat\beta|X) - \beta\right)\\[2ex]
&amp; = E\left(E(\hat\beta|X)\right) - E\left(\beta\right)\\[2ex]
&amp; = E\left(\beta\right) - E\left(\beta\right)\\[2ex]
&amp; = \beta - \beta = 0.
\end{align*}
\]</span></p>
</section><section id="standard-error-of-hatbeta_j" class="level3" data-number="4.4.2"><h3 data-number="4.4.2" class="anchored" data-anchor-id="standard-error-of-hatbeta_j">
<span class="header-section-number">4.4.2</span> Standard Error of <span class="math inline">\(\hat{\beta}_j\)</span>
</h3>
<p>The standard error of <span class="math inline">\(\hat{\beta}_j,\)</span> for each <span class="math inline">\(j=0,\dots,p,\)</span> is given by <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X)=\sqrt{Var(\hat\beta_j|X)},
\]</span> where <span class="math display">\[
Var(\hat\beta_j|X) = \left[Var(\hat\beta|X)\right]_{(j,j)}
\]</span> denotes the <span class="math inline">\(j\)</span>th diagonal element of the symmetric <span class="math inline">\((p+1)\times (p+1)\)</span> variance-covariance matrix <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=\begin{pmatrix}
Var(\hat\beta_0|X)&amp;Cov(\hat\beta_0,\hat\beta_1|X)&amp;\cdots&amp;Cov(\hat\beta_0,\hat\beta_{p}|X)\\
Cov(\hat\beta_1,\hat\beta_0|X)&amp;Var(\hat\beta_1|X)&amp;  &amp;Cov(\hat\beta_1,\hat\beta_{p}|X)\\
\vdots &amp;&amp;\ddots&amp;\\
Cov(\hat\beta_p,\hat\beta_0|X)&amp;Cov(\hat\beta_p,\hat\beta_1|X)&amp;\cdots&amp;Var(\hat\beta_{p}|X)\\
\end{pmatrix}
\end{align*}
\]</span></p>
<p>Thus, to compute a useful, explicit expression for the standard error, <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X)=\;\text{Explicit Expression}\;?,
\]</span> we need to compute an explicit expression for the symmetric <span class="math inline">\((p+1)\times(p+1)\)</span> variance-covariance matrix <span class="math inline">\(Var(\hat\beta|X).\)</span></p>
<p>Let us derive the general explicit expression for <span class="math inline">\(Var(\hat\beta|X).\)</span></p>
<p>Note that <span class="math display">\[
\begin{align*}
\hat{\beta}
&amp;=(X'X)^{-1}X'Y\\[2ex]
&amp;\text{Using that (by Assumption 1 Part (a)) $Y=X\beta + \epsilon$:}\\[2ex]
&amp;=(X'X)^{-1}X'\; \underbrace{(X\beta + \epsilon)}_{=Y}\\[2ex]
&amp;=\underbrace{(X'X)^{-1}X'X}_{=I_{(p+1)}}\;\beta + (X'X)^{-1}X'\epsilon\\
&amp;=\beta + (X'X)^{-1}X'\epsilon
\end{align*}
\]</span> This leads to the so-called <strong>sampling error expression</strong> <span class="math display">\[
\hat{\beta} - \beta = (X'X)^{-1}X'\epsilon.
\]</span> With this, we can derive the general explicit expression for <span class="math inline">\(Var(\hat\beta|X).\)</span> <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;\text{Adding/subtracting constants does not change variance:}\\[2ex]
&amp;=Var(\hat\beta - \beta|X)\\[2ex]
&amp;\text{Using the sampling error expression:}\\[2ex]
&amp;=Var((X'X)^{-1}X'\epsilon|X)\\[2ex]
&amp;=E\Big[\big((X'X)^{-1}X'\epsilon-\underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=0}\big)\times\\[2ex]
&amp;\phantom{=\Big(}\,\times\big((X'X)^{-1}X'\epsilon-\underbrace{E((X'X)^{-1}X'\epsilon|X)}_{=0}\big)'|X\Big]\\[2ex]
&amp;=E\left[((X'X)^{-1}X'\epsilon)((X'X)^{-1}X'\epsilon)'|X\right]\\[2ex]
&amp;=E\left[(X'X)^{-1}X'\epsilon\epsilon' X(X'X)^{-1}|X\right]\\[2ex]
&amp;=\;\;\;(X'X)^{-1}X'\underbrace{E\left(\epsilon\epsilon'|X\right)}_{=Var(\epsilon|X)}X(X'X)^{-1}
\end{align*}
\]</span> That is, the explicit expression for <span class="math inline">\(Var(\hat\beta|X)\)</span> depends on the explicit form of the symmetric <span class="math inline">\((n\times n)\)</span> matrix <span class="math inline">\(Var(\epsilon|X)\)</span> <span class="math display">\[
\begin{align*}
&amp;Var(\epsilon|X)=\\[2ex]
&amp;=\begin{pmatrix}
Var(\epsilon_1|X)&amp;Cov(\epsilon_1,\epsilon_2|X)&amp;\cdots&amp;Cov(\epsilon_1,\epsilon_n|X)\\
Cov(\epsilon_2,\epsilon_1|X)&amp;Var(\epsilon_2|X)&amp;  &amp;Cov(\epsilon_2,\epsilon_n|X)\\
\vdots &amp;&amp;\ddots&amp;\\
Cov(\epsilon_n,\epsilon_1|X)&amp;Cov(\epsilon_n,\epsilon_2|X)&amp;\cdots&amp;Var(\epsilon_n|X)\\
\end{pmatrix}
\end{align*}
\]</span></p>
<p>The explicit form of the symmetric <span class="math inline">\((n\times n)\)</span> matrix <span class="math inline">\(Var(\epsilon|X)\)</span> depends on our (hopefully correct) assumption on the error-term distribution (Assumption 4).</p>
<p>In the following, we consider the two most prominent types of assumptions for the symmetric <span class="math inline">\((n\times n)\)</span> matrix <span class="math inline">\(Var(\epsilon|X)\)</span>:</p>
<ul>
<li>Spherical, i.e., homoskedastic and uncorrelated errors</li>
<li>Heteroskedastic and uncorrelated errors</li>
</ul>
<section id="case-of-spherical-homoskedastic-and-uncorrelated-errors" class="level4" data-number="4.4.2.1"><h4 data-number="4.4.2.1" class="anchored" data-anchor-id="case-of-spherical-homoskedastic-and-uncorrelated-errors">
<span class="header-section-number">4.4.2.1</span> Case of Spherical (Homoskedastic and Uncorrelated) Errors</h4>
<p>If <span class="math display">\[
\begin{align*}
Var(\epsilon|X)
&amp;=
\begin{pmatrix}
\sigma^2 &amp; 0        &amp; \cdots &amp; 0\\
0        &amp; \sigma^2 &amp; \cdots &amp; 0\\
\vdots   &amp; \vdots   &amp; \ddots &amp; 0\\
0        &amp; 0        &amp; \cdots &amp; \sigma^2\\
\end{pmatrix}
=\sigma^2 I_n,
\end{align*}
\]</span> then <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=(X'X)^{-1}X' \left(Var(\epsilon|X)\right) X(X'X)^{-1}\\[2ex]
&amp;=(X'X)^{-1}X' \left(\sigma^2 I_n \right) X(X'X)^{-1}\\[2ex]
&amp;=\sigma^2\;(X'X)^{-1}X' \left( I_n \right) X(X'X)^{-1}\\[2ex]
&amp;=\sigma^2\;\underbrace{(X'X)^{-1}X'X}_{I_{p+1}}\;(X'X)^{-1}\\[2ex]
&amp;=\sigma^2\;(X'X)^{-1},
\end{align*}
\]</span> where the only unknown component is <span class="math inline">\(\sigma^2=Var(\epsilon_i).\)</span></p>
<p>We can estimate the homoskedastik error term variance <span class="math inline">\(\sigma^2\)</span> using the <strong>R</strong>esidual <strong>S</strong>tandard <strong>E</strong>rror: <span class="math display">\[
\begin{align*}
\hat\sigma = \operatorname{RSE}
&amp;=\sqrt{\frac{\operatorname{RSS}}{n-(p+1)}}\\[2ex]
&amp;=\sqrt{ \frac{1}{n-(p+1)} \sum_{i=1}^n e_i^2}.
\end{align*}
\]</span></p>
<p><strong>Summing up:</strong></p>
<p>In the case of spherical (homoskedastic and uncorrelated) error terms the standard error of <span class="math inline">\(\beta_j\)</span> is <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X) = \sqrt{\left[\sigma^2 \left(X'X\right)^{-1}\right]_{(j,j)}}.
\]</span> The above expression is infeasible since <span class="math inline">\(\sigma^2\)</span> is typically unknown. We can estimate this infeasible population version using the empirical standard error <span class="math display">\[
\widehat{\operatorname{SE}}(\hat\beta_j|X) = \sqrt{\left[\hat{\sigma}^2 \left(X'X\right)^{-1}\right]_{(j,j)}}.
\]</span></p>
<p>This is the default version for computing the standard error in statistical software packages such as <code>R</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>      <span class="op">&lt;-</span> <span class="fl">100</span>                           <span class="co"># Sample size</span></span>
<span><span class="va">X_1</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>               <span class="co"># Predictor variable X_1</span></span>
<span><span class="va">X_2</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span>               <span class="co"># Predictor variable X_2</span></span>
<span><span class="va">error</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">*</span><span class="fl">10</span>             <span class="co"># True (usually unknown) error</span></span>
<span><span class="va">Y</span>      <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X_1</span> <span class="op">-</span><span class="fl">5</span> <span class="op">*</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">error</span>  <span class="co"># Y variable</span></span>
<span><span class="va">lm_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span><span class="op">)</span>             <span class="co"># OLS regression </span></span>
<span></span>
<span><span class="co">## Standard OLS output table:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span>                         </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Y ~ X_1 + X_2)

Residuals:
    Min      1Q  Median      3Q     Max 
-39.071  -7.138  -0.575   9.570  33.368 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.0112     4.2440   0.238    0.812    
X_1           5.1954     0.4529  11.472  &lt; 2e-16 ***
X_2          -4.7001     0.6690  -7.026 2.95e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 12.84 on 97 degrees of freedom
Multiple R-squared:  0.6565,    Adjusted R-squared:  0.6494 
F-statistic: 92.68 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section><section id="case-of-heteroskedastic-and-uncorrelated-errors" class="level4" data-number="4.4.2.2"><h4 data-number="4.4.2.2" class="anchored" data-anchor-id="case-of-heteroskedastic-and-uncorrelated-errors">
<span class="header-section-number">4.4.2.2</span> Case of Heteroskedastic and Uncorrelated Errors</h4>
<p>If <span class="math display">\[
\begin{align*}
Var(\epsilon|X)
&amp;=
\begin{pmatrix}
\sigma_1^2 &amp; 0          &amp; \cdots &amp; 0\\
0          &amp; \sigma_2^2 &amp; \cdots &amp; 0\\
\vdots     &amp; \vdots     &amp; \ddots &amp; 0\\
0          &amp; 0          &amp; \cdots &amp; \sigma_n^2\\
\end{pmatrix}
&amp;=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2),
\end{align*}
\]</span> then <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
&amp;=(X'X)^{-1}X' \left(Var(\epsilon|X)\right) X(X'X)^{-1}\\[2ex]
&amp;=(X'X)^{-1} \left(X'\left(\operatorname{diag}(\sigma_1^2,\dots,\sigma_n^2) \right) X\right) (X'X)^{-1}\\[2ex]
&amp;=(X'X)^{-1} \left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right) (X'X)^{-1}\\[2ex]
\end{align*}
\]</span> Thus, the symmetric <span class="math inline">\((p+1)\times(p+1)\)</span> variance-covariance matrix <span class="math inline">\(Var(\hat\beta|X)\)</span> keeps its “sandwich form”, where the inner part of the sandwich <span class="math display">\[
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right)
\]</span> is typically unknown, since <span class="math inline">\(\sigma_1^2,\sigma_2^2,\dots,\sigma_n^2\)</span> are typically unknown.</p>
<p>There are different, so-called <strong>Heteroskedasticity Consistent (HC)</strong> estimators to estimate the unknown expression <span class="math display">\[
\left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right).
\]</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 66%">
</colgroup>
<thead><tr class="header">
<th>HC-Type</th>
<th>Formular</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>HC0</td>
<td><span class="math inline">\(\sum_{i=1}^ne_i^2X_iX_i'\)</span></td>
</tr>
<tr class="even">
<td>HC1</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{n}{n-(p+1)}e_i^2X_iX_i'\)</span></td>
</tr>
<tr class="odd">
<td>HC2</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{e_{i}^{2}}{1-h_{i}}X_iX_i'\)</span></td>
</tr>
<tr class="even">
<td>HC3</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{e_{i}^{2}}{\left(1-h_{i}\right)^{2}}X_iX_i'\)</span></td>
</tr>
<tr class="odd">
<td>HC4</td>
<td><span class="math inline">\(\sum_{i=1}^n\frac{e_{i}^{2}}{\left(1-h_{i}\right)^{\delta_{i}}}X_iX_i'\)</span></td>
</tr>
</tbody>
</table>
<p>HC3 is the most often used HC-estimator.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The statistic <span class="math inline">\(h_i\)</span> is simply the <span class="math inline">\(i\)</span>th diagonal element of the projection matrix <span class="math inline">\(P_X\)</span> <span class="math display">\[
h_i=[P_X]_{ii}
\]</span> and is called the <span class="math inline">\(i\)</span>th <strong>leverage statistic</strong>, where</p>
<ul>
<li>
<span class="math inline">\(1/n\leq h_i\leq 1\)</span> and</li>
<li>
<span class="math inline">\(\bar{h}=n^{-1}\sum_{i=1}^nh_i=(p+1)/n\)</span>.</li>
</ul>
<p>Observations <span class="math inline">\(X_i\)</span> with leverage statistics <span class="math inline">\(h_i\)</span> that greatly exceed the average leverage value <span class="math inline">\((p+1)/n\)</span> are referred to as “high leverage” observations. High leverage observations <span class="math inline">\(X_i\)</span> are observations that are far away from the predictor values of all other subjects.</p>
<p>High leverage observations <span class="math inline">\(X_i\)</span> have the potential to distort the estimation results, <span class="math inline">\(\hat\beta_n\)</span>. Indeed, a high leverage observation <span class="math inline">\(X_i\)</span> will have an distorting effect on the estimation results if the absolute value of the corresponding residual <span class="math inline">\(|e_i|\)</span> is unusually large—such observations are called <strong>influential outliers</strong>. Such observations increase the estimation uncertainty.</p>
<p>General idea of the HC2-HC4 estimators is to increase the estimated variance in order to account for the effects of influential outliers. The residuals <span class="math inline">\(e_i\)</span> belonging to <span class="math inline">\(X_i\)</span> values that have a large leverage <span class="math inline">\(h_i\)</span> receive a higher weight and thus increase the value of <span class="math inline">\(\widehat{E}(\epsilon^2_iX_iX_i').\)</span> This strategy takes into account increased estimation uncertainties due to single influential outliers.</p>
</div>
</div>
<p>The estimator HC0 was suggested in the econometrics literature by <span class="citation" data-cites="White1980">White (<a href="#ref-White1980" role="doc-biblioref">1980</a>)</span> and is justified by asymptotic (<span class="math inline">\(n\to\infty\)</span>) arguments. The estimators HC1, HC2 and HC3 were suggested by <span class="citation" data-cites="MacKinnon_White_1985">MacKinnon and White (<a href="#ref-MacKinnon_White_1985" role="doc-biblioref">1985</a>)</span> to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, <span class="citation" data-cites="Long_Ervin_2000">Long and Ervin (<a href="#ref-Long_Ervin_2000" role="doc-biblioref">2000</a>)</span> concludes that HC3 provides the best overall performance in finite samples. <span class="citation" data-cites="Cribari_2004">Cribari-Neto (<a href="#ref-Cribari_2004" role="doc-biblioref">2004</a>)</span> suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large <span class="math inline">\(h_i\)</span> values).</p>
<p><strong>Summing up:</strong></p>
<p>In the case of heteroskedastic and uncorrelated error terms the standard error of <span class="math inline">\(\beta_j\)</span> is <span class="math display">\[
\operatorname{SE}(\hat\beta_j|X) = \sqrt{\left[(X'X)^{-1} \left(\sum_{i=1}^n \sigma_i^2 X_i X_i'\right) (X'X)^{-1}\right]_{(j,j)}}.
\]</span> The above expression is the infeasible (since <span class="math inline">\(\sigma^2\)</span> is typically unknown) population version of the standard error. We can estimate this population version using the empirical standard error <span class="math display">\[
\widehat{\operatorname{SE}}(\hat\beta_j|X) = \sqrt{\left[(X'X)^{-1} \left(\textsf{HC}\right) (X'X)^{-1}\right]_{(j,j)}},
\]</span> where <span class="math inline">\(\textsf{HC}\)</span> is a placeholder for one the <strong>H</strong>eteroskedasticity <strong>C</strong>onsistent estimators HC1-HC4 given above.</p>
<p>The <code>sandwich</code>-package allows to compute these standard errors in <code>R</code></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>      <span class="op">&lt;-</span> <span class="fl">100</span>                           <span class="co"># Sample size</span></span>
<span><span class="va">X_1</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>               <span class="co"># Predictor variable X_1</span></span>
<span><span class="va">X_2</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span>               <span class="co"># Predictor variable X_2</span></span>
<span><span class="va">error</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">X_2</span><span class="op">)</span>     <span class="co"># True (usually unknown) heteroskedastic error</span></span>
<span><span class="va">Y</span>      <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X_1</span> <span class="op">-</span><span class="fl">5</span> <span class="op">*</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">error</span>  <span class="co"># Y variable</span></span>
<span></span>
<span></span>
<span><span class="co">## Package for computing robust variance estimations</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://sandwich.R-Forge.R-project.org/">"sandwich"</a></span><span class="op">)</span> <span class="co"># vcovHC(), </span></span>
<span></span>
<span><span class="co">## Package for producing an OLS output table (etc.)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"lmtest"</span><span class="op">)</span><span class="op">)</span> <span class="co"># coeftest</span></span>
<span></span>
<span><span class="co">## Estimate the linear regression model parameters</span></span>
<span><span class="va">lm_obj</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_1</span> <span class="op">+</span> <span class="va">X_2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">vcovHC3_mat</span> <span class="op">&lt;-</span> <span class="fu">sandwich</span><span class="fu">::</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_obj</span>, type<span class="op">=</span><span class="st">"HC3"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">lmtest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest</a></span><span class="op">(</span><span class="va">lm_obj</span>, vcov <span class="op">=</span> <span class="va">vcovHC3_mat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
t test of coefficients:

            Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept)  3.16955    2.17658   1.4562   0.1486    
X_1          4.92390    0.22302  22.0784   &lt;2e-16 ***
X_2         -4.57382    0.34716 -13.1748   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Note: The HC3-Robust SE estimates are: </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">vcovHC3_mat</span><span class="op">)</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)         X_1         X_2 
    2.17658     0.22302     0.34716 </code></pre>
</div>
</div>
</section></section></section><section id="inference" class="level2" data-number="4.5"><h2 data-number="4.5" class="anchored" data-anchor-id="inference">
<span class="header-section-number">4.5</span> Inference</h2>
<section id="confidence-intervals-for-beta_j" class="level3" data-number="4.5.1"><h3 data-number="4.5.1" class="anchored" data-anchor-id="confidence-intervals-for-beta_j">
<span class="header-section-number">4.5.1</span> Confidence Intervals for <span class="math inline">\(\beta_j\)</span>
</h3>
<p>Given the estimate <span class="math inline">\(\hat\beta_j\)</span> and the estimate of the standard error <span class="math display">\[
\widehat{\operatorname{SE}}(\hat\beta_j|X),
\]</span> we can construct the <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> confidence interval for the true (unknown) <span class="math inline">\(\beta_j\)</span>-parameter: <span class="math display">\[
\begin{align*}
&amp;\operatorname{CI}^{1-\alpha}_{\beta_j}=\\[2ex]
=&amp;\left[\hat\beta_j - q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j|X),\;
\hat\beta_j + q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j|X)\right]\\[2ex]
&amp;\text{More compact notation:}\\[2ex]
=&amp;\left[\hat\beta_j\; {\color{red}\pm} \; q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j|X)\right],
\end{align*}
\]</span> where <span class="math display">\[
q^{t,n-(p+1)}_{1-\alpha/2}
\]</span> denotes the <span class="math inline">\((1-\alpha/2)\)</span>-quantile of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(\operatorname{df}=n-(p+1)\)</span> degrees of freedom, and where <span class="math inline">\(\alpha\)</span> denotes the <strong>significance level</strong> with typical choices:</p>
<ul>
<li><span class="math inline">\(\alpha = 0.05\)</span></li>
<li><span class="math inline">\(\alpha = 0.01\)</span></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The confidence interval <span class="math display">\[
\operatorname{CI}^{1-\alpha}_{\beta_j}
\]</span> is a <strong>random confidence interval</strong>.</p>
<p>For a given realization of the training data random sample <span class="math display">\[
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{np}))
\]</span> we <strong>obs</strong>serve a specific realization <span class="math display">\[
\operatorname{CI}^{1-\alpha}_{\beta_j,obs}.
\]</span></p>
<p>There is a <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> chance (in resamplings from the training data random sample) that the <strong>random confidence interval</strong> <span class="math display">\[
\operatorname{CI}^{1-\alpha}_{\beta_j}
\]</span> contains the true (fix) parameter value <span class="math inline">\(\beta_j.\)</span></p>
<p>To understand the interpretation of confidence intervals, it is very instructive to look at visualizations:</p>
<ul>
<li><a href="https://rpsychologist.com/d3/ci/">Interactive visualization for interpreting confidence intervals</a></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Only the above frequentist point of view can be nicely interpreted.</p>
<p>A given, observed confidence interval <span class="math display">\[
\operatorname{CI}^{1-\alpha}_{\beta_j,obs},
\]</span> either contains the true parameter value or not and usually we do not know it, since we do not know the value of <span class="math inline">\(\beta_j.\)</span></p>
<p><img src="images/CI_meme.jpg" class="img-fluid"></p>
</div>
</div>
</section><section id="confidence-intervals-for-statistical-hypothesis-testing" class="level3" data-number="4.5.2"><h3 data-number="4.5.2" class="anchored" data-anchor-id="confidence-intervals-for-statistical-hypothesis-testing">
<span class="header-section-number">4.5.2</span> Confidence Intervals for Statistical Hypothesis Testing</h3>
<p>We can use the <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> confidence interval to do statistical hypothesis testing at the significance level <span class="math inline">\(0&lt;\alpha&lt;1.\)</span> Typical significance levels:</p>
<ul>
<li><span class="math inline">\(\alpha=0.05\)</span></li>
<li><span class="math inline">\(\alpha=0.01\)</span></li>
</ul>
<p>Let us consider the following null-hypothesis <span class="math inline">\((H_0)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_j\)</span> equals the <strong>null-hypothetical value</strong> <span class="math inline">\(\beta^{(H_0)}_{j}\)</span> versus the two-sided alternative hypothesis <span class="math inline">\((H_1)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_j\)</span> does <strong>not equal</strong> the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{j}:\)</span> <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_j=\beta^{(H_0)}_{j}\\
H_1:&amp;\;\beta_j\neq \beta^{(H_0)}_{j}
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Classic No-Effect Null-Hypothesis
</div>
</div>
<div class="callout-body-container callout-body">
<p>For the special case, where the null-hypothetical value equals zero <span class="math display">\[
\beta^{(H_0)}_{j}=0
\]</span> we test the classic <strong>no-effect null-hypothesis</strong>.</p>
</div>
</div>
<p><strong>Testing-Procedure:</strong></p>
<ul>
<li><p>If the observed (obs) realization of the confidence interval, <span class="math inline">\(\operatorname{CI}^{1-\alpha}_{\beta_j,obs},\)</span> <strong>contains</strong> the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{j},\)</span> i.e. <span class="math display">\[
\begin{align*}
\beta^{(H_0)}_{j}&amp;\in\operatorname{CI}^{1-\alpha}_{\beta_j,obs}\\[2ex]
\Leftrightarrow\beta^{(H_0)}_{j}&amp;\in
\left[
\hat{\beta}_{j,obs} \;\pm\; q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j)_{obs}
\right],
\end{align*}
\]</span> then we <strong>cannot reject the null hypothesis</strong> <span class="math inline">\(\beta_j=\beta^{(H_0)}_{j}.\)</span></p></li>
<li><p>If, however, the observed (obs) realization of the confidence interval, <span class="math inline">\(\operatorname{CI}^{1-\alpha}_{\beta_j,obs},\)</span> does <strong>not contain</strong> the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{j},\)</span> i.e. <span class="math display">\[
\begin{align*}
\beta^{(H_0)}_{j}&amp;\not\in\operatorname{CI}^{1-\alpha}_{\beta_j,obs}\\[2ex]
\Leftrightarrow\beta^{(H_0)}_{j}&amp;\not\in
\left[
\hat{\beta}_{j,obs} \;\pm\; q^{t,n-(p+1)}_{1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_j)_{obs}
\right],
\end{align*}
\]</span> then we <strong>reject the null hypothesis</strong> and adopt the alternative <span class="math inline">\(\beta_j\neq\beta^{(H_0)}_{j}.\)</span></p></li>
</ul></section><section id="t-test" class="level3" data-number="4.5.3"><h3 data-number="4.5.3" class="anchored" data-anchor-id="t-test">
<span class="header-section-number">4.5.3</span> <span class="math inline">\(t\)</span>-Test</h3>
<p>Standard errors can also be used to construct test statistics for statistical hypothesis testing. In the following, we look at the <span class="math inline">\(t\)</span>-test statistic.</p>
<p>Choose a significance level <span class="math inline">\(0&lt;\alpha&lt;1\)</span> such as, for instance,</p>
<ul>
<li><span class="math inline">\(\alpha=0.05\)</span></li>
<li><span class="math inline">\(\alpha=0.01\)</span></li>
</ul>
<p>Let us (again) consider the null-hypothesis <span class="math inline">\((H_0)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_j\)</span> equals the <strong>null-hypothetical value</strong> <span class="math inline">\(\beta^{(H_0)}_{j}\)</span> versus the two-sided alternative hypothesis <span class="math inline">\((H_1)\)</span> that the true (usually unknown) value <span class="math inline">\(\beta_j\)</span> does not equal the null-hypothetical value <span class="math inline">\(\beta^{(H_0)}_{j}:\)</span> <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_j=\beta^{(H_0)}_{j}\\[2ex]
H_1:&amp;\;\beta_j\neq \beta^{(H_0)}_{j}
\end{align*}
\]</span></p>
<p>The <strong>random</strong> <span class="math inline">\(t\)</span>-test statistic is given by <span class="math display">\[
T=\frac{\hat\beta_j - \beta^{(H_0)}_{j}}{\widehat{\operatorname{SE}}(\hat\beta_j)}
\]</span></p>
<p>Under the null-hypothesis, <span class="math inline">\(\beta_j=\beta^{(H_0)}_{j},\)</span> the <span class="math inline">\(t\)</span>-test statistic is <span class="math inline">\(t\)</span>-distributed with <span class="math inline">\(\operatorname{df}=n-(p+1)\)</span> degrees of freedom. <span class="math display">\[
T=\frac{\hat\beta_j - \beta^{(H_0)}_{j}}{\widehat{\operatorname{SE}}(\hat\beta_j)}\overset{H_0}{\sim} t_{n-(p+1)},
\]</span> where <span class="math inline">\(t_{n-(p+1)}\)</span> denotes the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(\operatorname{df}=n-(p+1)\)</span> degrees of freedom.</p>
<p>For a given realization of the training data random sample <span class="math display">\[
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{np}))
\]</span> we <strong>obs</strong>serve a specific realization of the <span class="math inline">\(t\)</span>-test statistic <span class="math display">\[
T_{obs}=\frac{\hat\beta_{j,obs} - \beta^{(H_0)}_{j}}{\widehat{\operatorname{SE}}(\hat\beta_j)_{obs}}
\]</span></p>
<section id="p-value" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="p-value"><strong><span class="math inline">\(p\)</span>-value:</strong></h4>
<p>The <span class="math inline">\(p\)</span>-value is the probability of seeing a realization of the <strong>random</strong> <span class="math inline">\(t\)</span>-test statistic, <span class="math inline">\(T,\)</span> which is more extreme than the observed value of the test-statistic, <span class="math inline">\(T_{obs},\)</span> given the null-hypothesis is true <span class="math display">\[
\begin{align*}
p_{obs}
&amp; = P\left(|T|\geq|T_{obs}|\;\;|\;\; \text{$H_0$ is true}\right)\\[2ex]
&amp; = 2\cdot\min\{P\left(T\geq T_{obs} \;\;|\;\; \text{$H_0$ is true}\right),\;\\
&amp; \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; P\left(T\leq T_{obs} \;\;|\;\; \text{$H_0$ is true}\right)\}.
\end{align*}
\]</span></p>
<p><strong>Testing-Procedure:</strong> <!-- To do the statistical hypothesis test, we need to select a significance level $\alpha$ (e.g.,  $\alpha=0.05$ or $\alpha=0.01$).  --></p>
<ul>
<li><p>If the observed realization of the <span class="math inline">\(p\)</span>-value is larger than or equal to the significance level <span class="math display">\[
p_{obs}\geq \alpha,
\]</span> then we <strong>cannot reject the null hypothesis</strong> <span class="math inline">\(\beta_j=\beta^{(H_0)}_{j}.\)</span></p></li>
<li><p>If, however, the observed realization of the <span class="math inline">\(p\)</span>-value is strictly smaller than the significance level <span class="math display">\[
p_{obs}&lt;\alpha,
\]</span> then we <strong>reject the null hypothesis</strong> and adopt the alternative hypothesis <span class="math inline">\(\beta_j\neq \beta^{(H_0)}_{j}.\)</span></p></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Equivalence of Confidence Intervals and <span class="math inline">\(t\)</span>-Test
</div>
</div>
<div class="callout-body-container callout-body">
<p>It can be shown that a statistical hypothesis test based on the above confidence interval <span class="math inline">\(\operatorname{CI}^{1-\alpha}_{\beta_j,obs}\)</span> leads to exactly the same test decisions as a statistical hypothesis test based on the above <span class="math inline">\(t\)</span>-test statistic <span class="math inline">\(T_{obs}.\)</span></p>
</div>
</div>
</section></section><section id="f-test" class="level3" data-number="4.5.4"><h3 data-number="4.5.4" class="anchored" data-anchor-id="f-test">
<span class="header-section-number">4.5.4</span> <span class="math inline">\(F\)</span>-Test</h3>
<p>The <span class="math inline">\(t\)</span>-test statistic (equivalently the confidence interval for <span class="math inline">\(\beta_j\)</span>) allows us to test a null-hypothesis about <strong>one</strong> parameter <span class="math inline">\(\beta_j.\)</span></p>
<p>To test whether there is a relationship between the response <span class="math inline">\(Y\)</span> and total <em>vector</em> predictors <span class="math inline">\((X_1,\dots,X_p),\)</span> we can use the <span class="math inline">\(F\)</span>-test statistic.</p>
<p>In this case, the <span class="math inline">\(F\)</span>-test tests the null-hypothesis <span class="math display">\[
\begin{align*}
H_0:&amp;\;\beta_1=\beta_2=\dots=\beta_p=0\\
\text{versus}\quad H_1:&amp;\;\text{at least one $\beta_j\neq 0$; $j=1,\dots,p$}
\end{align*}
\]</span></p>
<p><span class="math inline">\(F\)</span>-test statistic <span class="math display">\[
F=\frac{(\operatorname{TSS}-\operatorname{RSS})/p}{\operatorname{
  RSS}/(n-p-1)}\overset{H_0}{\sim} F_{p,n-p-1}
\]</span> Under <span class="math inline">\(H_0,\)</span> i.e.&nbsp;if <span class="math inline">\(H_0\)</span> is true, the <span class="math inline">\(F\)</span>-test statistic has a <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(p\)</span> numerator and <span class="math inline">\((n-p-1)\)</span> denominator degrees of freedom.</p>
<p>If <span class="math inline">\(H_0\)</span> is correct <span class="math display">\[
\begin{align*}
E((\operatorname{TSS}-\operatorname{RSS})/p)&amp;=\sigma^2\\[2ex]
E(\operatorname{RSS}/(n-p-1))&amp;=\sigma^2
\end{align*}
\]</span></p>
<p>Therefore:</p>
<ul>
<li>If <span class="math inline">\(H_0\)</span> is correct, we expect values of <span class="math inline">\(F\approx 1.\)</span>
</li>
<li>If <span class="math inline">\(H_1\)</span> is correct, we expect values of <span class="math inline">\(F\gg 1.\)</span>
</li>
</ul>
<p>For a given realization of the training data random sample <span class="math display">\[
((y_{1},x_{10},\dots,x_{1(p+1)}),\dots,(y_{n},x_{n0},\dots,x_{np}))
\]</span> we <strong>obs</strong>serve a specific realization of the <span class="math inline">\(F\)</span>-test statistic <span class="math display">\[
F_{obs}=\frac{(\operatorname{TSS}_{obs}-\operatorname{RSS}_{obs})/p}{\operatorname{
  RSS}_{obs}/(n-p-1)}
\]</span></p>
<section id="p-value-1" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="p-value-1"><strong><span class="math inline">\(p\)</span>-value:</strong></h4>
<p>The <span class="math inline">\(p\)</span>-value is the probability of seeing a realization of the <strong>random</strong> <span class="math inline">\(F\)</span>-test statistic, <span class="math inline">\(F,\)</span> which is more extreme than the observed value of the test-statistic, <span class="math inline">\(F_{obs},\)</span> given the null-hypothesis is true <span class="math display">\[
\begin{align*}
p_{obs}
&amp;=P\left( F \geq F_{obs} \;\;|\;\; \text{$H_0$ is true}\right),
\end{align*}
\]</span> where <span class="math inline">\(F_{obs}\)</span> denotes the observed value of the <span class="math inline">\(F\)</span>-test statistic computed from the observed training data, and where <span class="math inline">\(F\)</span> is a random variable that has a <span class="math inline">\(F_{p,n-p-1}\)</span> distribution.</p>
<p><strong>Testing-Procedure:</strong> <!-- To do the statistical hypothesis test, we need to select a significance level $\alpha$ (e.g.,  $\alpha=0.05$ or $\alpha=0.01$).  --></p>
<ul>
<li><p>If the observed realization of the <span class="math inline">\(p\)</span>-value is larger than or equal to the significance level <span class="math display">\[
p_{obs}\geq \alpha,
\]</span> then we <strong>cannot reject the null hypothesis</strong>.</p></li>
<li><p>If, however, the observed realization of the <span class="math inline">\(p\)</span>-value is strictly smaller than the significance level <span class="math display">\[
p_{obs}&lt;\alpha,
\]</span> then we <strong>reject the null hypothesis</strong> and adopt the alternative hypothesis.</p></li>
</ul></section></section><section id="probability-of-a-type-i-error-power-and-consistency" class="level3" data-number="4.5.5"><h3 data-number="4.5.5" class="anchored" data-anchor-id="probability-of-a-type-i-error-power-and-consistency">
<span class="header-section-number">4.5.5</span> Probability of a Type I Error, Power and Consistency</h3>
<p>Every statistical testing procedure (conducted using confidence intervals or test-statistics) involves the decision of</p>
<ul>
<li>not rejecting <span class="math inline">\(H_0\)</span>
</li>
</ul>
<p>versus</p>
<ul>
<li>rejecting <span class="math inline">\(H_0\)</span>
</li>
</ul>
<p>In applied research, we “aim” for rejecting <span class="math inline">\(H_0.\)</span> (Caution: the desire for rejecting <span class="math inline">\(H_0\)</span> is the reason of a lot of scientific misconduct!) For instance, when we are able to reject the no-effect null-hypothesis <span class="math display">\[
H_0: \beta_j = 0
\]</span> and thus able to adopt the alternative <span class="math display">\[
H_1: \beta_j \neq 0,
\]</span> then we can state in a publication that variable <span class="math inline">\(X_j\)</span> has an effect.</p>
<p>Thus, if we reject the null-hypothesis even though the null-hypothesis is true, we conduct a <strong>Type I Error</strong> and thus may falsely claim that variable <span class="math inline">\(X_j\)</span> has an effect. Such a false claim can be fatal.</p>
<p>Therefore, a statistical hypothesis test is only valid if it is able control the probability of conducting a type I error from above by the chosen significance level; i.e.&nbsp;if <span class="math display">\[
\underbrace{P(\text{reject }H_0 \;|\; H_0\text{ is true})}_{\text{Probability of a type I error}}\leq \alpha,
\]</span> where <span class="math inline">\(\alpha\)</span> is some small value like <span class="math inline">\(\alpha = 0.05\)</span> or <span class="math inline">\(\alpha = 0.01.\)</span> I.e., in <span class="math inline">\(100\)</span> resamples we expect to see at most <span class="math inline">\(\alpha\cdot 100\)</span> false rejections of <span class="math inline">\(H_0.\)</span></p>
<p>By choosing a small significance level like <span class="math inline">\(\alpha = 0.05\)</span> or <span class="math inline">\(\alpha = 0.01,\)</span> we make sure that we can be quite “sure” that we do not falsely reject <span class="math inline">\(H_0.\)</span></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>A statistical testing procedure only controls the probability of a type I error (falsely rejecting <span class="math inline">\(H_0,\)</span> when <span class="math inline">\(H_0\)</span> is true), <span class="math display">\[
\underbrace{P(\text{reject }H_0 \;|\; H_0\text{ is true})}_{\text{Probability of a type I error}}\leq \alpha,
\]</span> but not the probability of a type II error (falsely not rejecting <span class="math inline">\(H_0,\)</span> when <span class="math inline">\(H_1\)</span> is true). In fact, we typically do not know the probability of a type II error <span class="math display">\[
\underbrace{P(\text{not reject }H_0 \;|\; H_1\text{ is true})}_{\text{Probability of a type II error}}\leq \;\; {\color{red}?}.
\]</span></p>
<p>Therefore, when we cannot reject the null-hypothesis we cannot claim that the null-hypothesis is probably true, simply because we cannot guarantee that the probability of a type II error (falsely not rejecting <span class="math inline">\(H_0,\)</span> when <span class="math inline">\(H_1\)</span> is true) is sufficiently small.</p>
<p>Indeed, even very large <span class="math inline">\(p\)</span>-values <span class="math inline">\(p_{obs}\approx 1 \gg \alpha\)</span> can occur simply because a given violation of a null-hypothesis is smaller than the involved estimation errors.</p>
<center>
❗The <span class="math inline">\(p\)</span>-value is <strong>not</strong> the probability that the null-hypothesis is true❗
</center>
<p></p>
<p><img src="images/terminator.jpeg" class="img-fluid"></p>
</div>
</div>
<p>Under the alternative, i.e., if for instance<br><span class="math display">\[
H_1: \beta_j \neq 0
\]</span> is true, we want to be able to reject <span class="math inline">\(H_0\)</span> with a large as possible probability. The probability of rejecting a false null-hypothesis is called <strong>Power</strong> <span class="math display">\[
\underbrace{P(\text{reject }H_0 \;|\; H_1\text{ is true})}_{\text{Power}}
\]</span> We want that <span class="math display">\[
\underbrace{P(\text{reject }H_0 \;|\; H_1\text{ is true})}_{\text{Power}} \to 1\quad\text{as}\quad |\beta_j - 0| \to \infty
\]</span> for each given sample size <span class="math inline">\(n,\)</span> and that <span class="math display">\[
\underbrace{P(\text{reject }H_0 \;|\; H_1\text{ is true})}_{\text{Power}} \to 1\quad\text{as}\quad n \to \infty
\]</span> for each given violation of the null-hypothesis <span class="math inline">\(|\beta_j - 0|&gt;0.\)</span></p>
<p>Testing procedures that fulfill the latter property are called <strong>consistent.</strong></p>
</section></section><section id="the-omitted-variable-bias" class="level2" data-number="4.6"><h2 data-number="4.6" class="anchored" data-anchor-id="the-omitted-variable-bias">
<span class="header-section-number">4.6</span> The Omitted Variable Bias</h2>
<p>Multiple linear regression is more than mere composition of single simple linear regression models. Take a look at the following two simple linear regression results:</p>
<p><img src="images/Tab_3_3.png" class="img-fluid"></p>
<p>Observations:</p>
<ul>
<li><p>In the first simple linear regression, we see a statistical significant effect of <code>radio</code> on <code>sales</code>; i.e.&nbsp;we can reject the null hypotheses <span class="math display">\[
H_0:\beta_{radio}=0
\]</span> and adopt the alternative hypotheses <span class="math display">\[
H_1:\beta_{radio}\neq 0.
\]</span></p></li>
<li><p>In the second simple linear regression, we see a statistical significant effect of <code>newspaper</code> on <code>sales</code>; i.e.&nbsp;we can reject the null hypotheses <span class="math display">\[
H_0:\beta_{newspaper}=0
\]</span> and adopt the alternative hypotheses <span class="math display">\[
H_1:\beta_{newspaper}\neq 0.
\]</span></p></li>
</ul>
<p>By contrast, when looking at the <strong>multiple linear regression</strong> when regressing <code>sales</code> onto</p>
<ul>
<li>
<code>TV</code>,</li>
<li>
<code>radio</code> and<br>
</li>
<li>
<code>newspaper</code>,</li>
</ul>
<p>then the effect of <code>newspaper</code> becomes statistically <strong>insignificant</strong>; see Table 3.4.</p>
<p><img src="images/Tab_3_4.png" class="img-fluid"></p>
<p><strong>Reason: Omitted Variable Bias</strong></p>
<p>The reason for this change from a statistically significant effect of <code>newspaper</code> in the simple linear regression, to an insignificant effect in the multiple linear regression is the so-called <strong>Omitted Variable Bias</strong>.</p>
<p>Explanation of the omitted variables bias:</p>
<ul>
<li>
<code>radio</code> has a <strong>true positive effect</strong> on <code>sales</code>
</li>
<li>
<code>newspaper</code> has actually <strong>no effect</strong> on <code>sales</code>
</li>
<li>But, <code>newspaper</code> is correlated with <code>radio</code> <span class="math inline">\(r_{\texttt{newspaper},\texttt{radio}}=0.3541\)</span>; see Table 3.5</li>
</ul>
<p><img src="images/Tab_3_5.png" class="img-fluid"></p>
<ul>
<li>Thus, when <strong>omitting</strong> <code>radio</code> from the regression model, <code>newspaper</code> becomes a surrogate for <code>radio</code> and we see a spurious effect.</li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Interpreting statistically significant results as true effects (“a change of <span class="math inline">\(X_j\)</span> by one unit causes on average a change in <span class="math inline">\(Y\)</span> by <span class="math inline">\(\beta_{j}\)</span>”) is a delectate thing.</p>
<p>Even if the <span class="math inline">\(f(X)\)</span> is really so simple that we can write it as a simple or multiple linear regression model, we may miss to include all relevant predictor variables and thus statistically significant results may only be spurious effects due to omitted variables.</p>
</div>
</div>
<p><strong>Interpretation of the Coefficients in Table 3.4</strong></p>
<p>For fixed values of <code>TV</code> and <code>newspaper</code>, spending additionally 1000 USD for <code>radio</code>, increases on average <code>sales</code> by approximately 189 units.</p>
</section><section id="other-considerations-in-the-regression-model" class="level2" data-number="4.7"><h2 data-number="4.7" class="anchored" data-anchor-id="other-considerations-in-the-regression-model">
<span class="header-section-number">4.7</span> Other Considerations in the Regression Model</h2>
<section id="qualitative-predictors" class="level3" data-number="4.7.1"><h3 data-number="4.7.1" class="anchored" data-anchor-id="qualitative-predictors">
<span class="header-section-number">4.7.1</span> Qualitative Predictors</h3>
<p>Often some predictors are <em>qualitative</em> variables (also known as a <em>factor</em> variables). For instance, the <code>Credit</code> dataset contains the following qualitative predictors:</p>
<ul>
<li>
<code>own</code> (house ownership: yes/no)</li>
<li>
<code>student</code> (student status: yes/no)</li>
<li>
<code>status</code> (marital status: yes/no)</li>
<li>
<code>region</code> (regions: east, west or south)</li>
</ul>
<section id="predictors-with-only-two-levels" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="predictors-with-only-two-levels">Predictors with Only Two Levels</h4>
<p>If a <strong>qualitative predictor (factor)</strong> only has two levels (i.e.&nbsp;possible values), then incorporating it into a regression model is very simple: We simply create an indicator or <strong>dummy variable</strong> that takes on two possible numerical values; for instance, <span class="math display">\[
x_{i} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person owns a house}\\
  0&amp;\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
\]</span> Using this dummy variable as a predictor in the regression equation results in the following regression model: <span class="math display">\[\begin{align*}
y_{i}
&amp;=\beta_0 + \beta_1 x_i + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i &amp;\quad \text{if the $i$th person owns a house}\\
  \beta_0 + \epsilon_i           &amp;\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span>: The average credit card balance among those who do not own a house</li>
<li>
<span class="math inline">\(\beta_0+\beta_1\)</span>: The average credit card balance among those who do own a house</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The average difference in credit card balance between owners and non-owners</li>
</ul>
<p><img src="images/Tab_3_7.png" class="img-fluid"></p>
<p>Alternatively, instead of a 0/1 coding scheme, we could create a dummy variable <span class="math display">\[
x_{i} = \left\{
  \begin{array}{ll}
  1 &amp;\quad \text{if the $i$th person owns a house}\\
-1 &amp;\quad \text{if the $i$th person does not own a house.}
  \end{array}\right.
\]</span> <span class="math display">\[\begin{align*}
y_{i}
&amp;=\beta_0 + \beta_1 x_i + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1 + \epsilon_i&amp;\quad \text{if the $i$th person owns a house}\\
  \beta_0 - \beta_1 + \epsilon_i&amp;\quad \text{if the $i$th person does not own a house}
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span>: The overall average credit card balance (ignoring the house ownership effect)</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The average amount by which house owners and non-owners have credit card balances that are above and below the overall average, respectively.</li>
</ul></section><section id="qualitative-predictors-with-more-than-two-levels" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</h4>
<p>When a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the</p>
<p><code>region</code> <span class="math inline">\(\in\{\)</span><code>South</code>, <code>West</code>, <code>East</code><span class="math inline">\(\}\)</span></p>
<p>variable, we create <strong>two</strong> dummy variables. The first could be <span class="math display">\[
x_{i1} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person is from the South}\\
  0&amp;\quad \text{if the $i$th person is not from the South,}
  \end{array}\right.
\]</span> and the second could be <span class="math display">\[
x_{i2} = \left\{
  \begin{array}{ll}
  1&amp;\quad \text{if the $i$th person is from the West}\\
  0&amp;\quad \text{if the $i$th person is not from the West.}
  \end{array}\right.
\]</span> Using both of these dummy variables results in the following regression model: order to obtain the model <span class="math display">\[\begin{align*}
y_{i}&amp;=\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i\\[2ex]
&amp;= \left\{
  \begin{array}{ll}
  \beta_0 + \beta_1  + \epsilon_i&amp; \quad \text{if the $i$th person is from the South}\\
  \beta_0 + \beta_2  + \epsilon_i&amp; \quad \text{if the $i$th person is from the West}\\
  \beta_0            + \epsilon_i&amp; \quad \text{if the $i$th person is from the East.}\\
  \end{array}\right.
\end{align*}\]</span></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span>: The average credit card balance for individuals from the East</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The difference in the average balance between people from the South versus the East</li>
<li>
<span class="math inline">\(\beta_2\)</span>: The difference in the average balance between people from the West versus the East</li>
</ul>
<p><img src="images/Tab_3_8.png" class="img-fluid"></p>
<p>There are many different ways of coding qualitative variables besides the dummy variable approach taken here. All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular <strong>contrasts</strong>. (A detailed discussion of <em>contrasts</em> is beyond the scope of this lecture.)</p>
</section></section><section id="extensions-of-the-linear-model" class="level3" data-number="4.7.2"><h3 data-number="4.7.2" class="anchored" data-anchor-id="extensions-of-the-linear-model">
<span class="header-section-number">4.7.2</span> Extensions of the Linear Model</h3>
<section id="interaction-effects" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="interaction-effects">Interaction Effects</h4>
<p>Consider the following model</p>
<center>
<code>sales</code> <span class="math inline">\(= \beta_0 + \beta_1\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\)</span> <code>radio</code> <span class="math inline">\(+ \beta_3\)</span> <code>newspaper</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<p>which states, for instance, that the average increase in <code>sales</code> associated with a one-unit increase in <code>TV</code> is <span class="math inline">\(\beta_1,\)</span> regardless of the amount spent on <code>radio</code>.</p>
<p>However, this simple model may be incorrect. Suppose that there is a synergy effect, such that spending money on <code>radio</code> advertising actually increases the effectiveness of <code>TV</code> advertising.</p>
<p>Figure 3.5 suggests that such an effect may be present in the advertising data:</p>
<ul>
<li>When levels of either <code>TV</code> or <code>radio</code> are low, then the true <code>sales</code> are lower than predicted by the linear model.</li>
<li>But when advertising is split between the two media, then the model tends to <strong>underestimate</strong> sales. <img src="images/Fig_3_5.png" class="img-fluid">
</li>
</ul>
<p><strong>Solution: Interaction Effects:</strong></p>
<p>Consider the standard linear regression model with two variables, <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon.
\]</span> Here each predictor <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> has a given effect, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, on <span class="math inline">\(Y\)</span> and this effect does not depend on the value of the other predictor. <strong>(Additive Assumption)</strong></p>
<p>One way of extending this model is to include a third predictor, called an <strong>interaction term</strong>, which is constructed by computing the product of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2.\)</span> This results in the model <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 \overbrace{\color{red}X_1X_2}^{=X_3} + \epsilon.
\]</span> This is a powerful extension relaxing the additive assumption. Notice that the model can now be written as <span class="math display">\[
\begin{align*}
Y &amp;= \beta_0 + \underbrace{(\beta_1 + \beta_3 X_2)}_{=\tilde{\beta}_1(X_2)} X_1 + \beta_2 X_2 + \epsilon,
\end{align*}
\]</span> where the new slope parameter <span class="math inline">\(\tilde{\beta}_2(X_2)\)</span> is a linear function of <span class="math inline">\(X_2,\)</span> i.e. <span class="math display">\[
\tilde{\beta}_1(X_2)=\beta_1 + \beta_3 X_2.
\]</span></p>
<p>Thus, a change in the value of <span class="math inline">\(X_2\)</span> will change the association between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y.\)</span></p>
<p>A similar argument shows that a change in the value of <span class="math inline">\(X_1\)</span> changes the association between <span class="math inline">\(X_2\)</span> and <span class="math inline">\(Y.\)</span></p>
<p>Let us return to the <code>Advertising</code> example: A linear model that predicts <code>sales</code> using</p>
<ul>
<li>the predictor <code>radio</code>,</li>
<li>the predictor <code>TV</code>, and</li>
<li>the interaction <code>radio</code><span class="math inline">\(\times\)</span><code>TV</code>
</li>
</ul>
<p>takes the form</p>
<center>
<code>sales</code> <span class="math inline">\(= \beta_0 + \beta_1\times\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\times\)</span> <code>radio</code> <span class="math inline">\(+ \beta_3\times(\)</span> <code>radio</code><span class="math inline">\(\times\)</span> <code>TV</code><span class="math inline">\()+\epsilon\)</span>
</center>
<p> which can be rewritten as</p>
<center>
<code>sales</code> <span class="math inline">\(=\beta_0 + (\beta_1+ \beta_3\times\)</span> <code>radio</code> <span class="math inline">\()\times\)</span> <code>TV</code> <span class="math inline">\(+ \beta_2\times\)</span> <code>radio</code> <span class="math inline">\(+\epsilon\)</span>
</center>
<p><br></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<span class="math inline">\(\beta_3\)</span> denotes the increase in the effectiveness of TV advertising associated with a one-unit increase in radio advertising.</li>
</ul>
<p><img src="images/Tab_3_9.png" class="img-fluid"></p>
<p><strong>Interpretation of Table 3.9:</strong></p>
<ul>
<li>Both separate main effects, <code>TV</code> and <code>radio</code>, are statistically significant (<span class="math inline">\(p\)</span>-values smaller than 0.01).</li>
<li>Additionally, the <span class="math inline">\(p\)</span>-value for the interaction term, <code>TV</code><span class="math inline">\(\times\)</span><code>radio</code>, is extremely low, indicating that there is strong evidence for <span class="math inline">\(H_1: \beta_3\neq 0.\)</span> In other words, it is clear that the true relationship is not additive.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hierarchical Principle of Interaction Terms
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we include an interaction in a model, we should also include the main effects, even if the <span class="math inline">\(p\)</span>-values associated with their coefficients are not significant.</p>
</div>
</div>
<p><strong>Interactions with Qualitative Variables:</strong></p>
<p>An interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation.</p>
<p>Consider the <code>Credit</code> data set and suppose that we wish to predict <code>balance</code> using the predictors:</p>
<ul>
<li>
<code>income</code> (quantitative) and</li>
<li>
<code>student</code> (qualitative) using a dummy variable with <span class="math display">\[
x_{i2}=\left\{
\begin{array}{ll}
1&amp;\text{if person $i$ is a student}\\
0&amp;\text{if not}\\
\end{array}
\right.
\]</span>
</li>
</ul>
<p>In the absence of an interaction term, the model takes the form <img src="images/Eq_3_34.png" class="img-fluid"></p>
<p>Thus, the regression lines for students and non-students have different intercepts, <span class="math inline">\(\beta_0+\beta_2\)</span> versus <span class="math inline">\(\beta_0\)</span>, <strong>but the same slope</strong> <span class="math inline">\(\beta_1\)</span>.</p>
<p>This represents a potentially serious limitation of the model, since a change in <code>income</code> may have a very different effect on the credit card <code>balance</code> of a student versus a non-student.</p>
<p>This limitation can be addressed by adding an interaction variable, created by multiplying <code>income</code> with the dummy variable for student. Our model now becomes <img src="images/Eq_3_35.png" class="img-fluid"></p>
<p>Now we have different intercepts for students and non-students but also different slopes for these groups. <img src="images/Fig_3_7.png" class="img-fluid"></p>
</section><section id="polynomial-regression-non-linear-relationships" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="polynomial-regression-non-linear-relationships">Polynomial Regression: Non-linear Relationships</h4>
<p>Polynomial regression allows to accommodate non-linear relationships between the predictors <span class="math inline">\(X\)</span> and the outcome <span class="math inline">\(Y.\)</span> <img src="images/Fig_3_8.png" class="img-fluid"></p>
<p>For example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that a model of the form</p>
<center>
<code>mpg</code> <span class="math inline">\(=\beta_0 + \beta_1\times\)</span> <code>horsepower</code> <span class="math inline">\(+ \beta_2\times(\)</span><code>horsepower</code><span class="math inline">\()^2+\epsilon\)</span>
</center>
<p></p>
<p>This regression model involves predicting <code>mpg</code> using a non-linear function of <code>horsepower</code>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>But it is still a linear model!</strong> It’s simply a multiple linear regression model <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
\]</span> with</p>
<ul>
<li>
<span class="math inline">\(X_1=\)</span><code>horsepower</code> and</li>
<li>
<span class="math inline">\(X_2 =(\)</span><code>horsepower</code><span class="math inline">\()^2\)</span>
</li>
</ul>
<p>as the predictor variables.</p>
</div>
</div>
<p>Since this is nothing but a multiple linear regression model, we can use standard linear regression software to estimate <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> in order to fit the (quadratic) non-linear regression function.</p>
<p><img src="images/Tab_3_10.png" class="img-fluid"></p>
</section></section><section id="detecting-potential-problems" class="level3" data-number="4.7.3"><h3 data-number="4.7.3" class="anchored" data-anchor-id="detecting-potential-problems">
<span class="header-section-number">4.7.3</span> Detecting Potential Problems</h3>
<p>In this chapter, we discuss the use of <strong>diagnostic plots</strong> to detect potential problems.</p>
<p><strong>1. Non-linearity of the response-predictor relationships.</strong></p>
<p><strong>Diagnostic residual plots</strong> are most useful to detect possible non-linear response-predictor relationships.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span> </span>
<span></span>
<span><span class="co">## Gives the variable names in the Auto dataset</span></span>
<span><span class="co"># names(Auto)</span></span>
<span></span>
<span><span class="co">## Simple linear regression</span></span>
<span><span class="va">lmobj_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Quadratic regression </span></span>
<span><span class="va">lmobj_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Diagnostic Plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_1</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_2</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Plotting <strong>residuals versus fitted values</strong>, i.e.&nbsp;plotting the data pairs <span class="math display">\[
(\underbrace{y_i - \hat{y}_i}_{=e_i}, \hat{y}_i),\quad\text{for all}\quad i=1,\dots,n
\]</span> is a useful graphical tool for identifying non-linearity which works for the</p>
<ul>
<li>
<strong>simple linear regression model</strong> with <span class="math inline">\(\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_{i1}\)</span> and the</li>
<li>
<strong>multiple linear regression model</strong> with <span class="math inline">\(\hat{y}_i=\hat{\beta}_0+\sum_{j=1}^p\hat{\beta}_jx_{ij}\)</span>
</li>
</ul>
<p>If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as <span class="math display">\[
\log(X),\; \sqrt{X},\; \text{or}\; X^2
\]</span> in the regression model.</p>
<!-- In the later chapters, we will discuss other more advanced non-linear approaches for addressing this issue. -->
<p><strong>2. Correlation of Error Terms</strong></p>
<p>An important assumption of the linear regression model is that the error terms, <span class="math display">\[
\epsilon_1, \epsilon_2, \dots , \epsilon_n,
\]</span> are independent and thus uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that <span class="math inline">\(\epsilon_i\)</span> is positive provides little or no information about the sign of <span class="math inline">\(\epsilon_{i+1}.\)</span></p>
<p>Auto-correlations among the error terms typically occur in time series data. Figure 3.10 shows time-series of residuals with</p>
<ul>
<li>no auto-correlation (<span class="math inline">\(\rho=0\)</span>)</li>
<li>intermediate auto-correlation (<span class="math inline">\(\rho=0.5\)</span>)</li>
<li>strong auto-correlation (<span class="math inline">\(\rho=0.9\)</span>)</li>
</ul>
<p><img src="images/Fig_3_10.png" class="img-fluid"></p>
<p><strong>3. Non-Constant Variance of Error Terms (Heteroskedasticity)</strong></p>
<p>Another important issue is to check whether the errors can be assumed homoskedastic <span class="math display">\[
Var(\epsilon_i|X_i) = \sigma^2,\quad\text{for all}\quad i=1,\dots,n.
\]</span> or heteroskedastic <span class="math display">\[
Var(\epsilon_i|X_i) = \sigma^2_i,\quad i=1,\dots,n.
\]</span> One can identify <strong>non-constant variances (“heteroskedasticity”)</strong> in the errors, using diagnostic residual plots. Often one observes that the magnitude of the scattering of the residuals tends to increase with the fitted values. When faced with this problem, one possible solution is to transform the response <span class="math inline">\(Y\)</span> using a concave function such as <span class="math display">\[
\log(Y)\;\text{ or }\; \sqrt{Y}.
\]</span> Such a transformation results in a greater amount of shrinkage of the larger responses.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Quadratic regression </span></span>
<span><span class="va">lmobj_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Quadratic regression with transformed response log(Y)</span></span>
<span><span class="va">lmobj_3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">)</span><span class="op">)</span> <span class="op">~</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Diagnostic Plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_2</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmobj_3</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<!-- 
::: {.callout-caution}
The standard formulas for 

* standard errors, 
* confidence intervals, and 
* hypothesis tests 

in this chapter are based on the assumption of 

* uncorrelated error terms $Cor(\epsilon_i,\epsilon_j)=0$ for all $i\neq j$  
* with equal variances $Var(\epsilon_i)=\sigma^2$ for all $i=1,\dots,n.$  

If in fact there is correlation and/or heteroskedasticity among the error terms, then the estimated standard errors will be wrong leading to invalid inferences. 

Thus, if the error terms are auto-correlated and/or heteroskedastic, we need to take this into account by using so-called auto-correlation and/or heteroskedasticity robust standard errors. 

The `R` package [sandwich](https://cran.r-project.org/web/packages/sandwich/index.html) contains such robust standard error estimators. 
::: 
-->
<p><strong>Note:</strong> In case of doubt (homoskedastic vs heteroskedastic error terms), one should use HC-robust inference, which allows for valid inference under both homoskedastic and heterskedastic errors.</p>
<p><strong>4. Outliers</strong></p>
<p>An outlier is a point <span class="math inline">\(i\)</span> for which <span class="math inline">\(y_i\)</span> is far from the value <span class="math inline">\(\hat{y}_i\)</span> predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p>
<p>Outliers typically have a strong effect on the <span class="math inline">\(R^2\)</span> value since they add a <strong>very large residual</strong> to its computation.</p>
<p><strong>Harmless Outlier:</strong> Figure 3.12 shows a clear outlier (observation 20) which, however, has a typical predictor value <span class="math inline">\(x_i\)</span>; i.e.&nbsp;the <span class="math inline">\(x_i\)</span>-value is right in the center of all predicor values. Such outliers have little effect on the regression fit. <img src="images/Fig_3_12.png" class="img-fluid"></p>
<p><strong>Harmful Outlier:</strong> Figure 3.13 shows again a clear outlier (observation 41) which, however, has a predictor value <span class="math inline">\(x_i\)</span> that is <strong>very atypical</strong>. Such outliers are said to have <strong>large leverage</strong> giving them power to affect the regression fit considerably. <img src="images/Fig_3_13.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Critical outliers have both,</p>
<ul>
<li><strong>large residuals</strong></li>
</ul>
<p><em>and</em></p>
<ul>
<li>
<strong>large leverage</strong>.</li>
</ul>
</div>
</div>
<p><strong>5. High Leverage Points</strong></p>
<p>In order to quantify an observation’s leverage, we compute the <strong>leverage statistic</strong> <span class="math inline">\(h_i\)</span> for each observation <span class="math inline">\(i=1,\dots,n.\)</span> A large value of this statistic indicates an observation with high leverage.</p>
<p>In the case of the <strong>simple linear regression model</strong> <span class="math display">\[
h_i = \frac{1}{n} + \frac{(x_i-\bar{x})^2}{\sum_{j=1}^n(x_{j}-\bar{x})^2}.
\]</span></p>
<p>In the case of the <strong>multiple linear regression model</strong>, <span class="math inline">\(h_i\)</span> is the <span class="math inline">\(i\)</span>th diagonal value of the <span class="math inline">\((n\times n)\)</span>-dimensional “hat-matrix” <span class="math display">\[
H=X(X'X)^{-1}X'.
\]</span></p>
<ul>
<li>The leverage statistic <span class="math inline">\(h_i\)</span> is always between <span class="math inline">\(1/n\)</span> and <span class="math inline">\(1\)</span>
</li>
<li>The average leverage for all the observations is equal to <span class="math display">\[
\bar{h}=\frac{1}{n}\sum_{i=1}^n h_i=(p + 1)/n.
\]</span>
</li>
<li>If a given observation has a leverage statistic <span class="math inline">\(h_i\)</span> that greatly exceeds the average leverage value, <span class="math inline">\((p+1)/n,\)</span> then we may suspect that the corresponding point has high leverage.</li>
</ul>
<p><strong>6. Collinearity</strong></p>
<p>Collinearity refers to the situation in which two or more predictor variables are closely related to one another.</p>
<p>In the following example, the variables <code>Age</code> and <code>Limit</code> are essentially unrelated, but the variables <code>Rating</code> and <code>Limit</code> are closely related to one another.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Credit</span><span class="op">)</span> <span class="co"># names(Credit)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Age</span>,    x <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Limit</span>, main <span class="op">=</span> <span class="st">"No Collinearity"</span>, ylab <span class="op">=</span> <span class="st">"Age"</span>, xlab <span class="op">=</span> <span class="st">"Limit"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Rating</span>, x <span class="op">=</span> <span class="va">Credit</span><span class="op">$</span><span class="va">Limit</span>, main <span class="op">=</span> <span class="st">"Strong Collinearity"</span>, ylab <span class="op">=</span> <span class="st">"Rating"</span>, xlab <span class="op">=</span> <span class="st">"Limit"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The left panel of Figure 3.15 shows that, in the case of unrelated predictors (<code>Age</code> and <code>Limit</code>), the least squares problem has a minimum <span class="math inline">\((\hat\beta_{Age},\hat\beta_{Limit})\)</span> that is well identified since the minimum is well defined.</p>
<p>The right panel of Figure 3.15 shows that, in the case of collinear predictors (<code>Rating</code> and <code>Limit</code>), the least squares problem has a minimum <span class="math inline">\((\hat\beta_{Rating},\hat\beta_{Limit})\)</span> that is not well identified: One can substitute values of <span class="math inline">\(\hat\beta_{Limit}'\)</span> for <span class="math inline">\(\hat\beta_{Rating}'\)</span> ending up in new pairs <span class="math inline">\((\hat\beta_{Rating}',\hat\beta_{Limit}')\)</span> with basically the same RSS-value than the original value than it is achieved by the minimizer <span class="math inline">\((\hat\beta_{Rating},\hat\beta_{Limit})\)</span>.</p>
<p><img src="images/Fig_3_15.png" class="img-fluid"></p>
<p>Table 3.11 demonstrates that this identification problem between the collinear predictors (<code>Rating</code> and <code>Limit</code>) causes a variance inflation in the variance (square of standard error) of the estimators <span class="math inline">\(\hat\beta_{Rating}\)</span> and <span class="math inline">\(\hat\beta_{Limit}.\)</span></p>
<ul>
<li>In Model 1: <span class="math inline">\(\hat\beta_{Limit} = 0.005^2=0.000025\)</span>
</li>
<li>In Model 2: <span class="math inline">\(\hat\beta_{Rating} = 0.064^2=0.004096\)</span>
</li>
</ul>
<p><img src="images/Tab_3_11.png" class="img-fluid"></p>
<p>We call this situation <strong>multicollinearity</strong>.</p>
<p>To detect multicollinearity issues, one can use the variance inflation factor (VIF) <span class="math display">\[
\operatorname{VIF}(\hat{\beta}_j)=\frac{1}{1-R^2_{X_j|X_-j}},
\]</span> where <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors.</p>
<ul>
<li>If <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is close to one, then multicollinearity is present, and <span class="math inline">\(\operatorname{VIF}(\hat{\beta}_j)\)</span> will be large.</li>
</ul>
<p>In the <code>Credit</code> data, one gets for the predictors <code>age</code>, <code>rating</code>, and <code>limit</code> the following VIF values:</p>
<ul>
<li>1.01 (<code>age</code>)</li>
<li>160.67 (<code>rating</code>)</li>
<li>160.59 (<code>limit</code>)</li>
</ul>
<p>Thus, as we suspected, there is considerable collinearity in the data!</p>
<p>Possible solutions:</p>
<ol type="1">
<li><p>Drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. <br><strong>Caution:</strong> In econometrics, dropping control variables is generally not a good idea since control variables are there to rule out possible issues with omitted variables biases.</p></li>
<li><p>Combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.</p></li>
<li><p>Use a different estimation procedure like ridge regression.</p></li>
<li><p>Live with it. At least you know where the large stand errors are coming from.</p></li>
</ol></section></section><section id="comparison-linear-regression-vs.-k-nn-regression" class="level2" data-number="4.8"><h2 data-number="4.8" class="anchored" data-anchor-id="comparison-linear-regression-vs.-k-nn-regression">
<span class="header-section-number">4.8</span> Comparison: Linear Regression vs.&nbsp;K-NN Regression</h2>
<p>Linear regression is an example of a parametric approach because it assumes a linear model form for <span class="math display">\[
f(X_i)=\beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}
\]</span></p>
<p><strong>Advantages of parametric approaches:</strong></p>
<ul>
<li>Typically easy to fit</li>
<li>Simple interpretation</li>
<li>Simple inference</li>
</ul>
<p><strong>Disadvantages of parametric approaches:</strong></p>
<ul>
<li>The parametric model assumption can be far from true; i.e. <span class="math display">\[
f(X_i) \neq \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}
\]</span>
</li>
</ul>
<p><strong>Alternative:</strong></p>
<p><strong>Non-parametric methods</strong> such as <strong>K-nearest neighbors regression</strong> since non-parametric approaches do not explicitly assume a parametric form for <span class="math inline">\(f(X).\)</span></p>
<section id="k-nearest-neighbors-k-nn-regression" class="level3" data-number="4.8.1"><h3 data-number="4.8.1" class="anchored" data-anchor-id="k-nearest-neighbors-k-nn-regression">
<span class="header-section-number">4.8.1</span> K-Nearest Neighbors (K-NN) Regression</h3>
<p>Let <span class="math inline">\(x_0\in\mathbb{R}^p\)</span> denote a certain (multivariate) predictor value at which we want to estimate <span class="math display">\[
f(x_0)
\]</span> and let <span class="math inline">\(K\)</span> denote the number of closest predictior value neighbors of <span class="math inline">\(x_0.\)</span></p>
<p>KNN regression regression then computes the estimate <span class="math display">\[
\hat{f}_K(x_0)
\]</span> in two steps:</p>
<ol type="1">
<li>Compute the distances between <span class="math inline">\(x_0\)</span> and all training data predictior values <span class="math inline">\(X_1,\dots,X_{n_{Train}}\)</span> <span class="math display">\[
d(x_0,X_1),d(x_0,X_2)\dots,d(x_0,X_{n_{Train}}).
\]</span> Use these distances to identify the <span class="math inline">\(K\)</span> training data predictior values <span class="math inline">\(X_1,\dots,X_{n_{Train}}\)</span> that are closest to <span class="math inline">\(x_0\)</span> and collect their indices the index set <span class="math inline">\(\mathcal{N}_0,\)</span> where <span class="math display">\[
\begin{align*}
\mathcal{N}_0
&amp; =\{i\in\{1,2,\dots,n_{Train}\} \; |\; d(x_0,X_i)\text{ is one of the $K$ smallest distances}\}
\end{align*}
\]</span> such that <span class="math inline">\(\mathcal{N}_0\subset\{1,2,\dots,n_{Train}\}\)</span> with <span class="math inline">\(|\mathcal{N}_0|=K.\)</span>
</li>
<li>Estimate <span class="math inline">\(f(x_0)\)</span> using the sample average of all the training responses <span class="math inline">\(y_i\)</span> with <span class="math inline">\(i\in\mathcal{N}_0,\)</span> i.e.&nbsp; <span class="math display">\[
\hat{f}_K(x_0)=\frac{1}{K}\sum_{i\in\mathcal{N}_0}y_i.
\]</span>
</li>
</ol>
<p>The above two steps are then repeated for all predictior values <span class="math inline">\(x_0\in\mathbb{R}^p\)</span> of interest.</p>
<p>The performance of the estimator <span class="math inline">\(\hat{f}_K(x_0)\)</span> depends on</p>
<ul>
<li>the choice of <span class="math inline">\(K\)</span> and</li>
<li>the choice of distance <span class="math inline">\(d\)</span>
</li>
</ul>
<p>For real valued predictors, <span class="math inline">\(x_0,X_i\in\mathbb{R}^p\)</span> a usual choice is the Euclidian distance <span class="math display">\[
d_E(x_0, X_i) = ||x_0 - X_i||^2 = \sum_{j=1}^p (x_{0j} - X_{ij})^2.
\]</span></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Use Standardized Predictors!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Typically, it is important to compute the distances with respect to the standardized (centering, and scaling to unit variance) predictor variables; i.e. <span class="math display">\[
d^*_E(x_0, X_i) = ||x^*_0 - X^*_i||^2 = \sum_{j=1}^p (x^*_{0j} - X^*_{ij})^2,
\]</span> where <span class="math display">\[
x^*_{0j} = \frac{x_{0j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
\]</span> and <span class="math display">\[
X^*_{ij} = \frac{X_{ij} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
\]</span> with <span class="math inline">\(\bar{X}_{j} = \frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}X_{ij}.\)</span></p>
<p>Otherwise, the distance values could be dominated by one of the <span class="math inline">\(p\)</span> predictors.</p>
<p>E.g. when one predictor is age (values between <span class="math inline">\(0\)</span> and <span class="math inline">\(99\)</span>) and another predictor is yearly income (values between <span class="math inline">\(0\)</span> and <span class="math inline">\(12,000,000\)</span>), then the differences in income will dominate the differences in age only because of the different scales. <!-- 
$$
\frac{X_{1j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}},\dots,\frac{X_{n_{Train}j} - \bar{X}_{j}}{\sqrt{\frac{1}{n_{Train}}\sum_{i=1}^{n_{Train}}(X_{ij}-\bar{X}_{j})^2}}
$$  --></p>
</div>
</div>
<p>The left panel of Figure 3.16 shows the estimation result for <span class="math inline">\(K=1\)</span> and the right panel for <span class="math inline">\(K=9.\)</span></p>
<p><img src="images/Fig_3_16.png" class="img-fluid"></p>
<p>In general, the optimal value for <span class="math inline">\(K\)</span> will depend on the <em>bias-variance tradeoff</em>, which we introduced in <a href="Ch2_StatLearning.html" class="quarto-xref"><span>Chapter 2</span></a>:</p>
<p><strong>A small value for <span class="math inline">\(K\)</span></strong> provides the most flexible fit, which will have</p>
<ul>
<li>low bias <span class="math display">\[
    |\operatorname{Bias}(\hat{f}_K(x_0))| = |E(\hat{f}_K(x_0)) - f(x_0)| \;\text{ is small}
    \]</span>
</li>
<li>high variance <span class="math display">\[
    Var(\hat{f}_K(x_0))  \;\text{ is large}
    \]</span>
</li>
</ul>
<p>The low bias is due to the fact that the prediction <span class="math inline">\(\hat{f}_K(x_0)\)</span> at a given <span class="math inline">\(x_0\)</span> only uses a few (e.g.&nbsp;<span class="math inline">\(K=1\)</span>) close neighbors for which we can expect that they are “good neighbors:” Close neighbors are <strong>good neighbors</strong> since <span class="math inline">\(|f(x_0) - f(X_i)|\approx 0.\)</span></p>
<p>The high variance is due to the fact that the prediction <span class="math inline">\(\hat{f}_K(x_0)\)</span> at a given <span class="math inline">\(x_0\)</span> only depends on a small number of <span class="math inline">\(K\)</span> observations (e.g.&nbsp;<span class="math inline">\(K=1\)</span>) such that the law of larger numbers had no chance to reduce variance.</p>
<p><strong>A large value of <span class="math inline">\(K\)</span></strong> provides a less flexible fit, which will have</p>
<ul>
<li>large bias <span class="math display">\[
    |\operatorname{Bias}(\hat{f}_K(x_0))| = |E(\hat{f}_K(x_0)) - f(x_0)| \;\text{ is large}
    \]</span>
</li>
<li>low variance <span class="math display">\[
    Var(\hat{f}_K(x_0))  \;\text{ is small}
    \]</span>
</li>
</ul>
<p>The large bias is due to the fact that the prediction <span class="math inline">\(\hat{f}_K(x_0)\)</span> at a given <span class="math inline">\(x_0\)</span> uses observations from a larger neighborhood (e.g.&nbsp;<span class="math inline">\(K=30\)</span>) which increases the chance of considering rather distant <span class="math inline">\(||x_0-X_i||\gg 0\)</span> and thus “bad” neigboors. Distant neighbors are <strong>bad neighbors</strong> since <span class="math inline">\(|f(x_0) - f(X_i)|\gg 0.\)</span></p>
<p>The low variance is due to the fact that the prediction <span class="math inline">\(\hat{f}_K(x_0)\)</span> at a given <span class="math inline">\(x_0\)</span> depends on a larger number of <span class="math inline">\(K\)</span> observations (e.g.&nbsp;<span class="math inline">\(K=30\)</span>) such that the law of larger numbers has a chance to reduce variance.</p>
<p><strong>An optimal value of <span class="math inline">\(K\)</span></strong> can be chosen using, e.g., cross-validation; see <a href="Ch6_ResamplingMethods.html" class="quarto-xref"><span>Chapter 6</span></a>.</p>
<p>Generally, the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of <span class="math inline">\(f\)</span> and vice versa.</p>
<p><strong>Figure 3.17</strong> provides an example with data generated from a one-dimensional linear regression model:</p>
<ul>
<li>black solid lines: true <span class="math inline">\(f(x)\)</span>
</li>
<li>blue curves: KNN fits <span class="math inline">\(\hat{f}_K(x)\)</span> using <span class="math inline">\(K = 1\)</span> (left plot) and <span class="math inline">\(K = 9\)</span> (right plot).</li>
</ul>
<p>Observations:</p>
<ul>
<li>The KNN fit <span class="math inline">\(\hat{f}_K(x)\)</span> using <span class="math inline">\(K = 1\)</span> is far too wiggly</li>
<li>The KNN fit <span class="math inline">\(\hat{f}_K(x)\)</span> using <span class="math inline">\(K = 9\)</span> is much closer to the true <span class="math inline">\(f(X).\)</span>
</li>
</ul>
<p>However, since the true regression function is here linear, it is hard for a non-parametric approach to compete with simple linear regression: a non-parametric approach incurs a cost in variance that is here not offset by a reduction in bias. <img src="images/Fig_3_17.png" class="img-fluid"></p>
<p>The blue dashed line in the left-hand panel of <strong>Figure 3.18</strong> represents the simple linear regression fit to the same data. It is almost perfect.</p>
<p>The right-hand panel of <strong>Figure 3.18</strong> reveals that linear regression outperforms KNN for this data across different choices of <span class="math inline">\(K=1,2,\dots,10.\)</span> <img src="images/Fig_3_18.png" class="img-fluid"></p>
<p><strong>Figure 3.19</strong> displays a non-linear situations in which KNN performs much better than simple linear regression. <img src="images/Fig_3_19.png" class="img-fluid"></p>
<section id="curse-of-dimensionality" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="curse-of-dimensionality">Curse of Dimensionality</h4>
<p>Unfortunately, in higher dimensions, KNN often performs worse than simple/multiple linear regression, since non-parametric approaches suffer from the <strong>curse of dimensionality</strong>.</p>
<p><strong>Figure 3.20</strong> considers the same strongly non-linear situation as in the second row of <strong>Figure 3.19</strong>, except that we have added additional noise (i.e.&nbsp;redundant) predictors that are not associated with the response.</p>
<ul>
<li>When <span class="math inline">\(p = 1\)</span> or <span class="math inline">\(p = 2,\)</span> KNN outperforms linear regression.</li>
<li>But for <span class="math inline">\(p = 3\)</span> the results are mixed, and for <span class="math inline">\(p\geq 4\)</span> linear regression is superior to KNN. <img src="images/Fig_3_20.png" class="img-fluid">
</li>
</ul>
<p>Observations:</p>
<ul>
<li>When <span class="math inline">\(p=1\)</span>, a sample size of <span class="math inline">\(n=50\)</span> can provide enough information to estimate <span class="math inline">\(f(X)\)</span> accurately using non-parametric methods since the <span class="math inline">\(K\)</span> nearest neighbors can actually be close to a given test observation <span class="math inline">\(x_0.\)</span>
</li>
<li>However, when spreading the <span class="math inline">\(n=50\)</span> data points over a large number of, for instance, <span class="math inline">\(p=20\)</span> dimensions, the <span class="math inline">\(K\)</span> nearest neighbors tend to become far away from <span class="math inline">\(x_0\)</span> causing a large bias.</li>
</ul></section></section></section><section id="assignment" class="level2" data-number="4.9"><h2 data-number="4.9" class="anchored" data-anchor-id="assignment">
<span class="header-section-number">4.9</span> Assignment</h2>
<p>Consider the following data generating process:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## A Function to simulate data from a simple linear regression model</span></span>
<span><span class="va">myDataGenerator</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span><span class="op">{</span></span>
<span>  </span>
<span>  <span class="va">n</span>      <span class="op">&lt;-</span> <span class="fl">50</span>                                    <span class="co"># sample size</span></span>
<span>  <span class="va">beta_0</span> <span class="op">&lt;-</span> <span class="fl">0.1</span>                                   <span class="co"># intercept parameter</span></span>
<span>  <span class="va">beta_1</span> <span class="op">&lt;-</span> <span class="fl">5</span>                                     <span class="co"># slope parameter</span></span>
<span>  <span class="va">X</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, min <span class="op">=</span> <span class="op">-</span><span class="fl">2</span>, max <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>           <span class="co"># predictor</span></span>
<span>  <span class="va">error</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="op">(</span><span class="fl">5</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="co"># heteroskedastic error term</span></span>
<span>  <span class="va">Y</span>      <span class="op">&lt;-</span> <span class="va">beta_0</span> <span class="op">+</span> <span class="va">beta_1</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">error</span>           <span class="co"># generate Y </span></span>
<span>  </span>
<span>  <span class="co">## return realization of the random sample</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="st">"Y"</span> <span class="op">=</span> <span class="va">Y</span>, <span class="st">"X"</span> <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Each time you call the function <code>myDataGenerator()</code>, you generate a new realization of the random sample <span class="math display">\[
((Y_1,X_1'),\dots,(Y_n,X_n'))
\]</span> as defined by the data generating process specified in the function-body of <code>myDataGenerator()</code></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">data_sim</span>    <span class="op">&lt;-</span> <span class="fu">myDataGenerator</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data_sim</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           Y           X
1  -2.083072 -1.05464044
2   7.949106  0.30174268
3  -5.751286 -0.07096835
4  -4.556137  0.27741075
5  -6.019922 -1.42339074
6 -10.144683 -1.41716495</code></pre>
</div>
</div>
<p>For a given data generating process (as, for instance, defined in <code>myDataGenerator()</code>), we can approximate the bias of the OLS estimator by <span class="math display">\[
\widehat{\operatorname{Bias}}(\hat\beta) = \widehat{E}(\hat\beta) - \beta,
\]</span> where <span class="math display">\[
\begin{align*}
\widehat{E}(\hat\beta)
&amp; = \frac{1}{B}\sum_{j=1}^B \hat\beta_{b}\\[2ex]
&amp; = \frac{1}{B}\sum_{j=1}^B \begin{pmatrix}\hat\beta_{0,b}\\\hat\beta_{1,b}\end{pmatrix}\\[2ex]
&amp; = \begin{pmatrix}\frac{1}{B}\sum_{j=1}^B \hat\beta_{0,b}\\ \frac{1}{B}\sum_{j=1}^B \hat\beta_{1,b}\end{pmatrix}
\end{align*}
\]</span> denotes the sample mean across <span class="math inline">\(B\)</span>-many simulated realizations <span class="math display">\[
\hat\beta_1,\hat\beta_2,\dots,\hat\beta_B
\]</span> of the OLS estimator <span class="math inline">\(\hat\beta,\)</span> and where <span class="math inline">\(\beta\)</span> denotes the true parameter vector; i.e., here <span class="math display">\[
\beta
= \begin{pmatrix}\beta_{0}\\ \beta_{1}\end{pmatrix}
= \begin{pmatrix}0.1\\ 5\end{pmatrix}.
\]</span></p>
<p>By the law of large numbers, the approximation <span class="math display">\[
\widehat{\operatorname{Bias}}(\hat\beta) \approx \operatorname{Bias}(\hat\beta)
\]</span> becomes arbitrarily precise as <span class="math inline">\(B\to\infty.\)</span> Setting, for instance, <span class="math inline">\(B=100000\)</span> typically guarantees a very good approximation.</p>
<section id="assignment-task-nr.-1-bias-of-the-ols-estimator" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="assignment-task-nr.-1-bias-of-the-ols-estimator"><strong>Assignment Task Nr. 1 (Bias of the OLS Estimator):</strong></h4>
<ol type="1">
<li>Use the above given data generating process to generate <span class="math inline">\(B=100,000\)</span> OLS estimates of the true (usually unknown) parameter vector <span class="math inline">\(\beta'=(\beta_0,\beta_1).\)</span> Use two boxplot (one for the estimates of <span class="math inline">\(\beta_0\)</span> and another for those of <span class="math inline">\(\beta_1\)</span>) to visualize the estimation results.<br>
</li>
<li>Check the bias of the OLS estimator for the above given data generating process using the above described (Monte Carlo) simulation approach.</li>
<li>Based on your simulation results, can you state that the OLS estimatior is <em>generally</em> biased/unbiased?</li>
</ol></section><section id="assignment-task-nr.-2-coverage-probability-of-confidence-intervals" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="assignment-task-nr.-2-coverage-probability-of-confidence-intervals"><strong>Assignment Task Nr. 2 (Coverage Probability of Confidence Intervals):</strong></h4>
<ol type="1">
<li>Use the above given data generating process to generate <span class="math inline">\(B=100,000\)</span> realizations <span class="math display">\[
\operatorname{CI}^{1-\alpha}_{\beta_1,1},\operatorname{CI}^{1-\alpha}_{\beta_1,2},\dots,\operatorname{CI}^{1-\alpha}_{\beta_1,B}
\]</span> of the <span class="math inline">\((1-\alpha)\cdot 100 \%\)</span> confidence interval <span class="math inline">\(\operatorname{CI}^{1-\alpha}_{\beta_1}\)</span> for <span class="math inline">\(\beta_1.\)</span> Set <span class="math inline">\(\alpha=0.05.\)</span>
</li>
<li>Approximate the true coverage probability using the empirical coverage probability across all <span class="math inline">\(B\)</span> simulated confidence interals: <span class="math display">\[
P(\beta_1^{(H_0)} \in \operatorname{CI}^{1-\alpha}_{\beta_1})\approx \frac{1}{B}\sum_{j=1}^B 1_{\left(\beta_1^{(H_0)} \in \operatorname{CI}^{1-\alpha}_{\beta_1,j}\right)},
\]</span> where <span class="math inline">\(1_{(\text{TRUE})}=1\)</span> and <span class="math inline">\(1_{(\text{FALSE})}=0.\)</span><br>
Consider the following two cases:
<ol type="a">
<li>The null-hypothesis is true, i.e.&nbsp;<span class="math inline">\(\beta_1^{(H_0)} = 5 = \beta_1.\)</span>
</li>
<li>The null-hypothesis is false with <span class="math inline">\(\beta_1^{(H_0)} = 0 \neq \beta_1.\)</span>
</li>
</ol>
</li>
</ol>
<p>Note: By the law of large numbers, the above approximation becomes arbitrarily precise as <span class="math inline">\(B\to\infty.\)</span> Thus, setting, for instance, <span class="math inline">\(B=100,000\)</span> typically guarantees a very good approximation.</p>
<p><strong>Link to your personal assignment repository:</strong> <a href="https://classroom.github.com/a/GYoOx4tD">https://classroom.github.com/a/GYoOx4tD</a></p>
</section></section><section id="self-study-exercises" class="level2" data-number="4.10"><h2 data-number="4.10" class="anchored" data-anchor-id="self-study-exercises">
<span class="header-section-number">4.10</span> Self-Study: Exercises</h2>
<p>Prepare the following exercises of Chapter 3 in our course textbook <code>ISLR</code>:</p>
<ul>
<li>Exercise 1</li>
<li>Exercise 2</li>
<li>Exercise 3</li>
<li>Exercise 8</li>
<li>Exercise 9</li>
</ul>
<!-- {{< include Ch4_LinearRegression_Solutions.qmd >}} --><section id="solutions" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="solutions">Solutions</h3>
<section id="exercise-1" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="exercise-1">Exercise 1</h4>
<p><strong>1 a)</strong> Describe the null hypotheses to which the <span class="math inline">\(p\)</span>-values given in Table 3.4 correspond.</p>
<p><img src="DATA/Table.PNG" class="img-fluid"></p>
<p><strong>1 b)</strong> Explain what conclusions you can draw based on these <span class="math inline">\(p\)</span>-values. Your explanation should be phrased in terms of <code>sales</code>, <code>TV</code>, <code>radio</code>, and <code>newspaper</code>, rather than in terms of the coefficients of the linear model.</p>
<p><strong>Answers:</strong></p>
<p><strong>1 a)</strong> In Table 3.4, the null hypothesis for <code>TV</code> is that in the presence of <code>radio</code> ads and <code>newspaper</code> ads, <code>TV</code> ads have no effect on sales. Similarly, the null hypothesis for <code>radio</code> is that in the presence of <code>TV</code> ads and <code>newspaper</code> ads, <code>radio</code> ads have no effect on sales.</p>
<p><strong>1 b)</strong> The low p-values of <code>TV</code> and <code>radio</code> allow us to reject the “no effect” null hypotheses for <code>TV</code> and <code>radio</code>. Hence, we believe that</p>
<ul>
<li>
<code>TV</code> ads have an effect on <code>sales</code> in the presence of <code>radio</code> and <code>newspaper</code> ads.</li>
<li>
<code>radio</code> ads have an effect on <code>sales</code> in the presence of <code>TV</code> and <code>newspaper</code> ads.</li>
</ul>
<p>The high p-value of <code>newspaper</code> does <em>not</em> allow us to reject the “no effect” null-hypothesis. This constitutes an <strong>inconclusive result</strong> and only says that the possible effects of <code>newspaper</code> ads are not large enough to stand out from the estimation errors.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remember
</div>
</div>
<div class="callout-body-container callout-body">
<p>An insignificant hypothesis test result is never informative about whether the tested null hypothesis is true. We do not have an error-control for falsely accepting the null-hypothesis, i.e.&nbsp;for type-II-errors. We only have an error-control (by the significance level) for falsely rejecting the null-hypothesis, i.e.&nbsp;for type-I-errors.</p>
</div>
</div>
</section><section id="exercise-2" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="exercise-2">Exercise 2</h4>
<p>Carefully explain the main difference between the KNN classifier and KNN regression methods.</p>
<p><strong>Answer:</strong></p>
<p>KNN classifier and KNN regression methods are closely related in formula. However, the final result of KNN classifier is the classification output for <span class="math inline">\(Y\)</span> (qualitative), given a certain predictor <span class="math inline">\(x_0\)</span>, where as the output for a KNN regression predicts the quantitative value for <span class="math inline">\(f(x_0)\)</span>, given a certain predictor <span class="math inline">\(x_0\)</span>. <!-- 
Finally, KNN classifies the test observation $x_0$ to the class $j$ with the largest probability from @eq-DefKNN.  
--></p>
</section><section id="exercise-3" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="exercise-3">Exercise 3</h4>
<p>Suppose we have a data set with five predictors:</p>
<p><span class="math inline">\(X_1 =GPA\)</span></p>
<p><span class="math inline">\(X_2 = IQ\)</span></p>
<p><span class="math inline">\(X_3 = Gender\)</span> (<span class="math inline">\(1\)</span> for Female and <span class="math inline">\(0\)</span> for Male)</p>
<p><span class="math inline">\(X_4 =\)</span> Interaction between <span class="math inline">\(GPA\)</span> and <span class="math inline">\(IQ\)</span></p>
<p><span class="math inline">\(X_5 =\)</span> Interaction between <span class="math inline">\(GPA\)</span> and <span class="math inline">\(Gender\)</span></p>
<p>The response variable (in thousands of dollars) is defined as:</p>
<p><span class="math inline">\(Y =\)</span> starting salary after graduation</p>
<p>Suppose we use least squares to fit the model, and get:</p>
<p><span class="math inline">\(\hat{\beta}_0 = 50\)</span>, <span class="math inline">\(\hat{\beta}_1 = 20\)</span>, <span class="math inline">\(\hat{\beta}_2 = 0.07\)</span>, <span class="math inline">\(\hat{\beta}_3 = 35\)</span>, <span class="math inline">\(\hat{\beta}_4 = 0.01\)</span>, and <span class="math inline">\(\hat{\beta}_5 = −10\)</span>.</p>
<p>Thus we have:</p>
<p><span class="math display">\[
\begin{align*}
&amp;E[Y|X] = \\
&amp; 50 + 20\,\overbrace{GPA}^{X_1} + 0.07\,\overbrace{IQ}^{X_2} + 35\,\overbrace{Gender}^{X_3} +
0.01\,\overbrace{GPA\cdot IQ}^{X_4=X_1\cdot X_2} - 10\,\overbrace{GPA\cdot Gender}^{X_5=X_1\cdot X_3}
\end{align*}
\]</span></p>
<p><strong>3 a)</strong> Which answer is correct, and why?</p>
<ol type="i">
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, males earn more on average than females.</li>
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, females earn more on average than males.</li>
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, males earn more on average than females provided that the <span class="math inline">\(GPA\)</span> is high enough.</li>
<li>For a fixed value of <span class="math inline">\(IQ\)</span> and <span class="math inline">\(GPA\)</span>, females earn more on average than males provided that the <span class="math inline">\(GPA\)</span> is high enough.</li>
</ol>
<p><strong>Answer:</strong> Observe that: <span class="math display">\[
\begin{align*}
\text{Male\; $(X_3 = 0)$:}\quad   &amp; 50 + 20 X_1 + 0.07 X_2 + \phantom{3}0 + 0.01\,(X_1 \cdot X_2) -0     \\[1.5ex]
\text{Female\; $(X_3 = 1)$:}\quad &amp; 50 + 20 X_1 + 0.07 X_2 + 35 + 0.01(X_1 \cdot X_2) - 10\,X_1
\end{align*}
\]</span></p>
<p>Thus 3 a) iii. is correct, since once the <span class="math inline">\(X_1=\)</span><code>GPA</code> is high enough (<span class="math inline">\(35-10\,X_1&lt;0 \Leftrightarrow X_1&gt;3.5\)</span>), males earn more on average.</p>
<p><strong>3 b)</strong> Predict the salary of a female with <code>IQ</code> of 110 and a <code>GPA</code> of 4.0.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">GPA</span>    <span class="op">&lt;-</span>   <span class="fl">4</span></span>
<span><span class="va">IQ</span>     <span class="op">&lt;-</span> <span class="fl">110</span></span>
<span><span class="va">Gender</span> <span class="op">&lt;-</span>   <span class="fl">1</span> <span class="co"># female = 1</span></span>
<span><span class="co">## Prediction</span></span>
<span><span class="va">Y_hat</span>  <span class="op">&lt;-</span> <span class="fl">50</span> <span class="op">+</span> <span class="fl">20</span><span class="op">*</span><span class="va">GPA</span> <span class="op">+</span> <span class="fl">0.07</span><span class="op">*</span><span class="va">IQ</span> <span class="op">+</span> <span class="fl">35</span><span class="op">*</span><span class="va">Gender</span> <span class="op">+</span> <span class="fl">0.01</span><span class="op">*</span><span class="va">GPA</span><span class="op">*</span><span class="va">IQ</span> <span class="op">-</span> <span class="fl">10</span><span class="op">*</span><span class="va">GPA</span></span>
<span><span class="va">Y_hat</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 137.1</code></pre>
</div>
</div>
<p><strong>3 c)</strong> True or false: Since the coefficient for the <code>GPA</code><span class="math inline">\(\times\)</span><code>IQ</code> interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.</p>
<p><strong>Answer:</strong></p>
<p>False. We must examine the <span class="math inline">\(p\)</span>-value (or the <span class="math inline">\(t\)</span>-statistic) of the regression coefficient to determine if the interaction term is statistically significant or not.</p>
</section><section id="exercise-8" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="exercise-8">Exercise 8</h4>
<p>This question involves the use of simple linear regression on the <code>Auto</code> data set.</p>
<p><strong>8 a)</strong> Use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to perform a simple linear regression with <code>mpg</code> as the response and <code>horsepower</code> as the predictor. Use the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function to print the results.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Auto"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform linear regression</span></span>
<span><span class="va">lmObj_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Use summary function to print the results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ horsepower, data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5710  -3.2592  -0.3435   2.7630  16.9240 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***
horsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.906 on 390 degrees of freedom
Multiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 
F-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Comment on the output. For example:</p>
<p><strong>i)</strong> Is there a relationship between the predictor and the response?</p>
<p><strong>Answer:</strong></p>
<p>Yes, there is. The predictor horsepower has a statistically significant (<span class="math inline">\(p&lt;0.001\)</span>) linear relationship with the response.</p>
<p><strong>ii)</strong> How strong is the relationship between the predictor and the response?</p>
<p><strong>Answer:</strong></p>
<p>Statistical significance does not necessarily mean a practically strong or important relationship.</p>
<p>To quantify the strength of the relationship between the predictor and the response, we can look at the following quantities:</p>
<ul>
<li>Residual Standard Error (RSE) (estimate of the standard deviation of <span class="math inline">\(\epsilon\)</span>) in comparison to the RSE of the trivial linear regression model with only an intercept.</li>
<li>The <span class="math inline">\(R^2\)</span> Statistic (the proportion of variance explained by the model)</li>
<li>The <span class="math inline">\(F\)</span>-Statistic</li>
</ul>
<p>The Residual Standard Error (RSE) of the regression model with <code>intercept</code> and <code>horsepower</code> as predictors is given by:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## RSE of lm(mpg ~ horsepower):</span></span>
<span><span class="va">RSS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">n</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">RSE</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">RSS</span><span class="op">/</span><span class="op">(</span><span class="va">n</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">RSE</span>, <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.906</code></pre>
</div>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Alternatively: </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">$</span><span class="va">sigma</span>, <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.906</code></pre>
</div>
</div>
<p>This RSE value is considerable smaller than the RSE of a model with only an intercept:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lmObj_onlyIntercept</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="op">+</span><span class="fl">1</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="va">RSS_onlyIntercept</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lmObj_onlyIntercept</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">n</span>                   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">lmObj_onlyIntercept</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">RSE_onlyIntercept</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">RSS_onlyIntercept</span><span class="op">/</span><span class="op">(</span><span class="va">n</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">RSE_onlyIntercept</span>, <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7.805</code></pre>
</div>
</div>
<p>Thus, the larger model with <code>horsepower</code> included explains more of the variances in the response variable <code>mpg</code>. Including <code>horsepower</code> as a predictor reduces the RSE by <code>((RSE_onlyIntercept - RSE)/RSE_onlyIntercept)*100</code> %; i.e.&nbsp;by 37.15%.</p>
<p>The <span class="math inline">\(R^2\)</span> value:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">$</span><span class="va">r.squared</span>, <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.61</code></pre>
</div>
</div>
<p>shows that <span class="math inline">\(60\%\)</span> of variability in <span class="math inline">\(Y\)</span> can be explained using an intercept and <code>horsepower</code> as predictors.</p>
<p>The value of the <span class="math inline">\(F\)</span> statistic</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">$</span><span class="va">fstatistic</span>, <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> value  numdf  dendf 
599.72   1.00 390.00 </code></pre>
</div>
</div>
<p>is much larger than <span class="math inline">\(1\)</span> which means that the linear regression model with intercept and <code>horsepower</code> fits the data significantly better than the trivial regression model with only an intercept.</p>
<p><strong>iii)</strong> Is the relationship between the predictor and the response positive or negative?</p>
<p><strong>Answer:</strong></p>
<p>The relationship is negative, as we can see from the parameter estimate for <code>horsepower</code></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lmObj_1</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>horsepower 
-0.1578447 </code></pre>
</div>
</div>
<p><strong>iv)</strong> What is the predicted <code>mpg</code> associated with a <code>horsepower</code> of <span class="math inline">\(98\)</span>? What is the associated <span class="math inline">\(95\%\)</span> confidence interval? <!-- and prediction intervals? --></p>
<p><strong>Answer:</strong></p>
<p>The predicted value plus confidence interval:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Horsepower of 98</span></span>
<span><span class="va">new_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>horsepower <span class="op">=</span> <span class="fl">98</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># confidence interval </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">lmObj_1</span>, newdata <span class="op">=</span> <span class="va">new_df</span>, interval <span class="op">=</span> <span class="st">"confidence"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit      lwr      upr
1 24.46708 23.97308 24.96108</code></pre>
</div>
</div>
<!-- 
The predicted value plus prediction interval:





::: {.cell layout-align="center"}

```{.r .cell-code}
# Horsepower of 98
new_df <- data.frame(horsepower = 98)

# prediction interval
predict(object = lmObj_1, newdata = new_df, interval = "prediction")
```

::: {.cell-output .cell-output-stdout}

```
       fit     lwr      upr
1 24.46708 14.8094 34.12476
```


:::
:::





-->
<p><strong>8 b)</strong> Plot the response and the predictor. Use the <code><a href="https://rdrr.io/r/graphics/abline.html">abline()</a></code> function to display the least squares regression line.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Auto</span><span class="op">$</span><span class="va">horsepower</span>, y <span class="op">=</span> <span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, ylab <span class="op">=</span> <span class="st">"MPG"</span>, xlab <span class="op">=</span> <span class="st">"Horsepower"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lmObj_1</span>, col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, </span>
<span>       legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"(y,x)"</span>, <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"("</span>,<span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>,<span class="st">",x)"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>       pch<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="cn">NA</span><span class="op">)</span>, lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>,<span class="fl">1</span><span class="op">)</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"blue"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>8 c)</strong> Use the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lmObj_1</span>, col<span class="op">=</span><span class="st">'blue'</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Looking at the smoothing line of the residuals (<span class="math inline">\(e_i=y_i−\hat{y}_i\)</span>) vs.&nbsp;the fitted values (<span class="math inline">\(\hat{y}_i\)</span>), there is a strong pattern in the residuals, indicating non-linearity. You can see evidence of this also in the scatter plot in the answer for question 8 b).</p>
<p>There also appears to be non-constant variance in the error terms (heteroscedasticity), but this may be corrected to an extent when trying a quadratic fit. If not, transformations such as <span class="math inline">\(log(y)\)</span> or <span class="math inline">\(\sqrt{y}\)</span> can shrink larger responses by a greater amount and reduce this issue.</p>
<p>There are some observations with large standardized residuals &amp; high leverage (hence, high Cook’s Distance) that we need to review.</p>
</section><section id="exercise-9" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="exercise-9">Exercise 9</h4>
<p>This question involves the use of multiple linear regression on the <code>Auto</code> data set.</p>
<p><strong>9 a)</strong> Produce a scatterplot matrix which includes all of the variables in the data set.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Auto"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Produce scatterplot matrix</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/pairs.html">pairs</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>9 b)</strong> Compute the matrix of correlations between the variables using the function <code><a href="https://rdrr.io/r/stats/cor.html">cor()</a></code>. You will need to exclude the <code>name</code> variable, which is qualitative.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">Auto</span>, select <span class="op">=</span> <span class="op">-</span><span class="va">name</span><span class="op">)</span><span class="op">)</span>, <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              mpg cylinders displacement horsepower weight acceleration year
mpg           1.0      -0.8         -0.8       -0.8   -0.8          0.4  0.6
cylinders    -0.8       1.0          1.0        0.8    0.9         -0.5 -0.3
displacement -0.8       1.0          1.0        0.9    0.9         -0.5 -0.4
horsepower   -0.8       0.8          0.9        1.0    0.9         -0.7 -0.4
weight       -0.8       0.9          0.9        0.9    1.0         -0.4 -0.3
acceleration  0.4      -0.5         -0.5       -0.7   -0.4          1.0  0.3
year          0.6      -0.3         -0.4       -0.4   -0.3          0.3  1.0
origin        0.6      -0.6         -0.6       -0.5   -0.6          0.2  0.2
             origin
mpg             0.6
cylinders      -0.6
displacement   -0.6
horsepower     -0.5
weight         -0.6
acceleration    0.2
year            0.2
origin          1.0</code></pre>
</div>
</div>
<p><strong>9 c)</strong> Use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to perform a multiple linear regression with <code>mpg</code> as the response and all other variables except <code>name</code> as the predictors. Use the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function to print the results. Comment on the output by answering the below questions 9 c i) to 9 c iii).</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center" data-scrolled="true">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Perform multiplie linear regression</span></span>
<span><span class="va">fit.lm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span><span class="va">name</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Print results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.lm</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ . - name, data = Auto)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.5903 -2.1565 -0.1169  1.8690 13.0604 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***
cylinders     -0.493376   0.323282  -1.526  0.12780    
displacement   0.019896   0.007515   2.647  0.00844 ** 
horsepower    -0.016951   0.013787  -1.230  0.21963    
weight        -0.006474   0.000652  -9.929  &lt; 2e-16 ***
acceleration   0.080576   0.098845   0.815  0.41548    
year           0.750773   0.050973  14.729  &lt; 2e-16 ***
origin         1.426141   0.278136   5.127 4.67e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.328 on 384 degrees of freedom
Multiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 
F-statistic: 252.4 on 7 and 384 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p><strong>9 c i)</strong> Is there a relationship between the predictors and the response?</p>
<p><strong>Answer:</strong></p>
<p>Yes, there is a relationship between the predictors and the response. By testing the null hypothesis of whether all (except intercept) the regression coefficients are zero (i.e.&nbsp;H<span class="math inline">\(_0\)</span>: <span class="math inline">\(\beta_1=\dots=\beta_7=0\)</span>), we can see that the <span class="math inline">\(F\)</span>-statistic is big and its <span class="math inline">\(p\)</span>-value is close to zero, indicating evidence against the null hypothesis.</p>
<p><strong>9 c ii)</strong> Which predictors appear to have a statistically significant relationship to the response?</p>
<p><strong>Answer:</strong></p>
<p>Looking at the <span class="math inline">\(p\)</span>-values associated with each predictor’s <span class="math inline">\(t\)</span>-statistic, we see that <code>displacement</code>, <code>weight</code>, <code>year</code>, and <code>origin</code> have a statistically significant relationship, while <code>cylinders</code>, <code>horsepower</code>, and <code>acceleration</code> do not.</p>
<p><strong>Caution:</strong> This consideration neglects issues due to multiple testing. When testing at the significance level <span class="math inline">\(\alpha=0.05\)</span>, then each single test has a type I error (false H<span class="math inline">\(_0\)</span> rejections) rate of up to <span class="math inline">\(5\%\)</span>. These type I error rates accumulate since we consider seven hypothesis tests simultaneously, and thus the probability of seeing one type I error among the seven tests is up to <span class="math inline">\(7\cdot 5\%=35\%\)</span>. So is quite likely to see one type I error.</p>
<p><strong>Bonferroni correction for multiple testing:</strong> To determine if any of the seven predictors is statistically significant, the corresponding <span class="math inline">\(p\)</span>-value must be smaller than <span class="math inline">\(\alpha/7\)</span>. For instance, with <span class="math inline">\(\alpha/7=0.05/7\approx 0.007\)</span>, only <code>weight</code>, <code>year</code>, and <code>origin</code> have a statistically significant relationships to the response.</p>
<p><strong>9 c iii)</strong> What does the coefficient for the <code>year</code> variable suggest?</p>
<p><strong>Answer:</strong></p>
<p>The regression coefficient for <code>year</code> suggests that, on average, one <code>year</code> later year-of-construction is associated with an increased <code>mpg</code> by <span class="math inline">\(0.75\)</span>, when holding every other predictor value constant.</p>
<p><strong>9 d)</strong> Use the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">fit.lm</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>The “Residuals vs Fitted” plot (1st plot) shows some systematic deviations of the residuals from <span class="math inline">\(0\)</span>. The reason is that we are imposing a straight “line” (better hyper plane) fit for the conditional mean function <span class="math inline">\(E[Y|X]=f(X)\)</span> which appears non-linear here. This results in a systematic underestimation of the true conditional mean function for large and small fitted values <span class="math inline">\(\hat{y}=\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p\)</span>.</p></li>
<li><p>The “Normal Q-Q” plot (2nd plot) suggests non-normally distributed residuals–particularly the upper tail deviates from that of a normal distribution.</p></li>
<li><p>The “Residuals vs Leverage” plot (3rd plot) shows that there are some potential outliers that we can see when: standardized residuals are below <span class="math inline">\(-2\)</span> or above <span class="math inline">\(+2\)</span>. Moreover, the plot shows also potentially problematic “high-leverage” points with leverage values heavily exceeding the rule-of-thumb threshold <span class="math inline">\((p+1)/n=8/392=0.02\)</span>. All points with simultaneously high-leverages and large absolute standardized residuals should be handled with care since these may distort the estimation.</p></li>
<li><p>The “Scale-Location” plot (4th plot) shows is rather inconclusive about heteroscedasticity. However the “Residuals vs Fitted” plot (1st plot)shows some clear sign of heteroscedastic residuals.</p></li>
</ul>
<p><strong>9 e)</strong> Use the <code>*</code> and <code>:</code> symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?</p>
<p><strong>Answer:</strong></p>
<p>Violating the hierarchy principle:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fit.lm0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span><span class="op">+</span><span class="va">cylinders</span><span class="op">+</span><span class="va">year</span><span class="op">+</span><span class="va">weight</span><span class="op">:</span><span class="va">displacement</span>, </span>
<span>              data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.lm0</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ horsepower + cylinders + year + weight:displacement, 
    data = Auto)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.1046 -2.8861 -0.2415  2.3967 15.3221 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         -1.343e+01  5.043e+00  -2.663  0.00807 ** 
horsepower          -3.914e-02  1.278e-02  -3.063  0.00234 ** 
cylinders           -1.358e+00  3.233e-01  -4.201 3.31e-05 ***
year                 6.661e-01  6.019e-02  11.067  &lt; 2e-16 ***
weight:displacement -3.354e-06  1.352e-06  -2.480  0.01355 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.985 on 387 degrees of freedom
Multiple R-squared:  0.7419,    Adjusted R-squared:  0.7393 
F-statistic: 278.2 on 4 and 387 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Following the hierarchical principle:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fit.lm1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">~</span><span class="va">horsepower</span><span class="op">+</span><span class="va">cylinders</span><span class="op">+</span><span class="va">year</span><span class="op">+</span><span class="va">weight</span><span class="op">*</span><span class="va">displacement</span>, </span>
<span>              data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.lm1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ horsepower + cylinders + year + weight * displacement, 
    data = Auto)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.7530 -1.8228 -0.0602  1.5780 12.6133 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         -2.210e+00  3.819e+00  -0.579  0.56316    
horsepower          -3.396e-02  9.560e-03  -3.552  0.00043 ***
cylinders            2.072e-01  2.914e-01   0.711  0.47756    
year                 7.858e-01  4.555e-02  17.250  &lt; 2e-16 ***
weight              -1.084e-02  6.346e-04 -17.076  &lt; 2e-16 ***
displacement        -7.947e-02  9.905e-03  -8.023 1.26e-14 ***
weight:displacement  2.431e-05  2.141e-06  11.355  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.976 on 385 degrees of freedom
Multiple R-squared:  0.8568,    Adjusted R-squared:  0.8546 
F-statistic: 384.1 on 6 and 385 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Note that there is a difference between using <code>A:B</code> and <code>A*B</code> when running a regression. While the first includes only the interaction term between the variable <code>A</code> and <code>B</code>, the second one also includes the stand-alone variables <code>A</code> and <code>B</code>.</p>
<p>Generally, you should follow the hierarchical principle for interaction effects: If we include an interaction in a model, we should also include the main effects, even if the <span class="math inline">\(p\)</span>-values associated with their coefficients are not significant.</p>
<p><strong>9 f)</strong></p>
<p>Try a few different transformations of the variables, such as <span class="math inline">\(\log(X)\)</span>, <span class="math inline">\(\sqrt{X}\)</span>, <span class="math inline">\(X^2\)</span>. Comment on your findings.</p>
<p><strong>Answer:</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fit.lm2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">weight</span><span class="op">)</span><span class="op">+</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">)</span><span class="op">+</span></span>
<span>                <span class="va">acceleration</span><span class="op">+</span><span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">acceleration</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>,</span>
<span>              data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.lm2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ log(weight) + sqrt(horsepower) + acceleration + 
    I(acceleration^2), data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-11.2932  -2.5082  -0.2237   2.0237  15.7650 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       178.30303   10.80451  16.503  &lt; 2e-16 ***
log(weight)       -14.74259    1.73994  -8.473 5.06e-16 ***
sqrt(horsepower)   -1.85192    0.36005  -5.144 4.29e-07 ***
acceleration       -2.19890    0.63903  -3.441 0.000643 ***
I(acceleration^2)   0.06139    0.01857   3.305 0.001037 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.99 on 387 degrees of freedom
Multiple R-squared:  0.7414,    Adjusted R-squared:  0.7387 
F-statistic: 277.3 on 4 and 387 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">##</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">fit.lm2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This try suffers basically from the same issues as the model considered in 9 d)</p>
<p>Let’s consider again the model with all predictors (except <code>name</code>), but with transforming the outcome variable <code>mpg</code> by a <span class="math inline">\(\log\)</span>-transformation.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fit.lm3</span> <span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">)</span><span class="op">~</span> <span class="va">.</span> <span class="op">-</span><span class="va">name</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.lm3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log(mpg) ~ . - name, data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.40955 -0.06533  0.00079  0.06785  0.33925 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.751e+00  1.662e-01  10.533  &lt; 2e-16 ***
cylinders    -2.795e-02  1.157e-02  -2.415  0.01619 *  
displacement  6.362e-04  2.690e-04   2.365  0.01852 *  
horsepower   -1.475e-03  4.935e-04  -2.989  0.00298 ** 
weight       -2.551e-04  2.334e-05 -10.931  &lt; 2e-16 ***
acceleration -1.348e-03  3.538e-03  -0.381  0.70339    
year          2.958e-02  1.824e-03  16.211  &lt; 2e-16 ***
origin        4.071e-02  9.955e-03   4.089 5.28e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1191 on 384 degrees of freedom
Multiple R-squared:  0.8795,    Adjusted R-squared:  0.8773 
F-statistic: 400.4 on 7 and 384 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">##</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">fit.lm3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This model specification is much better!</p>
<ul>
<li>No clear issues of systematic under/over estimations for given fitted values.</li>
<li>No clear issues of heteroscedastic residuals.</li>
<li>Normality assumption may be wrong, but this isn’t problematic since we have a large dataset, such that a central limit theorem will make the estimators asymptotically normal distributed.</li>
<li>One large leverage point which, however, has a small residual.</li>
</ul></section></section></section><section id="self-study-r-lab-linear-regression" class="level2" data-number="4.11"><h2 data-number="4.11" class="anchored" data-anchor-id="self-study-r-lab-linear-regression">
<span class="header-section-number">4.11</span> Self-Study: <code>R</code>-Lab Linear Regression</h2>
<section id="libraries" class="level3" data-number="4.11.1"><h3 data-number="4.11.1" class="anchored" data-anchor-id="libraries">
<span class="header-section-number">4.11.1</span> Libraries</h3>
<p>The <code><a href="https://rdrr.io/r/base/library.html">library()</a></code> function is used to load <em>libraries</em>, or groups of functions and data sets that are not included in the base <code>R</code> distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries.</p>
<p>Here we load the <code>MASS</code> package, which is a very large collection of data sets and functions. We also load the <code>ISLR2</code> package, which includes the data sets associated with this book.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as <code>MASS</code>, come with <code>R</code> and do not need to be separately installed on your computer. However, other packages, such as <code>ISLR2</code>, must be downloaded the first time they are used. <!-- 
This can be done directly from within `R`. For example, on a Windows system,  select the `Install package` option under the `Packages` tab.  After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and `R` will automatically download the package. Alternatively,  --></p>
<p>This can be done, for instance, at the <code>R</code> command line via <code>install.packages("ISLR2")</code> function. This installation only needs to be done the first time you use a package. However, the <code><a href="https://rdrr.io/r/base/library.html">library()</a></code> function must be called within each <code>R</code> session.</p>
</section><section id="simple-linear-regression" class="level3" data-number="4.11.2"><h3 data-number="4.11.2" class="anchored" data-anchor-id="simple-linear-regression">
<span class="header-section-number">4.11.2</span> Simple Linear Regression</h3>
<p>The <code>ISLR2</code> library contains the <code>Boston</code> data set, which records <code>medv</code> (median house value) for <span class="math inline">\(506\)</span> census tracts in Boston. We will seek to predict <code>medv</code> using <span class="math inline">\(12\)</span> predictors such as <code>rmvar</code> (average number of rooms per house), <code>age</code> (average age of houses), and <code>lstat</code> (percent of households with low socioeconomic status).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat
1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21
  medv
1 24.0
2 21.6
3 34.7
4 33.4
5 36.2
6 28.7</code></pre>
</div>
</div>
<p>To find out more about the data set, we can type <code><a href="https://rdrr.io/pkg/ISLR2/man/Boston.html">?Boston</a></code>.</p>
<p>We will start by using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to fit a simple linear regression model, with <code>medv</code> as the response and <code>lstat</code> as the predictor. The basic syntax is <code>lm(y ~ x, data)</code>, where <code>y</code> is the response, <code>x</code> is the predictor, and <code>data</code> is the data set in which these two variables are kept.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(predvars, data, env): object 'medv' not found</code></pre>
</div>
</div>
<p>The command causes an error because <code>R</code> does not know where to find the variables <code>medv</code> and <code>lstat</code>.</p>
<p>The next line tells <code>R</code> that the variables are in <code>Boston</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Alternatively, we can attach the <code>Boston</code> object:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/attach.html">attach</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">)</span></span>
<span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we type <code>lm.fit</code>, some basic information about the model is output. For more detailed information, we use <code>summary(lm.fit)</code>. This gives us <span class="math inline">\(p\)</span>-values and standard errors for the coefficients, as well as the <span class="math inline">\(R^2\)</span> statistic and <span class="math inline">\(F\)</span>-statistic for the model.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat)

Coefficients:
(Intercept)        lstat  
      34.55        -0.95  </code></pre>
</div>
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.168  -3.990  -1.318   2.034  24.500 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***
lstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.216 on 504 degrees of freedom
Multiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 
F-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can use the <code><a href="https://rdrr.io/r/base/names.html">names()</a></code> or the <code><a href="https://rdrr.io/r/utils/str.html">str()</a></code> function in order to find out what other pieces of information are stored in <code>lm.fit</code>.</p>
<p>We can extract these quantities by name—e.g.&nbsp;<code>lm.fit$coefficients</code>.</p>
<p>For some objects, there are also specific extractor functions like <code><a href="https://rdrr.io/r/stats/coef.html">coef()</a></code> to access them.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"        </code></pre>
</div>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)       lstat 
 34.5538409  -0.9500494 </code></pre>
</div>
</div>
<p>In order to obtain a confidence interval for the coefficient estimates, we can use the <code><a href="https://rdrr.io/r/stats/confint.html">confint()</a></code> command.</p>
<p>Type <code>confint(lm.fit)</code> at the command line to obtain the confidence intervals for the linear regression coefficients.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                2.5 %     97.5 %
(Intercept) 33.448457 35.6592247
lstat       -1.026148 -0.8739505</code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function can be used to produce confidence intervals and prediction intervals for the prediction of <code>medv</code> for a given value of <code>lstat</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lm.fit</span>, <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>lstat <span class="op">=</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">10</span>, <span class="fl">15</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>        interval <span class="op">=</span> <span class="st">"confidence"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit      lwr      upr
1 29.80359 29.00741 30.59978
2 25.05335 24.47413 25.63256
3 20.30310 19.73159 20.87461</code></pre>
</div>
</div>
<p>For instance, the observed value of the 95% confidence interval associated with a <code>lstat</code> value of <span class="math inline">\(10,\)</span> i.e.&nbsp;associated with <span class="math display">\[
\beta_0 + \beta_1 \cdot 10
\]</span> is <span class="math display">\[
\operatorname{CI}_{\beta_0 + \beta_1 \cdot 10,obs}=[24.47, 25.63]
\]</span> with observed prediction value <span class="math display">\[
25.05335 = \hat\beta_0 + \hat\beta_1 \cdot 10.
\]</span></p>
<!-- predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), 
        interval = "prediction")
The 95\% prediction interval associated with a `lstat` value of 10 is $(12.828, 37.28)$. 

As expected, the confidence and prediction intervals are centered around the same point (a predicted value of $\hat{y}_i=25.05$ for `medv` when $x_{i2}=$`lstat` equals 10), but the latter are substantially wider.

::: {.callout-caution}
The `predict()` function with the option `interval = "prediction"` assumes **Gaussian** error terms; i.e. 
$$
\epsilon_i\sim\mathcal{N}(0,\sigma^2),\quad\text{for all}\quad i=1,\dots,n.
$$
If this distributional assumption is not fulfilled, you should not use the prediction interval.
::: -->
<p>We will now plot <code>medv</code> and <code>lstat</code> along with the least squares regression line using the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> and <code><a href="https://rdrr.io/r/graphics/abline.html">abline()</a></code> functions.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>There is some evidence for non-linearity in the relationship between <code>lstat</code> and <code>medv</code>. We will explore this issue later in this lab.</p>
<p>The <code><a href="https://rdrr.io/r/graphics/abline.html">abline()</a></code> function can be used to draw any line, not just the least squares regression line. To draw a line with intercept <code>a</code> and slope <code>b</code>, we type <code>abline(a, b)</code>. Below we experiment with some additional settings for plotting lines and points. The <code>lwd = 3</code> command causes the width of the regression line to be increased by a factor of 3; this works for the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> and <code><a href="https://rdrr.io/r/graphics/lines.html">lines()</a></code> functions also. We can also use the <code>pch</code> option to create different plotting symbols.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lm.fit</span>, lwd <span class="op">=</span> <span class="fl">3</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk10-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span>, pch <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk10-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="va">medv</span>, pch <span class="op">=</span> <span class="st">"+"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk10-4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, pch <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk10-5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Next we examine some diagnostic plots. Four diagnostic plots are automatically produced by applying the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> function directly to the output from <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>. In general, this command will produce one plot at a time, and hitting <em>Enter</em> will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the <code><a href="https://rdrr.io/r/graphics/par.html">par()</a></code> and <code>mfrow()</code> functions, which tell <code>R</code> to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, <code>par(mfrow = c(2, 2))</code> divides the plotting region into a <span class="math inline">\(2 \times 2\)</span> grid of panels.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk11-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Alternatively, we can compute the residuals from a linear regression fit using the <code><a href="https://rdrr.io/r/stats/residuals.html">residuals()</a></code> function. The function <code><a href="https://rdrr.io/r/stats/influence.measures.html">rstudent()</a></code> will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk12-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">rstudent</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk12-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>On the basis of the residual plots, there is some evidence of non-linearity.</p>
<p>Leverage statistics can be computed using the <code><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues()</a></code> function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk13-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.max</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>375 
375 </code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/base/which.min.html">which.max()</a></code> function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span>, decreasing <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       375        415        374 
0.02686517 0.02495670 0.02097101 </code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/base/sort.html">sort()</a></code> function can be used to sort and print values of a vector like <code>hatvalues(lm.fit)</code>.</p>
</section><section id="multiple-linear-regression" class="level3" data-number="4.11.3"><h3 data-number="4.11.3" class="anchored" data-anchor-id="multiple-linear-regression">
<span class="header-section-number">4.11.3</span> Multiple Linear Regression</h3>
<p>In order to fit a multiple linear regression model using least squares, we again use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function. The syntax <code>lm(y ~ x1 + x2 + x3)</code> is used to fit a model with three predictors, <code>x1</code>, <code>x2</code>, and <code>x3</code>. The <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function now outputs the regression coefficients for all the predictors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span> <span class="op">+</span> <span class="va">age</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + age, data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.981  -3.978  -1.283   1.968  23.158 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***
lstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***
age          0.03454    0.01223   2.826  0.00491 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.173 on 503 degrees of freedom
Multiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 
F-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code>Boston</code> data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ ., data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.595  -2.730  -0.518   1.777  26.199 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
zn           4.642e-02  1.373e-02   3.382 0.000778 ***
indus        2.056e-02  6.150e-02   0.334 0.738288    
chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
rm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***
age          6.922e-04  1.321e-02   0.052 0.958229    
dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
black        9.312e-03  2.686e-03   3.467 0.000573 ***
lstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.745 on 492 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 
F-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can access the individual components of a summary object by name (type <code><a href="https://rdrr.io/r/stats/summary.lm.html">?summary.lm</a></code> to see what is available). Hence <code>summary(lm.fit)$r.sq</code> gives us the <span class="math inline">\(R^2\)</span>, and <code>summary(lm.fit)$sigma</code> gives us the RSE. The <code><a href="https://rdrr.io/pkg/car/man/vif.html">vif()</a></code> function, part of the <code>car</code> package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The <code>car</code> package is not part of the base <code>R</code> installation so it must be downloaded the first time you use it via the <code><a href="https://rdrr.io/r/utils/install.packages.html">install.packages()</a></code> function in <code>R</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span><span class="op">)</span> <span class="co"># contains the vif() function</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span><span class="op">)</span> <span class="co"># computes the VIF statistics and sorts them</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    chas    black     crim  ptratio       rm       zn    lstat      age 
1.073995 1.348521 1.792192 1.799084 1.933744 2.298758 2.941491 3.100826 
     dis    indus      nox      rad      tax 
3.955945 3.991596 4.393720 7.484496 9.008554 </code></pre>
</div>
</div>
<p>What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, <code>age</code> has a high <span class="math inline">\(p\)</span>-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except <code>age</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="va">age</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ . - age, data = Boston)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.6054  -2.7313  -0.5188   1.7601  26.2243 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***
crim         -0.108006   0.032832  -3.290 0.001075 ** 
zn            0.046334   0.013613   3.404 0.000719 ***
indus         0.020562   0.061433   0.335 0.737989    
chas          2.689026   0.859598   3.128 0.001863 ** 
nox         -17.713540   3.679308  -4.814 1.97e-06 ***
rm            3.814394   0.408480   9.338  &lt; 2e-16 ***
dis          -1.478612   0.190611  -7.757 5.03e-14 ***
rad           0.305786   0.066089   4.627 4.75e-06 ***
tax          -0.012329   0.003755  -3.283 0.001099 ** 
ptratio      -0.952211   0.130294  -7.308 1.10e-12 ***
black         0.009321   0.002678   3.481 0.000544 ***
lstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.74 on 493 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 
F-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Alternatively, the <code><a href="https://rdrr.io/r/stats/update.html">update()</a></code> function can be used.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span><span class="op">(</span><span class="va">lm.fit</span>, <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="va">age</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="interaction-terms" class="level3" data-number="4.11.4"><h3 data-number="4.11.4" class="anchored" data-anchor-id="interaction-terms">
<span class="header-section-number">4.11.4</span> Interaction Terms</h3>
<p>It is easy to include interaction terms in a linear model using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function. The syntax <code>lstat:black</code> tells <code>R</code> to include an interaction term between <code>lstat</code> and <code>black</code>. The syntax <code>lstat * age</code> simultaneously includes <code>lstat</code>, <code>age</code>, and the interaction term <code>lstat</code><span class="math inline">\(\times\)</span><code>age</code> as predictors; it is a shorthand for <code>lstat + age + lstat:age</code>. %We can also pass in transformed versions of the predictors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb98"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span> <span class="op">*</span> <span class="va">age</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat * age, data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.806  -4.045  -1.333   2.085  27.552 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***
lstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***
age         -0.0007209  0.0198792  -0.036   0.9711    
lstat:age    0.0041560  0.0018518   2.244   0.0252 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.149 on 502 degrees of freedom
Multiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 
F-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section><section id="non-linear-transformations-of-the-predictors" class="level3" data-number="4.11.5"><h3 data-number="4.11.5" class="anchored" data-anchor-id="non-linear-transformations-of-the-predictors">
<span class="header-section-number">4.11.5</span> Non-linear Transformations of the Predictors</h3>
<p>The <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function can also accommodate non-linear transformations of the predictors. For instance, given a predictor <span class="math inline">\(X\)</span>, we can create a predictor <span class="math inline">\(X^2\)</span> using <code>I(X^2)</code>. The function <code><a href="https://rdrr.io/r/base/AsIs.html">I()</a></code> is needed since the <code>^</code> has a special meaning in a formula object; wrapping as we do allows the standard usage in <code>R</code>, which is to raise <code>X</code> to the power <code>2</code>. We now perform a regression of <code>medv</code> onto <code>lstat</code> and <code>lstat^2</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">lstat</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + I(lstat^2))

Residuals:
     Min       1Q   Median       3Q      Max 
-15.2834  -3.8313  -0.5295   2.3095  25.4148 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***
lstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***
I(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.524 on 503 degrees of freedom
Multiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 
F-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The near-zero <span class="math inline">\(p\)</span>-value associated with the quadratic term suggests that it leads to an improved model. We use the <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> function to further quantify the extent to which the quadratic fit is superior to the linear fit.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb102"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">lm.fit</span>, <span class="va">lm.fit2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: medv ~ lstat
Model 2: medv ~ lstat + I(lstat^2)
  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
1    504 19472                                 
2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Here Model 1 represents the linear submodel containing only one predictor, <code>lstat</code>, while Model 2 corresponds to the larger quadratic model that has two predictors, <code>lstat</code> and <code>lstat^2</code>. The <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior.</p>
<p>Here the <span class="math inline">\(F\)</span>-statistic is <span class="math inline">\(135\)</span> and the associated <span class="math inline">\(p\)</span>-value is virtually zero. This provides very clear evidence that the model containing the predictors <code>lstat</code> and <code>lstat^2</code> is far superior to the model that only contains the predictor <code>lstat</code>.</p>
<p>This is not surprising, since earlier we saw evidence for non-linearity in the relationship between <code>medv</code> and <code>lstat</code>.</p>
<p>If we type</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lm.fit2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="Ch4_LinearRegression_files/figure-html/chunk22-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>then we see that when the <code>lstat^2</code> term is included in the model, there is little discernible pattern in the residuals.</p>
<p>In order to create a <strong>cubic fit</strong>, we can include a predictor of the form <code>I(X^3)</code>. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function to create the polynomial within <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>. For example, the following command produces a fifth-order polynomial fit:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb105"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit5</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">lstat</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ poly(lstat, 5))

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5433  -3.1039  -0.7052   2.0844  27.1153 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***
poly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***
poly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***
poly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***
poly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***
poly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.215 on 500 degrees of freedom
Multiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 
F-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>This suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant <span class="math inline">\(p\)</span>-values in a regression fit.</p>
<p>By default, the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function, the argument <code>raw = TRUE</code> must be used.</p>
<p>Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb107"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">rm</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Boston</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ log(rm), data = Boston)

Residuals:
    Min      1Q  Median      3Q     Max 
-19.487  -2.875  -0.104   2.837  39.816 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***
log(rm)       54.055      2.739   19.73   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.915 on 504 degrees of freedom
Multiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 
F-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section><section id="qualitative-predictors-1" class="level3" data-number="4.11.6"><h3 data-number="4.11.6" class="anchored" data-anchor-id="qualitative-predictors-1">
<span class="header-section-number">4.11.6</span> Qualitative Predictors</h3>
<p>We will now examine the <code>Carseats</code> data, which is part of the <code>ISLR2</code> library. We will attempt to predict <code>Sales</code> (child car seat sales) in <span class="math inline">\(400\)</span> locations based on a number of predictors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb109"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Carseats</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education
1  9.50       138     73          11        276   120       Bad  42        17
2 11.22       111     48          16        260    83      Good  65        10
3 10.06       113     35          10        269    80    Medium  59        12
4  7.40       117    100           4        466    97    Medium  55        14
5  4.15       141     64           3        340   128       Bad  38        13
6 10.81       124    113          13        501    72       Bad  78        16
  Urban  US
1   Yes Yes
2   Yes Yes
3   Yes Yes
4   Yes Yes
5   Yes  No
6    No Yes</code></pre>
</div>
</div>
<p>The <code>Carseats</code> data includes qualitative predictors such as <code>shelveloc</code>, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor <code>shelveloc</code> takes on three possible values: <em>Bad</em>, <em>Medium</em>, and <em>Good</em>. Given a qualitative variable such as <code>shelveloc</code>, <code>R</code> generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Sales</span> <span class="op">~</span> <span class="va">.</span> <span class="op">+</span> <span class="va">Income</span><span class="op">:</span><span class="va">Advertising</span> <span class="op">+</span> <span class="va">Price</span><span class="op">:</span><span class="va">Age</span>, </span>
<span>    data <span class="op">=</span> <span class="va">Carseats</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9208 -0.7503  0.0177  0.6754  3.3413 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***
CompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***
Income              0.0108940  0.0026044   4.183 3.57e-05 ***
Advertising         0.0702462  0.0226091   3.107 0.002030 ** 
Population          0.0001592  0.0003679   0.433 0.665330    
Price              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***
ShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***
ShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***
Age                -0.0579466  0.0159506  -3.633 0.000318 ***
Education          -0.0208525  0.0196131  -1.063 0.288361    
UrbanYes            0.1401597  0.1124019   1.247 0.213171    
USYes              -0.1575571  0.1489234  -1.058 0.290729    
Income:Advertising  0.0007510  0.0002784   2.698 0.007290 ** 
Price:Age           0.0001068  0.0001333   0.801 0.423812    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.011 on 386 degrees of freedom
Multiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 
F-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/contrasts.html">contrasts()</a></code> function returns the coding that <code>R</code> uses for the dummy variables.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb113"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/attach.html">attach</a></span><span class="op">(</span><span class="va">Carseats</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/contrasts.html">contrasts</a></span><span class="op">(</span><span class="va">ShelveLoc</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Good Medium
Bad       0      0
Good      1      0
Medium    0      1</code></pre>
</div>
</div>
<p>Use <code><a href="https://rdrr.io/r/stats/contrasts.html">?contrasts</a></code> to learn about other contrasts, and how to set them.</p>
<p><code>R</code> has created a <code>ShelveLocGood</code> dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a <code>ShelveLocMedium</code> dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for <code>ShelveLocGood</code> in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And <code>ShelveLocMedium</code> has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.</p>
</section><section id="writing-functions" class="level3" data-number="4.11.7"><h3 data-number="4.11.7" class="anchored" data-anchor-id="writing-functions">
<span class="header-section-number">4.11.7</span> Writing Functions</h3>
<p>As we have seen, <code>R</code> comes with many useful functions, and still more functions are available by way of <code>R</code> libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the <code>ISLR2</code> and <code>MASS</code> libraries, called <code>LoadLibraries()</code>. Before we have created the function, <code>R</code> returns an error if we try to call it.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb115"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LoadLibraries</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(expr, envir, enclos): object 'LoadLibraries' not found</code></pre>
</div>
<div class="sourceCode" id="cb117"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">LoadLibraries</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in LoadLibraries(): could not find function "LoadLibraries"</code></pre>
</div>
</div>
<p>We now create the function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb119"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LoadLibraries</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span> <span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span></span>
<span> <span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span> <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="st">"The libraries have been loaded."</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now if we type in <code>LoadLibraries</code>, <code>R</code> will tell us what is in the function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb120"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">LoadLibraries</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>function() {
 library(ISLR2)
 library(MASS)
 print("The libraries have been loaded.")
}</code></pre>
</div>
</div>
<p>If we call the function, the libraries are loaded in and the print statement is output.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb122"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">LoadLibraries</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "The libraries have been loaded."</code></pre>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Cribari_2004" class="csl-entry" role="listitem">
Cribari-Neto, Francisco. 2004. <span>“Asymptotic Inference Under Heteroskedasticity of Unknown Form.”</span> <em>Computational Statistics &amp; Data Analysis</em> 45 (2): 215–33.
</div>
<div id="ref-Long_Ervin_2000" class="csl-entry" role="listitem">
Long, J Scott, and Laurie H Ervin. 2000. <span>“Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.”</span> <em>The American Statistician</em> 54 (3): 217–24.
</div>
<div id="ref-MacKinnon_White_1985" class="csl-entry" role="listitem">
MacKinnon, James G, and Halbert White. 1985. <span>“Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.”</span> <em>Journal of Econometrics</em> 29 (3): 305–25.
</div>
<div id="ref-White1980" class="csl-entry" role="listitem">
White, Halbert. 1980. <span>“A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.”</span> <em>Econometrica</em>, 817–38.
</div>
</div>
</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./Ch3_MatrixAlgebra.html" class="pagination-link" aria-label="Matrix Algebra">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch5_Classification.html" class="pagination-link" aria-label="Classification">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>